//===----------------------------------------------------------------------===//
// Copyright Â© 2024 Apple Inc. and the Pkl project authors. All rights reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//===----------------------------------------------------------------------===//
/// Global configuration options.
///
/// This module was generated from JSON Schema from
/// <file:///Users/thomas/src/github.com/vectordotdev/vector/schema.json>.
///
/// WARN: The root schema describes open-ended properties, but this is not possible to describe at the
/// module level.
module Schema

/// Controls how acknowledgements are handled for all sinks by default.
///
/// See [End-to-end Acknowledgements][e2e_acks] for more information on how Vector handles event
/// acknowledgement.
///
/// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
///
/// Default if undefined: `{ ["enabled"] = null }`
acknowledgements: Any?

/// The directory used for persisting Vector state data.
///
/// This is the directory where Vector will store any state data, such as disk buffers, file checkpoints,
/// and more.
///
/// Vector must have write permissions to this directory.
///
/// Default if undefined: `"/var/lib/vector/"`
data_dir: Any?

/// The amount of time, in seconds, that internal metrics will persist after having not been updated
/// before they expire and are removed.
///
/// Not set by default, which allows all internal metrics to grow unbounded over time. If you have a
/// configuration that emits many high-cardinality metrics, you may want to consider setting this to a
/// value that ensures that metrics live long enough to be emitted and captured, but not so long that
/// they continue to build up indefinitely, as this will consume a small amount of memory for each
/// metric.
@Deprecated
expire_metrics: (Any|ExpireMetricsAlternate1)?

/// The amount of time, in seconds, that internal metrics will persist after having not been updated
/// before they expire and are removed.
///
/// Not set by default, which allows all internal metrics to grow unbounded over time. If you have a
/// configuration that emits many high-cardinality metrics, you may want to consider setting this to a
/// value that ensures that metrics live long enough to be emitted and captured, but not so long that
/// they continue to build up indefinitely, as this will consume a small amount of memory for each
/// metric.
expire_metrics_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?

/// Default log schema for all events.
///
/// This is used if a component does not have its own specific log schema. All events use a log schema,
/// whether or not the default is used, to assign event fields on incoming events.
///
/// Default if undefined: `{ ["message_key"] = ".message" ["timestamp_key"] = ".timestamp" ["host_key"] =
/// ".host" ["source_type_key"] = ".source_type" ["metadata_key"] = ".metadata" }`
log_schema: LogSchema?

/// Proxy configuration.
///
/// Configure to proxy traffic through an HTTP(S) proxy when making external requests.
///
/// Similar to common proxy configuration convention, you can set different proxies to use based on the
/// type of traffic being proxied, as well as set specific hosts that should not be proxied.
///
/// Default if undefined: `{}`
proxy: Any?

/// Telemetry options.
///
/// Determines whether `source` and `service` tags should be emitted with the `component_sent_*` and
/// `component_received_*` events.
///
/// Default if undefined: `{ ["tags"] { ["emit_service"] = false ["emit_source"] = false } }`
telemetry: Telemetry?

/// The name of the time zone to apply to timestamp conversions that do not contain an explicit time
/// zone.
///
/// The time zone name may be any name in the [TZ database][tzdb] or `local` to indicate system local
/// time.
///
/// [tzdb]: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
timezone: Any?

/// An duration of time.
class ExpireMetricsAlternate1 {
  nsecs: UInt32

  secs: Int(isBetween(0.0, 9.007199254740991E15))
}

/// Default log schema for all events.
///
/// This is used if a component does not have its own specific log schema. All events use a log schema,
/// whether or not the default is used, to assign event fields on incoming events.
///
/// Default if undefined: `{ ["message_key"] = ".message" ["timestamp_key"] = ".timestamp" ["host_key"] =
/// ".host" ["source_type_key"] = ".source_type" ["metadata_key"] = ".metadata" }`
class LogSchema {
  /// The name of the event field to treat as the host which sent the message.
  ///
  /// This field will generally represent a real host, or container, that generated the message, but is
  /// somewhat source-dependent.
  ///
  /// Default if undefined: `".host"`
  host_key: Any?

  /// The name of the event field to treat as the event message.
  ///
  /// This would be the field that holds the raw message, such as a raw log line.
  ///
  /// Default if undefined: `".message"`
  message_key: Any?

  /// The name of the event field to set the event metadata in.
  ///
  /// Generally, this field will be set by Vector to hold event-specific metadata, such as annotations by
  /// the `remap` transform when an error or abort is encountered.
  ///
  /// Default if undefined: `".metadata"`
  metadata_key: Any?

  /// The name of the event field to set the source identifier in.
  ///
  /// This field will be set by the Vector source that the event was created in.
  ///
  /// Default if undefined: `".source_type"`
  source_type_key: Any?

  /// The name of the event field to treat as the event timestamp.
  ///
  /// Default if undefined: `".timestamp"`
  timestamp_key: Any?
}

/// Telemetry options.
///
/// Determines whether `source` and `service` tags should be emitted with the `component_sent_*` and
/// `component_received_*` events.
///
/// Default if undefined: `{ ["tags"] { ["emit_service"] = false ["emit_source"] = false } }`
class Telemetry {
  /// Configures whether to emit certain tags
  ///
  /// Default if undefined: `{ ["emit_service"] = false ["emit_source"] = false }`
  tags: Tags?
}

/// Configures whether to emit certain tags
///
/// Default if undefined: `{ ["emit_service"] = false ["emit_source"] = false }`
class Tags {
  /// True if the `service` tag should be emitted in the `component_received_*` and `component_sent_*`
  /// telemetry.
  emit_service: Boolean?

  /// True if the `source` tag should be emitted in the `component_received_*` and `component_sent_*`
  /// telemetry.
  emit_source: Boolean?
}

/// Options for building a `LengthDelimitedDecoder` or `LengthDelimitedEncoder`.
class `Codecs::common::lengthDelimited::LengthDelimitedCoderOptions` {
  /// Length field byte order (little or big endian)
  ///
  /// Default if undefined: `true`
  length_field_is_big_endian: Boolean?

  /// Number of bytes representing the field length
  ///
  /// Default if undefined: `4`
  length_field_length: Int(isBetween(0.0, 9.007199254740991E15))?

  /// Number of bytes in the header before the length field
  ///
  /// Default if undefined: `0`
  length_field_offset: Int(isBetween(0.0, 9.007199254740991E15))?

  /// Maximum frame length
  ///
  /// Default if undefined: `8388608`
  max_frame_length: Int(isBetween(0.0, 9.007199254740991E15))?
}

/// Uses the raw bytes as-is.
class DecodingAlternate1Alternate0 {
  /// Uses the raw bytes as-is.
  codec: "bytes"
}

/// Decodes the raw bytes as [JSON][json].
///
/// Config used to build a `JsonDeserializer`.
class DecodingAlternate1Alternate1 {
  /// Decodes the raw bytes as [JSON][json].
  ///
  /// [json]: https://www.json.org/
  codec: "json"
}

/// Decodes the raw bytes as [protobuf][protobuf].
///
/// Config used to build a `ProtobufDeserializer`.
class DecodingAlternate1Alternate2 {
  /// Decodes the raw bytes as [protobuf][protobuf].
  ///
  /// [protobuf]: https://protobuf.dev/
  codec: "protobuf"
}

/// Decodes the raw bytes as a Syslog message.
///
/// Config used to build a `SyslogDeserializer`.
class DecodingAlternate1Alternate3 {
  /// Decodes the raw bytes as a Syslog message.
  ///
  /// Decodes either as the [RFC 3164][rfc3164]-style format ("old" style) or the [RFC
  /// 5424][rfc5424]-style format ("new" style, includes structured data).
  ///
  /// [rfc3164]: https://www.ietf.org/rfc/rfc3164.txt [rfc5424]: https://www.ietf.org/rfc/rfc5424.txt
  codec: "syslog"
}

/// Decodes the raw bytes as [native Protocol Buffers format][vector_native_protobuf].
///
/// This codec is **[experimental][experimental]**.
///
/// [vector_native_protobuf]:
/// https://github.com/vectordotdev/vector/blob/master/lib/vector-core/proto/event.proto [experimental]:
/// https://vector.dev/highlights/2022-03-31-native-event-codecs
class DecodingAlternate1Alternate4 {
  /// Decodes the raw bytes as [native Protocol Buffers format][vector_native_protobuf].
  ///
  /// This codec is **[experimental][experimental]**.
  ///
  /// [vector_native_protobuf]:
  /// https://github.com/vectordotdev/vector/blob/master/lib/vector-core/proto/event.proto
  /// [experimental]: https://vector.dev/highlights/2022-03-31-native-event-codecs
  codec: "native"
}

/// Decodes the raw bytes as [native JSON format][vector_native_json].
///
/// Config used to build a `NativeJsonDeserializer`.
class DecodingAlternate1Alternate5 {
  /// Decodes the raw bytes as [native JSON format][vector_native_json].
  ///
  /// This codec is **[experimental][experimental]**.
  ///
  /// [vector_native_json]:
  /// https://github.com/vectordotdev/vector/blob/master/lib/codecs/tests/data/native_encoding/schema.cue
  /// [experimental]: https://vector.dev/highlights/2022-03-31-native-event-codecs
  codec: "native_json"
}

/// Decodes the raw bytes as a [GELF][gelf] message.
///
/// Config used to build a `GelfDeserializer`.
class DecodingAlternate1Alternate6 {
  /// Decodes the raw bytes as a [GELF][gelf] message.
  ///
  /// This codec is experimental for the following reason:
  ///
  /// The GELF specification is more strict than the actual Graylog receiver. Vector's decoder currently
  /// adheres more strictly to the GELF spec, with the exception that some characters such as `@` are
  /// allowed in field names.
  ///
  /// Other GELF codecs such as Loki's, use a [Go SDK][implementation] that is maintained by Graylog, and
  /// is much more relaxed than the GELF spec.
  ///
  /// Going forward, Vector will use that [Go SDK][implementation] as the reference implementation, which
  /// means the codec may continue to relax the enforcement of specification.
  ///
  /// [gelf]: https://docs.graylog.org/docs/gelf [implementation]:
  /// https://github.com/Graylog2/go-gelf/blob/v2/gelf/reader.go
  codec: "gelf"
}

/// Decodes the raw bytes as as an [Apache Avro][apache_avro] message.
///
/// [apache_avro]: https://avro.apache.org/
class DecodingAlternate1Alternate7 {
  /// Apache Avro-specific encoder options.
  avro: DecodingAlternate1Alternate7Avro

  /// Decodes the raw bytes as as an [Apache Avro][apache_avro] message.
  ///
  /// [apache_avro]: https://avro.apache.org/
  codec: "avro"
}

/// Apache Avro-specific encoder options.
class DecodingAlternate1Alternate7Avro {
  /// The Avro schema definition. Please note that the following [`apache_avro::types::Value`] variants
  /// are currently *not* supported: * `Date` * `Decimal` * `Duration` * `Fixed` * `TimeMillis`
  schema: String

  /// For Avro datum encoded in Kafka messages, the bytes are prefixed with the schema ID. Set this to
  /// true to strip the schema ID prefix. According to [Confluent Kafka's
  /// document](https://docs.confluent.io/platform/current/schema-registry/fundamentals/serdes-develop/index.html#wire-format).
  strip_schema_id_prefix: Boolean
}

/// Decodes the raw bytes as a string and passes them as input to a [VRL][vrl] program.
///
/// Config used to build a `VrlDeserializer`.
class DecodingAlternate1Alternate8 {
  /// Decodes the raw bytes as a string and passes them as input to a [VRL][vrl] program.
  ///
  /// [vrl]: https://vector.dev/docs/reference/vrl
  codec: "vrl"
}

/// Byte frames are passed through as-is according to the underlying I/O boundaries (for example, split
/// between messages or stream segments).
class `Core::option::Option<codecs::decoding::FramingConfig>Alternate1Alternate0` {
  /// Byte frames are passed through as-is according to the underlying I/O boundaries (for example, split
  /// between messages or stream segments).
  method: "bytes"
}

/// Config used to build a `CharacterDelimitedDecoder`.
class `Core::option::Option<codecs::decoding::FramingConfig>Alternate1Alternate1` {
  /// Byte frames which are delimited by a chosen character.
  method: "character_delimited"
}

/// Config used to build a `LengthDelimitedDecoder`.
class `Core::option::Option<codecs::decoding::FramingConfig>Alternate1Alternate2` {
  /// Byte frames which are prefixed by an unsigned big-endian 32-bit integer indicating the length.
  method: "length_delimited"
}

/// Config used to build a `NewlineDelimitedDecoder`.
class `Core::option::Option<codecs::decoding::FramingConfig>Alternate1Alternate3` {
  /// Byte frames which are delimited by a newline character.
  method: "newline_delimited"
}

/// Byte frames according to the [octet counting][octet_counting] format.
///
/// Config used to build a `OctetCountingDecoder`.
class `Core::option::Option<codecs::decoding::FramingConfig>Alternate1Alternate4` {
  /// Byte frames according to the [octet counting][octet_counting] format.
  ///
  /// [octet_counting]: https://tools.ietf.org/html/rfc6587#section-3.4.1
  method: "octet_counting"
}

/// Config used to build a `CsvSerializer`.
class `Codecs::encoding::format::csv::CsvSerializerConfig` {
  /// The CSV Serializer Options.
  csv: Csv
}

/// The CSV Serializer Options.
class Csv {
  /// Set the capacity (in bytes) of the internal buffer used in the CSV writer. This defaults to a
  /// reasonable setting.
  ///
  /// Default if undefined: `8192`
  capacity: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The field delimiter to use when writing CSV.
  ///
  /// Default if undefined: `44`
  delimiter: UInt8?

  /// Enable double quote escapes.
  ///
  /// This is enabled by default, but it may be disabled. When disabled, quotes in field data are escaped
  /// instead of doubled.
  ///
  /// Default if undefined: `true`
  double_quote: Boolean?

  /// The escape character to use when writing CSV.
  ///
  /// In some variants of CSV, quotes are escaped using a special escape character like \ (instead of
  /// escaping quotes by doubling them).
  ///
  /// To use this, `double_quotes` needs to be disabled as well otherwise it is ignored.
  ///
  /// Default if undefined: `34`
  escape: UInt8?

  /// Configures the fields that will be encoded, as well as the order in which they appear in the
  /// output.
  ///
  /// If a field is not present in the event, the output will be an empty string.
  ///
  /// Values of type `Array`, `Object`, and `Regex` are not supported and the output will be an empty
  /// string.
  fields: Listing<Any>

  /// The quote character to use when writing CSV.
  ///
  /// Default if undefined: `34`
  quote: UInt8?

  /// The quoting style to use when writing CSV data.
  ///
  /// Default if undefined: `"necessary"`
  quote_style: ("always"|"necessary"|"non_numeric"|"never")?
}

/// Config used to build a `JsonSerializer`.
class `Codecs::encoding::format::json::JsonSerializerConfig` {
  /// Options for the JsonSerializer.
  ///
  /// Default if undefined: `{ ["pretty"] = false }`
  json: Json?

  /// Controls how metric tag values are encoded.
  ///
  /// When set to `single`, only the last non-bare value of tags are displayed with the metric. When set
  /// to `full`, all metric tags are exposed as separate assignments.
  ///
  /// Default if undefined: `"single"`
  metric_tag_values: Any?
}

/// Options for the JsonSerializer.
///
/// Default if undefined: `{ ["pretty"] = false }`
class Json {
  /// Whether to use pretty JSON formatting.
  ///
  /// Default if undefined: `false`
  pretty: Boolean?
}

/// Matches an event if it is a log.
class `Vector::conditions::AnyConditionAlternate1Alternate0` {
  /// Matches an event if it is a log.
  type: "is_log"
}

/// Matches an event if it is a metric.
class `Vector::conditions::AnyConditionAlternate1Alternate1` {
  /// Matches an event if it is a metric.
  type: "is_metric"
}

/// Matches an event if it is a trace.
class `Vector::conditions::AnyConditionAlternate1Alternate2` {
  /// Matches an event if it is a trace.
  type: "is_trace"
}

/// A condition that uses the [Vector Remap Language](https://vector.dev/docs/reference/vrl) (VRL)
/// [boolean expression](https://vector.dev/docs/reference/vrl#boolean-expressions) against an event.
class `Vector::conditions::AnyConditionAlternate1Alternate3` {
  /// Matches an event with a [Vector Remap Language](https://vector.dev/docs/reference/vrl) (VRL)
  /// [boolean expression](https://vector.dev/docs/reference/vrl#boolean-expressions).
  type: "vrl"
}

/// A condition that uses the [Datadog Search](https://docs.datadoghq.com/logs/explorer/search_syntax/)
/// query syntax against an event.
class `Vector::conditions::AnyConditionAlternate1Alternate4` {
  /// Matches an event with a [Datadog Search](https://docs.datadoghq.com/logs/explorer/search_syntax/)
  /// query.
  type: "datadog_search"
}

/// Basic authentication.
///
/// The username and password are concatenated and encoded via [base64][base64].
///
/// [base64]: https://en.wikipedia.org/wiki/Base64
class `Core::option::Option<vector::http::Auth>Alternate1Alternate0` {
  /// The basic authentication password.
  password: Any

  /// Basic authentication.
  ///
  /// The username and password are concatenated and encoded via [base64][base64].
  ///
  /// [base64]: https://en.wikipedia.org/wiki/Base64
  strategy: "basic"

  /// The basic authentication username.
  user: String
}

/// Bearer authentication.
///
/// The bearer token value (OAuth2, JWT, etc.) is passed as-is.
class `Core::option::Option<vector::http::Auth>Alternate1Alternate1` {
  /// Bearer authentication.
  ///
  /// The bearer token value (OAuth2, JWT, etc.) is passed as-is.
  strategy: "bearer"

  /// The bearer authentication token.
  token: Any
}

/// Username/password authentication.
class `Core::option::Option<vector::nats::NatsAuthConfig>Alternate1Alternate0` {
  /// Username/password authentication.
  strategy: "user_password"

  /// Username and password configuration.
  user_password: UserPassword
}

/// Username and password configuration.
class UserPassword {
  /// Password.
  password: Any

  /// Username.
  user: String
}

/// Token authentication.
class `Core::option::Option<vector::nats::NatsAuthConfig>Alternate1Alternate1` {
  /// Token authentication.
  strategy: "token"

  /// Token configuration.
  token: Token
}

/// Token configuration.
class Token {
  /// Token.
  value: Any
}

/// Credentials file authentication. (JWT-based)
class `Core::option::Option<vector::nats::NatsAuthConfig>Alternate1Alternate2` {
  /// Credentials file configuration.
  credentials_file: CredentialsFile

  /// Credentials file authentication. (JWT-based)
  strategy: "credentials_file"
}

/// Credentials file configuration.
class CredentialsFile {
  /// Path to credentials file.
  path: String
}

/// NKey authentication.
class `Core::option::Option<vector::nats::NatsAuthConfig>Alternate1Alternate3` {
  /// NKeys configuration.
  nkey: Nkey

  /// NKey authentication.
  strategy: "nkey"
}

/// NKeys configuration.
class Nkey {
  /// User.
  ///
  /// Conceptually, this is equivalent to a public key.
  nkey: String

  /// Seed.
  ///
  /// Conceptually, this is equivalent to a private key.
  seed: String
}

/// Configuration settings for InfluxDB v0.x/v1.x.
class `Core::option::Option<vector::sinks::influxdb::InfluxDb1Settings>Alternate1` {
  /// The consistency level to use for writes.
  ///
  /// Only relevant when using InfluxDB v0.x/v1.x.
  consistency: String?

  /// The name of the database to write into.
  ///
  /// Only relevant when using InfluxDB v0.x/v1.x.
  database: String

  /// The password to authenticate with.
  ///
  /// Only relevant when using InfluxDB v0.x/v1.x.
  password: Any?

  /// The target retention policy for writes.
  ///
  /// Only relevant when using InfluxDB v0.x/v1.x.
  retention_policy_name: String?

  /// The username to authenticate with.
  ///
  /// Only relevant when using InfluxDB v0.x/v1.x.
  username: String?
}

/// Configuration settings for InfluxDB v2.x.
class `Core::option::Option<vector::sinks::influxdb::InfluxDb2Settings>Alternate1` {
  /// The name of the bucket to write into.
  ///
  /// Only relevant when using InfluxDB v2.x and above.
  bucket: String

  /// The name of the organization to write into.
  ///
  /// Only relevant when using InfluxDB v2.x and above.
  org: String

  /// The [token][token_docs] to authenticate with.
  ///
  /// Only relevant when using InfluxDB v2.x and above.
  ///
  /// [token_docs]: https://v2.docs.influxdata.com/v2.0/security/tokens/
  token: Any
}

/// Compression algorithm and compression level.
class `Vector::sinks::util::buffer::compression::CompressionAlternate1` {
  /// Compression algorithm.
  algorithm: "none"|"gzip"|"zlib"|"zstd"|"snappy"

  /// Compression level.
  level: Any(this == "none" || this == "fast" || this == "best" || this == "default" || this == 0 || this == 1 || this == 2 || this == 3 || this == 4 || this == 5 || this == 6 || this == 7 || this == 8 || this == 9 || this == 10 || this == 11 || this == 12 || this == 13 || this == 14 || this == 15 || this == 16 || this == 17 || this == 18 || this == 19 || this == 20 || this == 21)?
}

/// HTTP Basic authentication configuration.
class `Core::option::Option<vector::sources::util::http::auth::HttpSourceAuthConfig>Alternate1` {
  /// The password for basic authentication.
  password: Any

  /// The username for basic authentication.
  username: String
}

/// Configuration of multi-line aggregation.
class `Core::option::Option<vector::sources::util::multilineConfig::MultilineConfig>Alternate1` {
  /// Regular expression pattern that is used to determine whether or not more lines should be read.
  ///
  /// This setting must be configured in conjunction with `mode`.
  condition_pattern: String

  /// Aggregation mode.
  ///
  /// This setting must be configured in conjunction with `condition_pattern`.
  mode: "continue_through"|"continue_past"|"halt_before"|"halt_with"

  /// Regular expression pattern that is used to match the start of a new message.
  start_pattern: String

  /// The maximum amount of time to wait for the next additional line, in milliseconds.
  ///
  /// Once this timeout is reached, the buffered message is guaranteed to be flushed, even if incomplete.
  timeout_ms: Any
}

/// TCP keepalive settings for socket-based components.
class `Core::option::Option<vectorCore::tcp::TcpKeepaliveConfig>Alternate1` {
  /// The time to wait before starting to send TCP keepalive probes on an idle connection.
  time_secs: Number(isBetween(0.0, 9.007199254740991E15))?
}

/// Configures the TLS options for incoming/outgoing connections.
class `Core::option::Option<vectorCore::tls::settings::TlsSourceConfig>Alternate1` {
  /// Event field for client certificate metadata.
  client_metadata_key: Any?
}

/// AMQP connection options.
class `Vector::amqp::AmqpConfig` {
  /// URI for the AMQP server.
  ///
  /// The URI has the format of `amqp://<user>:<password>@<host>:<port>/<vhost>?timeout=<seconds>`.
  ///
  /// The default vhost can be specified by using a value of `%2f`.
  ///
  /// To connect over TLS, a scheme of `amqps` can be specified instead. For example, `amqps://...`.
  /// Additional TLS settings, such as client certificate verification, can be configured under the `tls`
  /// section.
  connection_string: String

  tls: Any?
}

/// Authenticate using a fixed access key and secret pair.
class AuthAlternate1Alternate2Alternate0 {
  /// The AWS access key ID.
  access_key_id: Any

  /// The ARN of an [IAM role][iam_role] to assume.
  ///
  /// [iam_role]: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html
  assume_role: String?

  /// The optional unique external ID in conjunction with role to assume.
  ///
  /// [external_id]:
  /// https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html
  external_id: String?

  /// The [AWS region][aws_region] to send STS requests to.
  ///
  /// If not set, this will default to the configured region for the service itself.
  ///
  /// [aws_region]: https://docs.aws.amazon.com/general/latest/gr/rande.html#regional-endpoints
  region: String?

  /// The AWS secret access key.
  secret_access_key: Any
}

/// Authenticate using credentials stored in a file.
///
/// Additionally, the specific credential profile to use can be set. The file format must match the
/// credentials file format outlined in
/// <https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html>.
class AuthAlternate1Alternate2Alternate1 {
  /// Path to the credentials file.
  credentials_file: String

  /// The credentials profile to use.
  ///
  /// Used to select AWS credentials from a provided credentials file.
  ///
  /// Default if undefined: `"default"`
  profile: String?
}

/// Assume the given role ARN.
class AuthAlternate1Alternate2Alternate2 {
  /// The ARN of an [IAM role][iam_role] to assume.
  ///
  /// [iam_role]: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html
  assume_role: String

  /// The optional unique external ID in conjunction with role to assume.
  ///
  /// [external_id]:
  /// https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html
  external_id: String?

  /// Configuration for authenticating with AWS through IMDS.
  ///
  /// Default if undefined: `{ ["max_attempts"] = 4 ["connect_timeout_seconds"] = 1
  /// ["read_timeout_seconds"] = 1 }`
  imds: AuthAlternate1Alternate2Alternate3Imds?

  /// Timeout for assuming the role, in seconds.
  ///
  /// Relevant when the default credentials chain or `assume_role` is used.
  load_timeout_secs: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The [AWS region][aws_region] to send STS requests to.
  ///
  /// If not set, this defaults to the configured region for the service itself.
  ///
  /// [aws_region]: https://docs.aws.amazon.com/general/latest/gr/rande.html#regional-endpoints
  region: String?
}

/// Configuration for authenticating with AWS through IMDS.
///
/// Default if undefined: `{ ["max_attempts"] = 4 ["connect_timeout_seconds"] = 1
/// ["read_timeout_seconds"] = 1 }`
class AuthAlternate1Alternate2Alternate3Imds {
  /// Connect timeout for IMDS.
  ///
  /// Default if undefined: `1`
  connect_timeout_seconds: Any?

  /// Number of IMDS retries for fetching tokens and metadata.
  ///
  /// Default if undefined: `4`
  max_attempts: UInt32?

  /// Read timeout for IMDS.
  ///
  /// Default if undefined: `1`
  read_timeout_seconds: Any?
}

/// Default authentication strategy which tries a variety of substrategies in sequential order.
class AuthAlternate1Alternate2Alternate3 {
  /// Configuration for authenticating with AWS through IMDS.
  ///
  /// Default if undefined: `{ ["max_attempts"] = 4 ["connect_timeout_seconds"] = 1
  /// ["read_timeout_seconds"] = 1 }`
  imds: AuthAlternate1Alternate2Alternate3Imds?

  /// Timeout for successfully loading any credentials, in seconds.
  ///
  /// Relevant when the default credentials chain or `assume_role` is used.
  load_timeout_secs: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The [AWS region][aws_region] to send STS requests to.
  ///
  /// If not set, this defaults to the configured region for the service itself.
  ///
  /// [aws_region]: https://docs.aws.amazon.com/general/latest/gr/rande.html#regional-endpoints
  region: String?
}

/// Configuration of the region/endpoint to use when interacting with an AWS service.
///
/// Default if undefined: `{ ["region"] = null ["endpoint"] = null }`
class `Vector::aws::region::RegionOrEndpoint` {
  /// Custom endpoint for use with AWS-compatible services.
  endpoint: String?

  /// The [AWS region][aws_region] of the target service.
  ///
  /// [aws_region]: https://docs.aws.amazon.com/general/latest/gr/rande.html#regional-endpoints
  region: String?
}

/// Transformations to prepare an event for serialization.
class `Vector::codecs::encoding::config::EncodingConfig` {
  /// List of fields that are excluded from the encoded event.
  except_fields: Listing<Any>?

  /// List of fields that are included in the encoded event.
  only_fields: Listing<Any>?

  /// Format used for timestamp fields.
  timestamp_format: (Any|"unix"|"rfc3339"|"unix_ms"|"unix_us"|"unix_ns"|"unix_float")?
}

/// Encodes an event as an [Apache Avro][apache_avro] message.
///
/// [apache_avro]: https://avro.apache.org/
class `Vector::codecs::encoding::config::EncodingConfigAlternate0` {
  /// Apache Avro-specific encoder options.
  avro: `Vector::codecs::encoding::config::EncodingConfigAlternate0Avro`

  /// Encodes an event as an [Apache Avro][apache_avro] message.
  ///
  /// [apache_avro]: https://avro.apache.org/
  codec: "avro"
}

/// Apache Avro-specific encoder options.
class `Vector::codecs::encoding::config::EncodingConfigAlternate0Avro` {
  /// The Avro schema.
  schema: String
}

/// Encodes an event as a CSV message.
///
/// Config used to build a `CsvSerializer`.
class `Vector::codecs::encoding::config::EncodingConfigAlternate1` {
  /// Encodes an event as a CSV message.
  ///
  /// This codec must be configured with fields to encode.
  codec: "csv"
}

/// Encodes an event as a [GELF][gelf] message.
///
/// This codec is experimental for the following reason:
///
/// The GELF specification is more strict than the actual Graylog receiver. Vector's encoder currently
/// adheres more strictly to the GELF spec, with the exception that some characters such as `@` are
/// allowed in field names.
///
/// Other GELF codecs such as Loki's, use a [Go SDK][implementation] that is maintained by Graylog, and
/// is much more relaxed than the GELF spec.
///
/// Going forward, Vector will use that [Go SDK][implementation] as the reference implementation, which
/// means the codec may continue to relax the enforcement of specification.
///
/// [gelf]: https://docs.graylog.org/docs/gelf [implementation]:
/// https://github.com/Graylog2/go-gelf/blob/v2/gelf/reader.go
class `Vector::codecs::encoding::config::EncodingConfigAlternate2` {
  /// Encodes an event as a [GELF][gelf] message.
  ///
  /// This codec is experimental for the following reason:
  ///
  /// The GELF specification is more strict than the actual Graylog receiver. Vector's encoder currently
  /// adheres more strictly to the GELF spec, with the exception that some characters such as `@` are
  /// allowed in field names.
  ///
  /// Other GELF codecs such as Loki's, use a [Go SDK][implementation] that is maintained by Graylog, and
  /// is much more relaxed than the GELF spec.
  ///
  /// Going forward, Vector will use that [Go SDK][implementation] as the reference implementation, which
  /// means the codec may continue to relax the enforcement of specification.
  ///
  /// [gelf]: https://docs.graylog.org/docs/gelf [implementation]:
  /// https://github.com/Graylog2/go-gelf/blob/v2/gelf/reader.go
  codec: "gelf"
}

/// Encodes an event as [JSON][json].
///
/// Config used to build a `JsonSerializer`.
class `Vector::codecs::encoding::config::EncodingConfigAlternate3` {
  /// Encodes an event as [JSON][json].
  ///
  /// [json]: https://www.json.org/
  codec: "json"
}

/// Encodes an event as a [logfmt][logfmt] message.
///
/// [logfmt]: https://brandur.org/logfmt
class `Vector::codecs::encoding::config::EncodingConfigAlternate4` {
  /// Encodes an event as a [logfmt][logfmt] message.
  ///
  /// [logfmt]: https://brandur.org/logfmt
  codec: "logfmt"
}

/// Encodes an event in the [native Protocol Buffers format][vector_native_protobuf].
///
/// This codec is **[experimental][experimental]**.
///
/// [vector_native_protobuf]:
/// https://github.com/vectordotdev/vector/blob/master/lib/vector-core/proto/event.proto [experimental]:
/// https://vector.dev/highlights/2022-03-31-native-event-codecs
class `Vector::codecs::encoding::config::EncodingConfigAlternate5` {
  /// Encodes an event in the [native Protocol Buffers format][vector_native_protobuf].
  ///
  /// This codec is **[experimental][experimental]**.
  ///
  /// [vector_native_protobuf]:
  /// https://github.com/vectordotdev/vector/blob/master/lib/vector-core/proto/event.proto
  /// [experimental]: https://vector.dev/highlights/2022-03-31-native-event-codecs
  codec: "native"
}

/// Encodes an event in the [native JSON format][vector_native_json].
///
/// This codec is **[experimental][experimental]**.
///
/// [vector_native_json]:
/// https://github.com/vectordotdev/vector/blob/master/lib/codecs/tests/data/native_encoding/schema.cue
/// [experimental]: https://vector.dev/highlights/2022-03-31-native-event-codecs
class `Vector::codecs::encoding::config::EncodingConfigAlternate6` {
  /// Encodes an event in the [native JSON format][vector_native_json].
  ///
  /// This codec is **[experimental][experimental]**.
  ///
  /// [vector_native_json]:
  /// https://github.com/vectordotdev/vector/blob/master/lib/codecs/tests/data/native_encoding/schema.cue
  /// [experimental]: https://vector.dev/highlights/2022-03-31-native-event-codecs
  codec: "native_json"
}

/// Encodes an event as a [Protobuf][protobuf] message.
///
/// Config used to build a `ProtobufSerializer`.
class `Vector::codecs::encoding::config::EncodingConfigAlternate7` {
  /// Encodes an event as a [Protobuf][protobuf] message.
  ///
  /// [protobuf]: https://protobuf.dev/
  codec: "protobuf"
}

/// No encoding.
///
/// This encoding uses the `message` field of a log event.
///
/// Be careful if you are modifying your log events (for example, by using a `remap` transform) and
/// removing the message field while doing additional parsing on it, as this could lead to the encoding
/// emitting empty strings for the given event.
class `Vector::codecs::encoding::config::EncodingConfigAlternate8` {
  /// No encoding.
  ///
  /// This encoding uses the `message` field of a log event.
  ///
  /// Be careful if you are modifying your log events (for example, by using a `remap` transform) and
  /// removing the message field while doing additional parsing on it, as this could lead to the encoding
  /// emitting empty strings for the given event.
  codec: "raw_message"
}

/// Plain text encoding.
///
/// Config used to build a `TextSerializer`.
class `Vector::codecs::encoding::config::EncodingConfigAlternate9` {
  /// Plain text encoding.
  ///
  /// This encoding uses the `message` field of a log event. For metrics, it uses an encoding that
  /// resembles the Prometheus export format.
  ///
  /// Be careful if you are modifying your log events (for example, by using a `remap` transform) and
  /// removing the message field while doing additional parsing on it, as this could lead to the encoding
  /// emitting empty strings for the given event.
  codec: "text"
}

/// Encoding configuration.
class `Vector::codecs::encoding::config::EncodingConfigWithFraming` {
  /// Configures how events are encoded into raw bytes.
  encoding: Any

  /// Framing configuration.
  framing: (
    Any
    |
    FramingAlternate1Alternate0
    |FramingAlternate1Alternate1
    |FramingAlternate1Alternate2
    |FramingAlternate1Alternate3)?
}

/// Event data is not delimited at all.
class FramingAlternate1Alternate0 {
  /// Event data is not delimited at all.
  method: "bytes"
}

/// Config used to build a `CharacterDelimitedEncoder`.
class FramingAlternate1Alternate1 {
  /// Event data is delimited by a single ASCII (7-bit) character.
  method: "character_delimited"
}

/// Event data is prefixed with its length in bytes.
///
/// Config used to build a `LengthDelimitedEncoder`.
class FramingAlternate1Alternate2 {
  /// Event data is prefixed with its length in bytes.
  ///
  /// The prefix is a 32-bit unsigned integer, little endian.
  method: "length_delimited"
}

/// Event data is delimited by a newline (LF) character.
class FramingAlternate1Alternate3 {
  /// Event data is delimited by a newline (LF) character.
  method: "newline_delimited"
}

/// Transformations to prepare an event for serialization.
class `Vector::codecs::encoding::transformer::Transformer` {
  /// List of fields that are excluded from the encoded event.
  except_fields: Listing<Any>?

  /// List of fields that are included in the encoded event.
  only_fields: Listing<Any>?

  /// Format used for timestamp fields.
  timestamp_format: (Any|"unix"|"rfc3339"|"unix_ms"|"unix_us"|"unix_ns"|"unix_float")?
}

/// A unit test input.
///
/// An input describes not only the type of event to insert, but also which transform within the
/// configuration to insert it to.
class `Vector::config::TestInput` {
  /// The name of the transform to insert the input event to.
  insert_at: String

  /// The set of log fields to use when creating a log input event.
  ///
  /// Only relevant when `type` is `log`.
  log_fields: Mapping<String, 
    String
    |Int(isBetween(-9.007199254740991E15, 9.007199254740991E15))
    |Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))
    |Boolean>?

  /// The metric to use as an input event.
  ///
  /// Only relevant when `type` is `metric`.
  metric: (Any|MetricAlternate1)?

  /// The vrl expression to generate the input event.
  ///
  /// Only relevant when `type` is `vrl`.
  source: String?

  /// The type of the input event.
  ///
  /// Can be either `raw`, `vrl`, `log`, or `metric.
  ///
  /// Default if undefined: `"raw"`
  type: String?

  /// The raw string value to use as the input event.
  ///
  /// Use this only when the input event should be a raw event (i.e. unprocessed/undecoded log event) and
  /// when the input type is set to `raw`.
  value: String?
}

/// Metric value. Container for the actual value of a metric.
class MetricAlternate1 {
  /// The interval, in milliseconds, of this metric.
  ///
  /// Intervals represent the time window over which this metric applies, and is generally only used for
  /// tracking rates (change over time) on counters.
  interval_ms: Number(isBetween(1.0, 4.294967295E9))?

  /// The timestamp of when the metric was created.
  ///
  /// Metrics may sometimes have no timestamp, or have no meaningful value if the metric is an
  /// aggregation or transformed heavily enough from its original form such that the original timestamp
  /// would not represent a meaningful value.
  timestamp: String?
}

/// A cumulative numerical value that can only increase or be reset to zero.
class MetricAlternate1Alternate0 {
  counter: Counter
}

class Counter {
  /// The value of the counter.
  value: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))
}

/// A single numerical value that can arbitrarily go up and down.
class MetricAlternate1Alternate1 {
  gauge: Gauge
}

class Gauge {
  /// The value of the gauge.
  value: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))
}

/// A set of (unordered) unique values for a key.
class MetricAlternate1Alternate2 {
  set: Set
}

class Set {
  /// The values in the set.
  values: Listing<String>(isDistinct)
}

/// A set of observations without any aggregation or sampling.
class MetricAlternate1Alternate3 {
  distribution: Distribution
}

class Distribution {
  /// The observed values within this distribution.
  samples: Listing<Sample>

  /// The type of statistics to derive for this distribution.
  statistic: "histogram"|"summary"
}

/// A single observation.
class Sample {
  /// The rate at which the value was observed.
  rate: UInt32

  /// The value of the observation.
  value: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))
}

/// A set of observations which are counted into buckets.
///
/// It also contains the total count of all observations and their sum to allow calculating the mean.
class MetricAlternate1Alternate4 {
  aggregated_histogram: AggregatedHistogram
}

class AggregatedHistogram {
  /// A histogram bucket.
  ///
  /// The buckets within this histogram.
  buckets: Listing<Bucket>

  /// The total number of observations contained within this histogram.
  count: Int(isBetween(0.0, 9.007199254740991E15))

  /// The sum of all observations contained within this histogram.
  sum: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))
}

/// A histogram bucket.
///
/// Histogram buckets represent the `count` of observations where the value of the observations does not
/// exceed the specified `upper_limit`.
class Bucket {
  /// The number of values tracked in this bucket.
  count: Int(isBetween(0.0, 9.007199254740991E15))

  /// The upper limit of values in the bucket.
  upper_limit: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))
}

/// A set of observations which are represented by quantiles.
///
/// Each quantile contains the upper value of the quantile (0 <= Ï <= 1). It also contains the total
/// count of all observations and their sum to allow calculating the mean.
class MetricAlternate1Alternate5 {
  aggregated_summary: AggregatedSummary
}

class AggregatedSummary {
  /// The total number of observations contained within this summary.
  count: Int(isBetween(0.0, 9.007199254740991E15))

  /// A single quantile observation.
  ///
  /// The quantiles measured from this summary.
  quantiles: Listing<Quantile>

  /// The sum of all observations contained within this histogram.
  sum: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))
}

/// A single quantile observation.
///
/// Quantiles themselves are "cut points dividing the range of a probability distribution into continuous
/// intervals with equal probabilities". [[1][quantiles_wikipedia]].
///
/// We use quantiles to measure the value along these probability distributions for representing
/// client-side aggregations of distributions, which represent a collection of observations over a
/// specific time window.
///
/// In general, we typically use the term "quantile" to represent the concept of _percentiles_, which
/// deal with whole integers -- 0, 1, 2, .., 99, 100 -- even though quantiles are floating-point numbers
/// and can represent higher-precision cut points, such as 0.9999, or the 99.99th percentile.
///
/// [quantiles_wikipedia]: https://en.wikipedia.org/wiki/Quantile
class Quantile {
  /// The value of the quantile.
  ///
  /// This value must be between 0.0 and 1.0, inclusive.
  quantile: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))

  /// The estimated value of the given quantile within the probability distribution.
  value: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))
}

/// A data structure that can answer questions about the cumulative distribution of the contained samples
/// in space-efficient way.
///
/// Sketches represent the data in a way that queries over it have bounded error guarantees without
/// needing to hold every single sample in memory. They are also, typically, able to be merged with other
/// sketches of the same type such that client-side _and_ server-side aggregation can be accomplished
/// without loss of accuracy in the queries.
class MetricAlternate1Alternate6 {
  sketch: Sketch
}

class Sketch {
  /// A generalized metrics sketch.
  sketch: SketchAlternate0
}

/// [DDSketch][ddsketch] implementation based on the [Datadog Agent][ddagent].
///
/// While DDSketch has open-source implementations based on the white paper, the version used in the
/// Datadog Agent itself is subtly different. This version is suitable for sending directly to Datadog's
/// sketch ingest endpoint.
///
/// [ddsketch]: https://www.vldb.org/pvldb/vol12/p2195-masson.pdf [ddagent]:
/// https://github.com/DataDog/datadog-agent
class SketchAlternate0 {
  /// [DDSketch][ddsketch] implementation based on the [Datadog Agent][ddagent].
  ///
  /// This implementation is subtly different from the open-source implementations of `DDSketch`, as
  /// Datadog made some slight tweaks to configuration values and in-memory layout to optimize it for
  /// insertion performance within the agent.
  ///
  /// We've mimicked the agent version of `DDSketch` here in order to support a future where we can take
  /// sketches shipped by the agent, handle them internally, merge them, and so on, without any loss of
  /// accuracy, eventually forwarding them to Datadog ourselves.
  ///
  /// As such, this implementation is constrained in the same ways: the configuration parameters cannot
  /// be changed, the collapsing strategy is fixed, and we support a limited number of methods for
  /// inserting into the sketch.
  ///
  /// Importantly, we have a special function, again taken from the agent version, to allow us to
  /// interpolate histograms, specifically our own aggregated histograms, into a sketch so that we can
  /// emit useful default quantiles, rather than having to ship the buckets -- upper bound and count --
  /// to a downstream system that might have no native way to do the same thing, basically providing no
  /// value as they have no way to render useful data from them.
  ///
  /// [ddsketch]: https://www.vldb.org/pvldb/vol12/p2195-masson.pdf [ddagent]:
  /// https://github.com/DataDog/datadog-agent
  AgentDDSketch: AgentDDSketch
}

/// [DDSketch][ddsketch] implementation based on the [Datadog Agent][ddagent].
///
/// This implementation is subtly different from the open-source implementations of `DDSketch`, as
/// Datadog made some slight tweaks to configuration values and in-memory layout to optimize it for
/// insertion performance within the agent.
///
/// We've mimicked the agent version of `DDSketch` here in order to support a future where we can take
/// sketches shipped by the agent, handle them internally, merge them, and so on, without any loss of
/// accuracy, eventually forwarding them to Datadog ourselves.
///
/// As such, this implementation is constrained in the same ways: the configuration parameters cannot be
/// changed, the collapsing strategy is fixed, and we support a limited number of methods for inserting
/// into the sketch.
///
/// Importantly, we have a special function, again taken from the agent version, to allow us to
/// interpolate histograms, specifically our own aggregated histograms, into a sketch so that we can emit
/// useful default quantiles, rather than having to ship the buckets -- upper bound and count -- to a
/// downstream system that might have no native way to do the same thing, basically providing no value as
/// they have no way to render useful data from them.
///
/// [ddsketch]: https://www.vldb.org/pvldb/vol12/p2195-masson.pdf [ddagent]:
/// https://github.com/DataDog/datadog-agent
class AgentDDSketch {
  /// The average value of all observations within the sketch.
  avg: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))

  /// A split representation of sketch bins.
  ///
  /// The bins within the sketch.
  bins: Bins

  /// The number of observations within the sketch.
  count: UInt32

  /// The maximum value of all observations within the sketch.
  max: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))

  /// The minimum value of all observations within the sketch.
  min: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))

  /// The sum of all observations within the sketch.
  sum: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))
}

/// A split representation of sketch bins.
///
/// The bins within the sketch.
class Bins {
  /// The bin keys.
  k: Listing<Int16>

  /// The bin counts.
  n: Listing<UInt16>
}

/// Configurable sinks in Vector.
class `Vector::config::sink::SinkOuter<alloc::string::String>` {
  /// Configures the buffering behavior for this sink.
  ///
  /// More information about the individual buffer types, and buffer behavior, can be found in the
  /// [Buffering Model][buffering_model] section.
  ///
  /// [buffering_model]: /docs/about/under-the-hood/architecture/buffering-model/
  ///
  /// Default if undefined: `{ ["type"] = "memory" ["max_events"] = 500 ["when_full"] = "block" }`
  buffer: (
    BufferAlternate1Alternate0|BufferAlternate1Alternate1
    |Listing<BufferAlternate1Alternate0|BufferAlternate1Alternate1>)?

  /// Healthcheck configuration.
  ///
  /// Default if undefined: `{ ["enabled"] = true ["uri"] = null }`
  healthcheck: Healthcheck?

  /// The full URI to make HTTP healthcheck requests to.
  ///
  /// This must be a valid URI, which requires at least the scheme and host. All other components --
  /// port, path, etc -- are allowed as well.
  @Deprecated
  healthcheck_uri: Any?

  /// A list of upstream [source][sources] or [transform][transforms] IDs.
  ///
  /// Wildcards (`*`) are supported.
  ///
  /// See [configuration][configuration] for more info.
  ///
  /// [sources]: https://vector.dev/docs/reference/configuration/sources/ [transforms]:
  /// https://vector.dev/docs/reference/configuration/transforms/ [configuration]:
  /// https://vector.dev/docs/reference/configuration/
  inputs: Any

  /// Proxy configuration.
  ///
  /// Configure to proxy traffic through an HTTP(S) proxy when making external requests.
  ///
  /// Similar to common proxy configuration convention, you can set different proxies to use based on the
  /// type of traffic being proxied, as well as set specific hosts that should not be proxied.
  ///
  /// Default if undefined: `{}`
  proxy: Any?
}

/// Events are buffered in memory.
///
/// This is more performant, but less durable. Data will be lost if Vector is restarted forcefully or
/// crashes.
class BufferAlternate1Alternate0 {
  /// The maximum number of events allowed in the buffer.
  ///
  /// Default if undefined: `500`
  max_events: Int(isBetween(1.0, 9.007199254740991E15))?

  /// Events are buffered in memory.
  ///
  /// This is more performant, but less durable. Data will be lost if Vector is restarted forcefully or
  /// crashes.
  type: "memory"

  /// Event handling behavior when a buffer is full.
  ///
  /// Default if undefined: `"block"`
  when_full: ("block"|"drop_newest"|"overflow")?
}

/// Events are buffered on disk.
///
/// This is less performant, but more durable. Data that has been synchronized to disk will not be lost
/// if Vector is restarted forcefully or crashes.
///
/// Data is synchronized to disk every 500ms.
class BufferAlternate1Alternate1 {
  /// The maximum size of the buffer on disk.
  ///
  /// Must be at least ~256 megabytes (268435488 bytes).
  max_size: Int(isBetween(2.68435488E8, 9.007199254740991E15))

  /// Events are buffered on disk.
  ///
  /// This is less performant, but more durable. Data that has been synchronized to disk will not be lost
  /// if Vector is restarted forcefully or crashes.
  ///
  /// Data is synchronized to disk every 500ms.
  type: "disk"

  /// Event handling behavior when a buffer is full.
  ///
  /// Default if undefined: `"block"`
  when_full: ("block"|"drop_newest"|"overflow")?
}

/// Healthcheck configuration.
///
/// Default if undefined: `{ ["enabled"] = true ["uri"] = null }`
class Healthcheck {
  /// Whether or not to check the health of the sink when Vector starts up.
  enabled: Boolean?

  /// The full URI to make HTTP healthcheck requests to.
  ///
  /// This must be a valid URI, which requires at least the scheme and host. All other components --
  /// port, path, etc -- are allowed as well.
  uri: Any?
}

/// Configurable sources in Vector.
class `Vector::config::source::SourceOuter` {
  /// Proxy configuration.
  ///
  /// Configure to proxy traffic through an HTTP(S) proxy when making external requests.
  ///
  /// Similar to common proxy configuration convention, you can set different proxies to use based on the
  /// type of traffic being proxied, as well as set specific hosts that should not be proxied.
  ///
  /// Default if undefined: `{}`
  proxy: Any?
}

/// Configurable transforms in Vector.
class `Vector::config::transform::TransformOuter<alloc::string::String>` {
  /// A list of upstream [source][sources] or [transform][transforms] IDs.
  ///
  /// Wildcards (`*`) are supported.
  ///
  /// See [configuration][configuration] for more info.
  ///
  /// [sources]: https://vector.dev/docs/reference/configuration/sources/ [transforms]:
  /// https://vector.dev/docs/reference/configuration/transforms/ [configuration]:
  /// https://vector.dev/docs/reference/configuration/
  inputs: Any
}

/// Configuration for the `unit_test` sink.
class `Vector::config::unitTest::unitTestComponents::UnitTestSinkConfig` {
  /// Name of the test that this sink is being used for.
  test_name: String

  /// List of names of the transform/branch associated with this sink.
  transform_ids: Listing<String>
}

/// Configuration for the `file` enrichment table.
class `Vector::enrichmentTables::file::FileConfig` {
  /// File-specific settings.
  file: File

  /// Key/value pairs representing mapped log field names and types.
  ///
  /// This is used to coerce log fields from strings into their proper types. The available types are
  /// listed in the `Types` list below.
  ///
  /// Timestamp coercions need to be prefaced with `timestamp|`, for example `"timestamp|%F"`. Timestamp
  /// specifiers can use either of the following:
  ///
  /// 1. One of the built-in-formats listed in the `Timestamp Formats` table below. 2. The [time format
  /// specifiers][chrono_fmt] from Rustâs `chrono` library.
  ///
  /// ### Types
  ///
  /// - **`bool`** - **`string`** - **`float`** - **`integer`** - **`date`** - **`timestamp`** (see the
  /// table below for formats)
  ///
  /// ### Timestamp Formats
  ///
  /// | Format | Description | Example |
  /// |----------------------|----------------------------------------------------------------------------------|----------------------------------|
  /// | `%F %T` | `YYYY-MM-DD HH:MM:SS` | `2020-12-01 02:37:54` | | `%v %T` | `DD-Mmm-YYYY HH:MM:SS` |
  /// `01-Dec-2020 02:37:54` | | `%FT%T` | [ISO 8601][iso8601]/[RFC 3339][rfc3339], without time zone |
  /// `2020-12-01T02:37:54` | | `%FT%TZ` | [ISO 8601][iso8601]/[RFC 3339][rfc3339], UTC |
  /// `2020-12-01T09:37:54Z` | | `%+` | [ISO 8601][iso8601]/[RFC 3339][rfc3339], UTC, with time zone |
  /// `2020-12-01T02:37:54-07:00` | | `%a, %d %b %Y %T` | [RFC 822][rfc822]/[RFC 2822][rfc2822], without
  /// time zone | `Tue, 01 Dec 2020 02:37:54` | | `%a %b %e %T %Y` | [ctime][ctime] format | `Tue Dec 1
  /// 02:37:54 2020` | | `%s` | [UNIX timestamp][unix_ts] | `1606790274` | | `%a %d %b %T %Y` |
  /// [date][date] command, without time zone | `Tue 01 Dec 02:37:54 2020` | | `%a %d %b %T %Z %Y` |
  /// [date][date] command, with time zone | `Tue 01 Dec 02:37:54 PST 2020` | | `%a %d %b %T %z %Y` |
  /// [date][date] command, with numeric time zone | `Tue 01 Dec 02:37:54 -0700 2020` | | `%a %d %b %T
  /// %#z %Y` | [date][date] command, with numeric time zone (minutes can be missing or present) | `Tue
  /// 01 Dec 02:37:54 -07 2020` |
  ///
  /// [date]: https://man7.org/linux/man-pages/man1/date.1.html [ctime]:
  /// https://www.cplusplus.com/reference/ctime [unix_ts]: https://en.wikipedia.org/wiki/Unix_time
  /// [rfc822]: https://tools.ietf.org/html/rfc822#section-5 [rfc2822]:
  /// https://tools.ietf.org/html/rfc2822#section-3.3 [iso8601]: https://en.wikipedia.org/wiki/ISO_8601
  /// [rfc3339]: https://tools.ietf.org/html/rfc3339 [chrono_fmt]:
  /// https://docs.rs/chrono/latest/chrono/format/strftime/index.html#specifiers
  ///
  /// Default if undefined: `{}`
  schema: Mapping<String, String>?
}

/// File-specific settings.
class File {
  /// File encoding configuration.
  encoding: EncodingAlternate0

  /// The path of the enrichment table file.
  ///
  /// Currently, only [CSV][csv] files are supported.
  ///
  /// [csv]: https://en.wikipedia.org/wiki/Comma-separated_values
  path: Any
}

/// Decodes the file as a [CSV][csv] (comma-separated values) file.
///
/// [csv]: https://wikipedia.org/wiki/Comma-separated_values
class EncodingAlternate0 {
  /// The delimiter used to separate fields in each row of the CSV file.
  ///
  /// Default if undefined: `","`
  delimiter: String(length.isBetween(1, 1))?

  /// Whether or not the file contains column headers.
  ///
  /// When set to `true`, the first row of the CSV file will be read as the header row, and the values
  /// will be used for the names of each column. This is the default behavior.
  ///
  /// When set to `false`, columns are referred to by their numerical index.
  ///
  /// Default if undefined: `true`
  include_headers: Boolean?

  /// Decodes the file as a [CSV][csv] (comma-separated values) file.
  ///
  /// [csv]: https://wikipedia.org/wiki/Comma-separated_values
  type: "csv"
}

/// Configuration for the `geoip` enrichment table.
class `Vector::enrichmentTables::geoip::GeoipConfig` {
  /// The locale to use when querying the database.
  ///
  /// MaxMind includes localized versions of some of the fields within their database, such as country
  /// name. This setting can control which of those localized versions are returned by the transform.
  ///
  /// More information on which portions of the geolocation data are localized, and what languages are
  /// available, can be found [here][locale_docs].
  ///
  /// [locale_docs]:
  /// https://support.maxmind.com/hc/en-us/articles/4414877149467-IP-Geolocation-Data#h_01FRRGRYTGZB29ERDBZCX3MR8Q
  ///
  /// Default if undefined: `"en"`
  locale: String?

  /// Path to the [MaxMind GeoIP2][geoip2] or [GeoLite2 binary city database file][geolite2]
  /// (**GeoLite2-City.mmdb**).
  ///
  /// Other databases, such as the country database, are not supported. `mmdb` enrichment table can be
  /// used for other databases.
  ///
  /// [geoip2]: https://dev.maxmind.com/geoip/geoip2/downloadable [geolite2]:
  /// https://dev.maxmind.com/geoip/geoip2/geolite2/#Download_Access
  path: String
}

/// Configuration for the `mmdb` enrichment table.
class `Vector::enrichmentTables::mmdb::MmdbConfig` {
  /// Path to the [MaxMind][maxmind] database
  ///
  /// [maxmind]: https://maxmind.com
  path: String
}

/// Configuration of the authentication strategy for interacting with GCP services.
class `Vector::gcp::GcpAuthConfig` {
  /// An [API key][gcp_api_key].
  ///
  /// Either an API key or a path to a service account credentials JSON file can be specified.
  ///
  /// If both are unset, the `GOOGLE_APPLICATION_CREDENTIALS` environment variable is checked for a
  /// filename. If no filename is named, an attempt is made to fetch an instance service account for the
  /// compute instance the program is running on. If this is not on a GCE instance, then you must define
  /// it with an API key or service account credentials JSON file.
  ///
  /// [gcp_api_key]: https://cloud.google.com/docs/authentication/api-keys
  api_key: Any?

  /// Path to a [service account][gcp_service_account_credentials] credentials JSON file.
  ///
  /// Either an API key or a path to a service account credentials JSON file can be specified.
  ///
  /// If both are unset, the `GOOGLE_APPLICATION_CREDENTIALS` environment variable is checked for a
  /// filename. If no filename is named, an attempt is made to fetch an instance service account for the
  /// compute instance the program is running on. If this is not on a GCE instance, then you must define
  /// it with an API key or service account credentials JSON file.
  ///
  /// [gcp_service_account_credentials]: https://cloud.google.com/docs/authentication/production#manually
  credentials_path: String?

  /// Skip all authentication handling. For use with integration tests only.
  ///
  /// Default if undefined: `false`
  skip_authentication: Boolean?
}

/// Configuration of HTTP server keepalive parameters.
class `Vector::http::KeepaliveConfig` {
  /// The factor by which to jitter the `max_connection_age_secs` value.
  ///
  /// A value of 0.1 means that the actual duration will be between 90% and 110% of the specified maximum
  /// duration.
  ///
  /// Default if undefined: `0.1`
  max_connection_age_jitter_factor: Number(isBetween(0.0, 1.0))?

  /// The maximum amount of time a connection may exist before it is closed by sending a `Connection:
  /// close` header on the HTTP response. Set this to a large value like `100000000` to "disable" this
  /// feature
  ///
  /// Only applies to HTTP/0.9, HTTP/1.0, and HTTP/1.1 requests.
  ///
  /// A random jitter configured by `max_connection_age_jitter_factor` is added to the specified duration
  /// to spread out connection storms.
  ///
  /// Default if undefined: `300`
  max_connection_age_secs: Number(isBetween(0.0, 9.007199254740991E15))?
}

/// Configuration of internal metrics for file-based components.
class `Vector::internalEvents::file::FileInternalMetricsConfig` {
  /// Whether or not to include the "file" tag on the component's corresponding internal metrics.
  ///
  /// This is useful for distinguishing between different files while monitoring. However, the tag's
  /// cardinality is unbounded.
  ///
  /// Default if undefined: `false`
  include_file_tag: Boolean?
}

/// Kafka authentication configuration.
class `Vector::kafka::KafkaAuthConfig` {
  /// Configuration for SASL authentication when interacting with Kafka.
  sasl: (Any|SaslAlternate1)?

  tls: Any?
}

/// Configuration for SASL authentication when interacting with Kafka.
class SaslAlternate1 {
  /// Enables SASL authentication.
  ///
  /// Only `PLAIN`- and `SCRAM`-based mechanisms are supported when configuring SASL authentication using
  /// `sasl.*`. For other mechanisms, `librdkafka_options.*` must be used directly to configure other
  /// `librdkafka`-specific values. If using `sasl.kerberos.*` as an example, where `*` is
  /// `service.name`, `principal`, `kinit.md`, etc., then `librdkafka_options.*` as a result becomes
  /// `librdkafka_options.sasl.kerberos.service.name`, `librdkafka_options.sasl.kerberos.principal`, etc.
  ///
  /// See the [librdkafka
  /// documentation](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md) for details.
  ///
  /// SASL authentication is not supported on Windows.
  enabled: Boolean?

  /// The SASL mechanism to use.
  mechanism: String?

  /// The SASL password.
  password: Any?

  /// The SASL username.
  username: String?
}

/// TLS configuration.
///
/// Default if undefined: `{ ["url"] = null ["request"] { ["headers"] {} } ["poll_interval_secs"] = 30 }`
class `Vector::providers::http::HttpConfig` {
  /// How often to poll the provider, in seconds.
  poll_interval_secs: Int(isBetween(0.0, 9.007199254740991E15))?

  /// Proxy configuration.
  ///
  /// Configure to proxy traffic through an HTTP(S) proxy when making external requests.
  ///
  /// Similar to common proxy configuration convention, you can set different proxies to use based on the
  /// type of traffic being proxied, as well as set specific hosts that should not be proxied.
  ///
  /// Default if undefined: `{}`
  proxy: Any?

  /// Request settings.
  request: Request?

  /// URL for the HTTP provider.
  url: Uri?
}

/// Request settings.
class Request {
  /// HTTP headers to add to the request.
  ///
  /// Default if undefined: `{}`
  headers: Mapping<String, String>?
}

/// Configuration of the region/endpoint to use when interacting with an AWS service.
class `Vector::secrets::awsSecretsManager::AwsSecretsManagerBackend` {
  /// Configuration of the authentication strategy for interacting with AWS services.
  ///
  /// Default if undefined: `{ ["load_timeout_secs"] = null ["imds"] { ["max_attempts"] = 4
  /// ["connect_timeout_seconds"] = 1 ["read_timeout_seconds"] = 1 } ["region"] = null }`
  auth: Any?

  /// ID of the secret to resolve.
  secret_id: String

  tls: Any?
}

/// Configuration for the `exec` secrets backend.
class `Vector::secrets::exec::ExecBackend` {
  /// Command arguments to execute.
  ///
  /// The path to the script or binary must be the first argument.
  command: Listing<String>

  /// The timeout, in seconds, to wait for the command to complete.
  ///
  /// Default if undefined: `5`
  timeout: Int(isBetween(0.0, 9.007199254740991E15))?
}

/// Configuration for the `test` secrets backend.
class `Vector::secrets::test::TestBackend` {
  /// Fixed value to replace all secrets with.
  replacement: String
}

/// Configuration for the `amqp` sink.
///
/// Supports AMQP version 0.9.1
class `Vector::sinks::SinksAlternate0` {
  /// Send events to AMQP 0.9.1 compatible brokers like RabbitMQ.
  type: "amqp"
}

/// Configuration for the `appsignal` sink.
class `Vector::sinks::SinksAlternate1` {
  /// Deliver log and metric event data to AppSignal.
  type: "appsignal"
}

/// Configuration for the `aws_cloudwatch_logs` sink.
class `Vector::sinks::SinksAlternate2` {
  /// Publish log events to AWS CloudWatch Logs.
  type: "aws_cloudwatch_logs"
}

/// Configuration for the `aws_cloudwatch_metrics` sink.
class `Vector::sinks::SinksAlternate3` {
  /// Publish metric events to AWS CloudWatch Metrics.
  type: "aws_cloudwatch_metrics"
}

/// Configuration for the `aws_kinesis_firehose` sink.
class `Vector::sinks::SinksAlternate4` {
  /// Publish logs to AWS Kinesis Data Firehose topics.
  type: "aws_kinesis_firehose"
}

/// Configuration for the `aws_kinesis_streams` sink.
class `Vector::sinks::SinksAlternate5` {
  /// Publish logs to AWS Kinesis Streams topics.
  type: "aws_kinesis_streams"
}

/// Configuration for the `aws_s3` sink.
class `Vector::sinks::SinksAlternate6` {
  /// Store observability events in the AWS S3 object storage system.
  type: "aws_s3"
}

/// Configuration for the `aws_sns` sink.
class `Vector::sinks::SinksAlternate7` {
  /// Publish observability events to AWS Simple Notification Service topics.
  type: "aws_sns"
}

/// Configuration for the `aws_sqs` sink.
class `Vector::sinks::SinksAlternate8` {
  /// Publish observability events to AWS Simple Queue Service topics.
  type: "aws_sqs"
}

/// Configuration for the `axiom` sink.
class `Vector::sinks::SinksAlternate9` {
  /// Deliver log events to Axiom.
  type: "axiom"
}

/// Configuration for the `azure_blob` sink.
class `Vector::sinks::SinksAlternate10` {
  /// Store your observability data in Azure Blob Storage.
  type: "azure_blob"
}

/// Configuration for the `azure_monitor_logs` sink.
class `Vector::sinks::SinksAlternate11` {
  /// Publish log events to the Azure Monitor Logs service.
  type: "azure_monitor_logs"
}

/// Configuration for the `blackhole` sink.
class `Vector::sinks::SinksAlternate12` {
  /// Send observability events nowhere, which can be useful for debugging purposes.
  type: "blackhole"
}

/// Configuration for the `clickhouse` sink.
class `Vector::sinks::SinksAlternate13` {
  /// Deliver log data to a ClickHouse database.
  type: "clickhouse"
}

/// Configuration for the `console` sink.
class `Vector::sinks::SinksAlternate14` {
  /// Display observability events in the console, which can be useful for debugging purposes.
  type: "console"
}

/// Configuration for the `databend` sink.
class `Vector::sinks::SinksAlternate15` {
  /// Deliver log data to a Databend database.
  type: "databend"
}

/// Configuration for the `datadog_events` sink.
class `Vector::sinks::SinksAlternate16` {
  /// Publish observability events to the Datadog Events API.
  type: "datadog_events"
}

/// Configuration for the `datadog_logs` sink.
class `Vector::sinks::SinksAlternate17` {
  /// Publish log events to Datadog.
  type: "datadog_logs"
}

/// Configuration for the `datadog_metrics` sink.
class `Vector::sinks::SinksAlternate18` {
  /// Publish metric events to Datadog.
  type: "datadog_metrics"
}

/// Configuration for the `datadog_traces` sink.
class `Vector::sinks::SinksAlternate19` {
  /// Publish trace events to Datadog.
  type: "datadog_traces"
}

/// Configuration for the `elasticsearch` sink.
class `Vector::sinks::SinksAlternate20` {
  /// Index observability events in Elasticsearch.
  type: "elasticsearch"
}

/// Configuration for the `file` sink.
class `Vector::sinks::SinksAlternate21` {
  /// Output observability events into files.
  type: "file"
}

/// Configuration for the `gcp_chronicle_unstructured` sink.
class `Vector::sinks::SinksAlternate22` {
  /// Store unstructured log events in Google Chronicle.
  type: "gcp_chronicle_unstructured"
}

/// Configuration for the `gcp_cloud_storage` sink.
class `Vector::sinks::SinksAlternate23` {
  /// Store observability events in GCP Cloud Storage.
  type: "gcp_cloud_storage"
}

/// Configuration for the `gcp_pubsub` sink.
class `Vector::sinks::SinksAlternate24` {
  /// Publish observability events to GCP's Pub/Sub messaging system.
  type: "gcp_pubsub"
}

/// Configuration for the `gcp_stackdriver_logs` sink.
class `Vector::sinks::SinksAlternate25` {
  /// Deliver logs to GCP's Cloud Operations suite.
  type: "gcp_stackdriver_logs"
}

/// Configuration for the `gcp_stackdriver_metrics` sink.
class `Vector::sinks::SinksAlternate26` {
  /// Deliver metrics to GCP's Cloud Monitoring system.
  type: "gcp_stackdriver_metrics"
}

/// Configuration items for GreptimeDB
class `Vector::sinks::SinksAlternate27` {
  /// Ingest metrics data into GreptimeDB.
  type: "greptimedb"
}

/// Configuration for the `honeycomb` sink.
class `Vector::sinks::SinksAlternate28` {
  /// Deliver log events to Honeycomb.
  type: "honeycomb"
}

/// Configuration for the `http` sink.
class `Vector::sinks::SinksAlternate29` {
  /// Deliver observability event data to an HTTP server.
  type: "http"
}

/// Configuration for the `humio_logs` sink.
class `Vector::sinks::SinksAlternate30` {
  /// Deliver log event data to Humio.
  type: "humio_logs"
}

/// Configuration for the `humio_metrics` sink.
class `Vector::sinks::SinksAlternate31` {
  /// Deliver metric event data to Humio.
  type: "humio_metrics"
}

/// Configuration for the `influxdb_logs` sink.
class `Vector::sinks::SinksAlternate32` {
  /// Deliver log event data to InfluxDB.
  type: "influxdb_logs"
}

/// Configuration for the `influxdb_metrics` sink.
class `Vector::sinks::SinksAlternate33` {
  /// Deliver metric event data to InfluxDB.
  type: "influxdb_metrics"
}

/// Configuration for the `kafka` sink.
class `Vector::sinks::SinksAlternate34` {
  /// Publish observability event data to Apache Kafka topics.
  type: "kafka"
}

/// Configuration for the `logdna` sink.
class `Vector::sinks::SinksAlternate35` {
  /// Deliver log event data to LogDNA.
  type: "logdna"
}

/// Configuration for the `loki` sink.
class `Vector::sinks::SinksAlternate36` {
  /// Deliver log event data to the Loki aggregation system.
  type: "loki"
}

/// Configuration for the `mezmo` (formerly `logdna`) sink.
class `Vector::sinks::SinksAlternate37` {
  /// Deliver log event data to Mezmo.
  type: "mezmo"
}

/// Configuration for the `mqtt` sink
class `Vector::sinks::SinksAlternate38` {
  /// This component is missing a description.
  type: "mqtt"
}

/// Configuration for the `nats` sink.
class `Vector::sinks::SinksAlternate39` {
  /// Publish observability data to subjects on the NATS messaging system.
  type: "nats"
}

/// Configuration for the `new_relic` sink.
class `Vector::sinks::SinksAlternate40` {
  /// Deliver events to New Relic.
  type: "new_relic"
}

/// Configuration for the `papertrail` sink.
class `Vector::sinks::SinksAlternate41` {
  /// Deliver log events to Papertrail from SolarWinds.
  type: "papertrail"
}

/// Configuration for the `prometheus_exporter` sink.
class `Vector::sinks::SinksAlternate42` {
  /// Expose metric events on a Prometheus compatible endpoint.
  type: "prometheus_exporter"
}

/// Configuration for the `prometheus_remote_write` sink.
class `Vector::sinks::SinksAlternate43` {
  /// Deliver metric data to a Prometheus remote write endpoint.
  type: "prometheus_remote_write"
}

/// Configuration for the `pulsar` sink.
class `Vector::sinks::SinksAlternate44` {
  /// Publish observability events to Apache Pulsar topics.
  type: "pulsar"
}

/// Configuration for the `redis` sink.
class `Vector::sinks::SinksAlternate45` {
  /// Publish observability data to Redis.
  type: "redis"
}

/// Configuration for the `sematext_logs` sink.
class `Vector::sinks::SinksAlternate46` {
  /// Publish log events to Sematext.
  type: "sematext_logs"
}

/// Configuration for the `sematext_metrics` sink.
class `Vector::sinks::SinksAlternate47` {
  /// Publish metric events to Sematext.
  type: "sematext_metrics"
}

/// Configuration for the `socket` sink.
class `Vector::sinks::SinksAlternate48` {
  /// Deliver logs to a remote socket endpoint.
  type: "socket"
}

/// Configuration for the `splunk_hec_logs` sink.
class `Vector::sinks::SinksAlternate49` {
  /// Deliver log data to Splunk's HTTP Event Collector.
  type: "splunk_hec_logs"
}

/// Configuration of the `splunk_hec_metrics` sink.
class `Vector::sinks::SinksAlternate50` {
  /// Deliver metric data to Splunk's HTTP Event Collector.
  type: "splunk_hec_metrics"
}

/// Configuration for the `statsd` sink.
class `Vector::sinks::SinksAlternate51` {
  /// Deliver metric data to a StatsD aggregator.
  type: "statsd"
}

/// Configuration for the `unit_test` sink.
class `Vector::sinks::SinksAlternate52` {
  /// Unit test.
  type: "unit_test"
}

/// Configuration for the `unit_test_stream` sink.
class `Vector::sinks::SinksAlternate53` {
  /// Unit test stream.
  type: "unit_test_stream"
}

/// Configuration for the `vector` sink.
class `Vector::sinks::SinksAlternate54` {
  /// Relay observability data to a Vector instance.
  type: "vector"
}

/// Configuration for the `webhdfs` sink.
class `Vector::sinks::SinksAlternate55` {
  /// WebHDFS.
  type: "webhdfs"
}

/// Configuration for the `websocket` sink.
class `Vector::sinks::SinksAlternate56` {
  /// Deliver observability event data to a websocket listener.
  type: "websocket"
}

/// Configuration for the `amqp` sink.
///
/// AMQP connection options.
class `Vector::sinks::amqp::config::AmqpSinkConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Configures how events are encoded into raw bytes.
  encoding: Any

  /// The exchange to publish messages to.
  exchange: Any

  /// Configure the AMQP message properties.
  ///
  /// AMQP message properties.
  properties: (Any|PropertiesAlternate1)?

  /// Template used to generate a routing key which corresponds to a queue binding.
  routing_key: Any?
}

/// Configure the AMQP message properties.
///
/// AMQP properties configuration.
class PropertiesAlternate1 {
  /// Content-Encoding for the AMQP messages.
  content_encoding: String?

  /// Content-Type for the AMQP messages.
  content_type: String?

  /// Expiration for AMQP messages (in milliseconds)
  expiration_ms: Number(isBetween(0.0, 9.007199254740991E15))?
}

/// Configuration for the `appsignal` sink.
class `Vector::sinks::appsignal::config::AppsignalConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: Batch?

  /// Compression configuration.
  ///
  /// All compression algorithms use the default compression level unless otherwise specified.
  ///
  /// Default if undefined: `"gzip"`
  compression: Any?

  /// Transformations to prepare an event for serialization.
  ///
  /// Default if undefined: `{}`
  encoding: Any?

  /// The URI for the AppSignal API to send data to.
  ///
  /// Default if undefined: `"https://appsignal-endpoint.net"`
  endpoint: Uri?

  /// A valid app-level AppSignal Push API key.
  push_api_key: Any

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  tls: Any?
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
class Batch {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  ///
  /// Default if undefined: `450000`
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  ///
  /// Default if undefined: `100`
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `1.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// The [AWS region][aws_region] of the target service.
///
/// [aws_region]:
/// https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html
class `Vector::sinks::awsCloudwatchLogs::config::CloudwatchLogsSinkConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The ARN of an [IAM role][iam_role] to assume at startup.
  ///
  /// [iam_role]: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html
  @Deprecated
  assume_role: String?

  /// Configuration of the authentication strategy for interacting with AWS services.
  ///
  /// Default if undefined: `{ ["load_timeout_secs"] = null ["imds"] { ["max_attempts"] = 4
  /// ["connect_timeout_seconds"] = 1 ["read_timeout_seconds"] = 1 } ["region"] = null }`
  auth: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::awsCloudwatchLogs::config::CloudwatchLogsSinkConfigBatch`?

  /// Compression configuration.
  ///
  /// All compression algorithms use the default compression level unless otherwise specified.
  ///
  /// Default if undefined: `"none"`
  compression: Any?

  /// Dynamically create a [log group][log_group] if it does not already exist.
  ///
  /// This ignores `create_missing_stream` directly after creating the group and creates the first
  /// stream.
  ///
  /// [log_group]:
  /// https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html
  ///
  /// Default if undefined: `true`
  create_missing_group: Boolean?

  /// Dynamically create a [log stream][log_stream] if it does not already exist.
  ///
  /// [log_stream]:
  /// https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html
  ///
  /// Default if undefined: `true`
  create_missing_stream: Boolean?

  /// Configures how events are encoded into raw bytes.
  encoding: Any

  /// The [group name][group_name] of the target CloudWatch Logs stream.
  ///
  /// [group_name]:
  /// https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html
  group_name: Any

  /// Outbound HTTP request settings.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } ["headers"] {} }`
  request: Any?

  /// Retention policy configuration for AWS CloudWatch Log Group
  ///
  /// Default if undefined: `{ ["enabled"] = false }`
  retention: Retention?

  /// The [stream name][stream_name] of the target CloudWatch Logs stream.
  ///
  /// There can only be one writer to a log stream at a time. If multiple instances are writing to the
  /// same log group, the stream name must include an identifier that is guaranteed to be unique per
  /// instance.
  ///
  /// [stream_name]:
  /// https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html
  stream_name: Any

  tls: Any?
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
class `Vector::sinks::awsCloudwatchLogs::config::CloudwatchLogsSinkConfigBatch` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  ///
  /// Default if undefined: `1048576`
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  ///
  /// Default if undefined: `10000`
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `1.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Retention policy configuration for AWS CloudWatch Log Group
///
/// Default if undefined: `{ ["enabled"] = false }`
class Retention {
  /// If retention is enabled, the number of days to retain logs for.
  ///
  /// Default if undefined: `0`
  days: UInt32?

  /// Whether or not to set a retention policy when creating a new Log Group.
  ///
  /// Default if undefined: `false`
  enabled: Boolean?
}

/// The [AWS region][aws_region] of the target service.
///
/// [aws_region]:
/// https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html
class `Vector::sinks::awsCloudwatchMetrics::CloudWatchMetricsSinkConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The ARN of an [IAM role][iam_role] to assume at startup.
  ///
  /// [iam_role]: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html
  @Deprecated
  assume_role: String?

  /// Configuration of the authentication strategy for interacting with AWS services.
  ///
  /// Default if undefined: `{ ["load_timeout_secs"] = null ["imds"] { ["max_attempts"] = 4
  /// ["connect_timeout_seconds"] = 1 ["read_timeout_seconds"] = 1 } ["region"] = null }`
  auth: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::sematext::metrics::SematextMetricsConfigBatch`?

  /// Compression configuration.
  ///
  /// All compression algorithms use the default compression level unless otherwise specified.
  ///
  /// Default if undefined: `"none"`
  compression: Any?

  /// The default [namespace][namespace] to use for metrics that do not have one.
  ///
  /// Metrics with the same name can only be differentiated by their namespace, and not all metrics have
  /// their own namespace.
  ///
  /// [namespace]:
  /// https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html#Namespace
  default_namespace: String

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 150 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: `Vector::sinks::awsCloudwatchMetrics::CloudWatchMetricsSinkConfigRequest`?

  tls: Any?
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
class `Vector::sinks::sematext::metrics::SematextMetricsConfigBatch` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  ///
  /// Default if undefined: `20`
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `1.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Middleware settings for outbound requests.
///
/// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
/// etc.
///
/// Note that the retry backoff policy follows the Fibonacci sequence.
///
/// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"] =
/// 150 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
/// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
/// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
/// 2.5 ["max_concurrency_limit"] = 200 } }`
class `Vector::sinks::awsCloudwatchMetrics::CloudWatchMetricsSinkConfigRequest` {
  /// Configuration of adaptive concurrency parameters.
  ///
  /// These parameters typically do not require changes from the default, and incorrect values can lead
  /// to meta-stable or unstable performance and sink behavior. Proceed with caution.
  ///
  /// Default if undefined: `{ ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4
  /// ["rtt_deviation_scale"] = 2.5 ["max_concurrency_limit"] = 200 }`
  adaptive_concurrency: Any?

  /// Configuration for outbound request concurrency.
  ///
  /// This can be set either to one of the below enum values or to a positive integer, which denotes a
  /// fixed concurrency limit.
  ///
  /// Default if undefined: `"adaptive"`
  concurrency: Any?

  /// The time window used for the `rate_limit_num` option.
  ///
  /// Default if undefined: `1`
  rate_limit_duration_secs: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum number of requests allowed within the `rate_limit_duration_secs` time window.
  ///
  /// Default if undefined: `150`
  rate_limit_num: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum number of retries to make for failed requests.
  ///
  /// Default if undefined: `9223372036854775807`
  retry_attempts: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The amount of time to wait before attempting the first retry for a failed request.
  ///
  /// After the first retry has failed, the fibonacci sequence is used to select future backoffs.
  ///
  /// Default if undefined: `1`
  retry_initial_backoff_secs: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The jitter mode to use for retry backoff behavior.
  ///
  /// Default if undefined: `"Full"`
  retry_jitter_mode: Any?

  /// The maximum amount of time to wait between retries.
  ///
  /// Default if undefined: `30`
  retry_max_duration_secs: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The time a request can take before being aborted.
  ///
  /// Datadog highly recommends that you do not lower this value below the service's internal timeout, as
  /// this could create orphaned requests, pile on retries, and result in duplicate data downstream.
  ///
  /// Default if undefined: `60`
  timeout_secs: Int(isBetween(0.0, 9.007199254740991E15))?
}

/// Configuration of the region/endpoint to use when interacting with an AWS service.
class `Vector::sinks::awsKinesis::config::KinesisSinkBaseConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Configuration of the authentication strategy for interacting with AWS services.
  ///
  /// Default if undefined: `{ ["load_timeout_secs"] = null ["imds"] { ["max_attempts"] = 4
  /// ["connect_timeout_seconds"] = 1 ["read_timeout_seconds"] = 1 } ["region"] = null }`
  auth: Any?

  /// Compression configuration.
  ///
  /// All compression algorithms use the default compression level unless otherwise specified.
  ///
  /// Default if undefined: `"none"`
  compression: Any?

  /// Configures how events are encoded into raw bytes.
  encoding: Any

  /// The log field used as the Kinesis recordâs partition key value.
  ///
  /// If not specified, a unique partition key is generated for each Kinesis record.
  partition_key_field: Any?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  /// Whether or not to retry successful requests containing partial failures.
  ///
  /// Default if undefined: `false`
  request_retry_partial: Boolean?

  /// The [stream name][stream_name] of the target Kinesis Firehose delivery stream.
  ///
  /// [stream_name]:
  /// https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html
  stream_name: String

  tls: Any?
}

/// Base configuration for the `aws_kinesis_` sinks. The actual specific sink configuration types should
/// either wrap this in a newtype wrapper, or should extend it in a new struct with `serde(flatten)`.
class `Vector::sinks::awsKinesis::firehose::config::KinesisFirehoseSinkConfig` {
  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::awsKinesis::firehose::config::KinesisFirehoseSinkConfigBatch`?
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
class `Vector::sinks::awsKinesis::firehose::config::KinesisFirehoseSinkConfigBatch` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  ///
  /// Default if undefined: `4194304`
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  ///
  /// Default if undefined: `500`
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `1.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Base configuration for the `aws_kinesis_` sinks. The actual specific sink configuration types should
/// either wrap this in a newtype wrapper, or should extend it in a new struct with `serde(flatten)`.
class `Vector::sinks::awsKinesis::streams::config::KinesisStreamsSinkConfig` {
  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::awsKinesis::streams::config::KinesisStreamsSinkConfigBatch`?
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
class `Vector::sinks::awsKinesis::streams::config::KinesisStreamsSinkConfigBatch` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  ///
  /// Default if undefined: `5000000`
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  ///
  /// Default if undefined: `500`
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `1.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Encoding configuration.
class `Vector::sinks::awsS3::config::S3SinkConfig` {
  /// Canned ACL to apply to the created objects.
  ///
  /// For more information, see [Canned ACL][canned_acl].
  ///
  /// [canned_acl]: https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl
  acl: (
    Any
    |
    "private"
    |"public-read"
    |"public-read-write"
    |"aws-exec-read"
    |"authenticated-read"
    |"bucket-owner-read"
    |"bucket-owner-full-control"
    |"log-delivery-write")?

  /// Overrides what content encoding has been applied to the object.
  ///
  /// Directly comparable to the `Content-Encoding` HTTP header.
  ///
  /// If not specified, the compression scheme used dictates this value.
  content_encoding: String?

  /// Overrides the MIME type of the object.
  ///
  /// Directly comparable to the `Content-Type` HTTP header.
  ///
  /// If not specified, the compression scheme used dictates this value. When `compression` is set to
  /// `none`, the value `text/x-log` is used.
  content_type: String?

  /// Grants `READ`, `READ_ACP`, and `WRITE_ACP` permissions on the created objects to the named
  /// [grantee].
  ///
  /// This allows the grantee to read the created objects and their metadata, as well as read and modify
  /// the ACL on the created objects.
  ///
  /// [grantee]: https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#specifying-grantee
  grant_full_control: String?

  /// Grants `READ` permissions on the created objects to the named [grantee].
  ///
  /// This allows the grantee to read the created objects and their metadata.
  ///
  /// [grantee]: https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#specifying-grantee
  grant_read: String?

  /// Grants `READ_ACP` permissions on the created objects to the named [grantee].
  ///
  /// This allows the grantee to read the ACL on the created objects.
  ///
  /// [grantee]: https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#specifying-grantee
  grant_read_acp: String?

  /// Grants `WRITE_ACP` permissions on the created objects to the named [grantee].
  ///
  /// This allows the grantee to modify the ACL on the created objects.
  ///
  /// [grantee]: https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#specifying-grantee
  grant_write_acp: String?

  /// AWS S3 Server-Side Encryption algorithms.
  ///
  /// The Server-side Encryption algorithm used when storing these objects.
  server_side_encryption: (Any|"AES256"|"aws:kms")?

  /// Specifies the ID of the AWS Key Management Service (AWS KMS) symmetrical customer managed customer
  /// master key (CMK) that is used for the created objects.
  ///
  /// Only applies when `server_side_encryption` is configured to use KMS.
  ///
  /// If not specified, Amazon S3 uses the AWS managed CMK in AWS to protect the data.
  ssekms_key_id: String?

  /// The storage class for the created objects.
  ///
  /// See the [S3 Storage Classes][s3_storage_classes] for more details.
  ///
  /// [s3_storage_classes]: https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html
  ///
  /// Default if undefined: `"STANDARD"`
  storage_class: (
    "STANDARD"
    |"REDUCED_REDUNDANCY"
    |"INTELLIGENT_TIERING"
    |"STANDARD_IA"
    |"EXPRESS_ONEZONE"
    |"ONEZONE_IA"
    |"GLACIER"
    |"DEEP_ARCHIVE")?

  /// The tag-set for the object.
  tags: Mapping<String, String>?
}

/// Base Configuration `aws_s_s` for sns and sqs sink.
class `Vector::sinks::awsSS::config::BaseSSSinkConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The ARN of an [IAM role][iam_role] to assume at startup.
  ///
  /// [iam_role]: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html
  @Deprecated
  assume_role: String?

  /// Configuration of the authentication strategy for interacting with AWS services.
  ///
  /// Default if undefined: `{ ["load_timeout_secs"] = null ["imds"] { ["max_attempts"] = 4
  /// ["connect_timeout_seconds"] = 1 ["read_timeout_seconds"] = 1 } ["region"] = null }`
  auth: Any?

  /// Configures how events are encoded into raw bytes.
  encoding: Any

  /// The message deduplication ID value to allow AWS to identify duplicate messages.
  ///
  /// This value is a template which should result in a unique string for each event. See the [AWS
  /// documentation][deduplication_id_docs] for more about how AWS does message deduplication.
  ///
  /// [deduplication_id_docs]:
  /// https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html
  message_deduplication_id: String?

  /// The tag that specifies that a message belongs to a specific message group.
  ///
  /// Can be applied only to FIFO queues.
  message_group_id: String?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  tls: Any?
}

/// Base Configuration `aws_s_s` for sns and sqs sink.
class `Vector::sinks::awsSS::sns::config::SnsSinkConfig` {
  /// The ARN of the Amazon SNS topic to which messages are sent.
  topic_arn: Uri
}

/// Base Configuration `aws_s_s` for sns and sqs sink.
class `Vector::sinks::awsSS::sqs::config::SqsSinkConfig` {
  /// The URL of the Amazon SQS queue to which messages are sent.
  queue_url: Uri
}

/// Configuration for the `axiom` sink.
class `Vector::sinks::axiom::AxiomConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Compression configuration.
  ///
  /// All compression algorithms use the default compression level unless otherwise specified.
  ///
  /// Default if undefined: `"none"`
  compression: Any?

  /// The Axiom dataset to write to.
  dataset: String

  /// The Axiom organization ID.
  ///
  /// Only required when using personal tokens.
  org_id: String?

  /// Outbound HTTP request settings.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } ["headers"] {} }`
  request: Any?

  tls: Any?

  /// The Axiom API token.
  token: Any

  /// URI of the Axiom endpoint to send data to.
  ///
  /// Only required if not using Axiom Cloud.
  url: Uri?
}

/// Encoding configuration.
class `Vector::sinks::azureBlob::config::AzureBlobSinkConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: Any?

  /// Whether or not to append a UUID v4 token to the end of the blob key.
  ///
  /// The UUID is appended to the timestamp portion of the object key, such that if the blob key
  /// generated is `date=2022-07-18/1658176486`, setting this field to `true` results in an blob key that
  /// looks like `date=2022-07-18/1658176486-30f6652c-71da-4f9f-800d-a1189c47c547`.
  ///
  /// This ensures there are no name collisions, and can be useful in high-volume workloads where blob
  /// keys must be unique.
  blob_append_uuid: Boolean?

  /// A prefix to apply to all blob keys.
  ///
  /// Prefixes are useful for partitioning objects, such as by creating a blob key that stores blobs
  /// under a particular directory. If using a prefix for this purpose, it must end in `/` to act as a
  /// directory path. A trailing `/` is **not** automatically added.
  ///
  /// Default if undefined: `"blob/%F/"`
  blob_prefix: Any?

  /// The timestamp format for the time component of the blob key.
  ///
  /// By default, blob keys are appended with a timestamp that reflects when the blob are sent to Azure
  /// Blob Storage, such that the resulting blob key is functionally equivalent to joining the blob
  /// prefix with the formatted timestamp, such as `date=2022-07-18/1658176486`.
  ///
  /// This would represent a `blob_prefix` set to `date=%F/` and the timestamp of Mon Jul 18 2022
  /// 20:34:44 GMT+0000, with the `filename_time_format` being set to `%s`, which renders timestamps in
  /// seconds since the Unix epoch.
  ///
  /// Supports the common [`strftime`][chrono_strftime_specifiers] specifiers found in most languages.
  ///
  /// When set to an empty string, no timestamp is appended to the blob prefix.
  ///
  /// [chrono_strftime_specifiers]:
  /// https://docs.rs/chrono/latest/chrono/format/strftime/index.html#specifiers
  blob_time_format: String?

  /// Compression configuration.
  ///
  /// All compression algorithms use the default compression level unless otherwise specified.
  ///
  /// Default if undefined: `"gzip"`
  compression: Any?

  /// The Azure Blob Storage Account connection string.
  ///
  /// Authentication with access key is the only supported authentication method.
  ///
  /// Either `storage_account`, or this field, must be specified.
  connection_string: Any?

  /// The Azure Blob Storage Account container name.
  container_name: String

  /// The Azure Blob Storage Endpoint URL.
  ///
  /// This is used to override the default blob storage endpoint URL in cases where you are using
  /// credentials read from the environment/managed identities or access tokens without using an explicit
  /// connection_string (which already explicitly supports overriding the blob endpoint URL).
  ///
  /// This may only be used with `storage_account` and is ignored when used with `connection_string`.
  endpoint: String?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 250 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: `Vector::sinks::azureBlob::config::AzureBlobSinkConfigRequest`?

  /// The Azure Blob Storage Account name.
  ///
  /// Attempts to load credentials for the account in the following ways, in order:
  ///
  /// - read from environment variables ([more information][env_cred_docs]) - looks for a [Managed
  /// Identity][managed_ident_docs] - uses the `az` CLI tool to get an access token ([more
  /// information][az_cli_docs])
  ///
  /// Either `connection_string`, or this field, must be specified.
  ///
  /// [env_cred_docs]:
  /// https://docs.rs/azure_identity/latest/azure_identity/struct.EnvironmentCredential.html
  /// [managed_ident_docs]:
  /// https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview
  /// [az_cli_docs]:
  /// https://docs.microsoft.com/en-us/cli/azure/account?view=azure-cli-latest#az-account-get-access-token
  storage_account: String?
}

/// Middleware settings for outbound requests.
///
/// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
/// etc.
///
/// Note that the retry backoff policy follows the Fibonacci sequence.
///
/// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"] =
/// 250 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
/// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
/// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
/// 2.5 ["max_concurrency_limit"] = 200 } }`
class `Vector::sinks::azureBlob::config::AzureBlobSinkConfigRequest` {
  /// Configuration of adaptive concurrency parameters.
  ///
  /// These parameters typically do not require changes from the default, and incorrect values can lead
  /// to meta-stable or unstable performance and sink behavior. Proceed with caution.
  ///
  /// Default if undefined: `{ ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4
  /// ["rtt_deviation_scale"] = 2.5 ["max_concurrency_limit"] = 200 }`
  adaptive_concurrency: Any?

  /// Configuration for outbound request concurrency.
  ///
  /// This can be set either to one of the below enum values or to a positive integer, which denotes a
  /// fixed concurrency limit.
  ///
  /// Default if undefined: `"adaptive"`
  concurrency: Any?

  /// The time window used for the `rate_limit_num` option.
  ///
  /// Default if undefined: `1`
  rate_limit_duration_secs: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum number of requests allowed within the `rate_limit_duration_secs` time window.
  ///
  /// Default if undefined: `250`
  rate_limit_num: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum number of retries to make for failed requests.
  ///
  /// Default if undefined: `9223372036854775807`
  retry_attempts: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The amount of time to wait before attempting the first retry for a failed request.
  ///
  /// After the first retry has failed, the fibonacci sequence is used to select future backoffs.
  ///
  /// Default if undefined: `1`
  retry_initial_backoff_secs: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The jitter mode to use for retry backoff behavior.
  ///
  /// Default if undefined: `"Full"`
  retry_jitter_mode: Any?

  /// The maximum amount of time to wait between retries.
  ///
  /// Default if undefined: `30`
  retry_max_duration_secs: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The time a request can take before being aborted.
  ///
  /// Datadog highly recommends that you do not lower this value below the service's internal timeout, as
  /// this could create orphaned requests, pile on retries, and result in duplicate data downstream.
  ///
  /// Default if undefined: `60`
  timeout_secs: Int(isBetween(0.0, 9.007199254740991E15))?
}

/// Configuration for the `azure_monitor_logs` sink.
class `Vector::sinks::azureMonitorLogs::config::AzureMonitorLogsConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The [Resource ID][resource_id] of the Azure resource the data should be associated with.
  ///
  /// [resource_id]:
  /// https://docs.microsoft.com/en-us/azure/azure-monitor/platform/data-collector-api#request-headers
  azure_resource_id: String?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: Any?

  /// The [unique identifier][uniq_id] for the Log Analytics workspace.
  ///
  /// [uniq_id]:
  /// https://docs.microsoft.com/en-us/azure/azure-monitor/platform/data-collector-api#request-uri-parameters
  customer_id: String

  /// Transformations to prepare an event for serialization.
  ///
  /// Default if undefined: `{}`
  encoding: Any?

  /// [Alternative host][alt_host] for dedicated Azure regions.
  ///
  /// [alt_host]:
  /// https://docs.azure.cn/en-us/articles/guidance/developerdifferences#check-endpoints-in-azure
  ///
  /// Default if undefined: `"ods.opinsights.azure.com"`
  host: String?

  /// The [record type][record_type] of the data that is being submitted.
  ///
  /// Can only contain letters, numbers, and underscores (_), and may not exceed 100 characters.
  ///
  /// [record_type]:
  /// https://docs.microsoft.com/en-us/azure/azure-monitor/platform/data-collector-api#request-headers
  log_type: String(matches(Regex("[a-zA-Z0-9_]{1,100}")))

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  /// The [primary or the secondary key][shared_key] for the Log Analytics workspace.
  ///
  /// [shared_key]:
  /// https://docs.microsoft.com/en-us/azure/azure-monitor/platform/data-collector-api#authorization
  shared_key: Any

  /// Use this option to customize the log field used as [`TimeGenerated`][1] in Azure.
  ///
  /// The setting of `log_schema.timestamp_key`, usually `timestamp`, is used here by default. This field
  /// should be used in rare cases where `TimeGenerated` should point to a specific log field. For
  /// example, use this field to set the log field `source_timestamp` as holding the value that should be
  /// used as `TimeGenerated` on the Azure side.
  ///
  /// [1]: https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-standard-columns#timegenerated
  time_generated_key: Any?

  tls: Any?
}

/// Configuration for the `blackhole` sink.
///
/// Default if undefined: `{ ["print_interval_secs"] = 0 ["rate"] = null }`
class `Vector::sinks::blackhole::config::BlackholeConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The interval between reporting a summary of activity.
  ///
  /// Set to `0` (default) to disable reporting.
  ///
  /// Default if undefined: `0`
  print_interval_secs: Any?

  /// The number of events, per second, that the sink is allowed to consume.
  ///
  /// By default, there is no limit.
  rate: Number(isBetween(0.0, 9.007199254740991E15))?
}

/// Configuration for the `clickhouse` sink.
class `Vector::sinks::clickhouse::config::ClickhouseConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  auth: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: Any?

  /// Compression configuration.
  ///
  /// All compression algorithms use the default compression level unless otherwise specified.
  ///
  /// Default if undefined: `"gzip"`
  compression: Any?

  /// The database that contains the table that data is inserted into.
  database: Any?

  /// Sets `date_time_input_format` to `best_effort`, allowing ClickHouse to properly parse RFC3339/ISO
  /// 8601.
  ///
  /// Default if undefined: `false`
  date_time_best_effort: Boolean?

  /// Transformations to prepare an event for serialization.
  ///
  /// Default if undefined: `{}`
  encoding: Any?

  /// The endpoint of the ClickHouse server.
  endpoint: Any

  /// Data format.
  ///
  /// The format to parse input data.
  ///
  /// Default if undefined: `"json_each_row"`
  format: ("json_each_row"|"json_as_object"|"json_as_string")?

  /// Sets `insert_distributed_one_random_shard`, allowing ClickHouse to insert data into a random shard
  /// when using Distributed Table Engine.
  ///
  /// Default if undefined: `false`
  insert_random_shard: Boolean?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  /// Sets `input_format_skip_unknown_fields`, allowing ClickHouse to discard fields not present in the
  /// table schema.
  ///
  /// Default if undefined: `false`
  skip_unknown_fields: Boolean?

  /// The table that data is inserted into.
  table: Any

  tls: Any?
}

/// Encoding configuration.
class `Vector::sinks::console::config::ConsoleSinkConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The [standard stream][standard_streams] to write to.
  ///
  /// [standard_streams]: https://en.wikipedia.org/wiki/Standard_streams
  ///
  /// Default if undefined: `"stdout"`
  target: ("stdout"|"stderr")?
}

/// Configuration for the `databend` sink.
class `Vector::sinks::databend::config::DatabendConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The username and password to authenticate with. Overrides the username and password in DSN.
  auth: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: Any?

  /// Compression configuration.
  ///
  /// Default if undefined: `"none"`
  compression: ("none"|"gzip")?

  /// The database that contains the table that data is inserted into. Overrides the database in DSN.
  database: String?

  /// Transformations to prepare an event for serialization.
  ///
  /// Default if undefined: `{ ["codec"] = "json" ["json"] { ["pretty"] = false } }`
  encoding: Encoding?

  /// The DSN of the Databend server.
  endpoint: Any

  /// Defines how missing fields are handled for NDJson. Refer to
  /// https://docs.databend.com/sql/sql-reference/file-format-options#null_field_as
  ///
  /// Default if undefined: `"NULL"`
  missing_field_as: ("ERROR"|"NULL"|"FIELD_DEFAULT"|"TYPE_DEFAULT")?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  /// The table that data is inserted into.
  table: String

  /// The TLS configuration to use when connecting to the Databend server.
  @Deprecated
  tls: Any?
}

/// Transformations to prepare an event for serialization.
///
/// Default if undefined: `{ ["codec"] = "json" ["json"] { ["pretty"] = false } }`
class Encoding {
  /// List of fields that are excluded from the encoded event.
  except_fields: Listing<Any>?

  /// List of fields that are included in the encoded event.
  only_fields: Listing<Any>?

  /// Format used for timestamp fields.
  timestamp_format: (Any|"unix"|"rfc3339"|"unix_ms"|"unix_us"|"unix_ns"|"unix_float")?
}

/// Encodes an event as a CSV message.
///
/// Options for the CSV encoder.
class EncodingEncodingAlternate0 {
  /// Encodes an event as a CSV message.
  ///
  /// This codec must be configured with fields to encode.
  codec: "csv"
}

/// Encodes an event as [JSON][json].
///
/// Encoding options specific to the Json serializer.
class EncodingAlternate1 {
  /// Encodes an event as [JSON][json].
  ///
  /// [json]: https://www.json.org/
  codec: "json"
}

/// Shared configuration for Datadog sinks. Contains the maximum set of common settings that applies to
/// all DD sink components.
class `Vector::sinks::datadog::LocalDatadogCommonConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The default Datadog [API key][api_key] to use in authentication of HTTP requests.
  ///
  /// If an event has a Datadog [API key][api_key] set explicitly in its metadata, it takes precedence
  /// over this setting.
  ///
  /// This value can also be set by specifying the `DD_API_KEY` environment variable. The value specified
  /// here takes precedence over the environment variable.
  ///
  /// [api_key]: https://docs.datadoghq.com/api/?lang=bash#authentication [global_options]:
  /// /docs/reference/configuration/global-options/#datadog
  default_api_key: Any?

  /// The endpoint to send observability data to.
  ///
  /// The endpoint must contain an HTTP scheme, and may specify a hostname or IP address and port. The
  /// API path should NOT be specified as this is handled by the sink.
  ///
  /// If set, overrides the `site` option.
  endpoint: String?

  /// The Datadog [site][dd_site] to send observability data to.
  ///
  /// This value can also be set by specifying the `DD_SITE` environment variable. The value specified
  /// here takes precedence over the environment variable.
  ///
  /// If not specified by the environment variable, a default value of `datadoghq.com` is taken.
  ///
  /// [dd_site]: https://docs.datadoghq.com/getting_started/site
  site: String?

  tls: Any?
}

/// Shared configuration for Datadog sinks. Contains the maximum set of common settings that applies to
/// all DD sink components.
class `Vector::sinks::datadog::events::config::DatadogEventsConfig` {
  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?
}

/// Shared configuration for Datadog sinks. Contains the maximum set of common settings that applies to
/// all DD sink components.
class `Vector::sinks::datadog::logs::config::DatadogLogsConfig` {
  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::datadog::logs::config::DatadogLogsConfigBatch`?

  compression: Any?

  /// Transformations to prepare an event for serialization.
  ///
  /// Default if undefined: `{}`
  encoding: Any?

  /// Outbound HTTP request settings.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } ["headers"] {} }`
  request: Any?
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
class `Vector::sinks::datadog::logs::config::DatadogLogsConfigBatch` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  ///
  /// Default if undefined: `4250000`
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  ///
  /// Default if undefined: `1000`
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `5.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Shared configuration for Datadog sinks. Contains the maximum set of common settings that applies to
/// all DD sink components.
class `Vector::sinks::datadog::metrics::config::DatadogMetricsConfig` {
  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::datadog::metrics::config::DatadogMetricsConfigBatch`?

  /// Sets the default namespace for any metrics sent.
  ///
  /// This namespace is only used if a metric has no existing namespace. When a namespace is present, it
  /// is used as a prefix to the metric name, and separated with a period (`.`).
  default_namespace: String?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
class `Vector::sinks::datadog::metrics::config::DatadogMetricsConfigBatch` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  ///
  /// Default if undefined: `100000`
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `2.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Shared configuration for Datadog sinks. Contains the maximum set of common settings that applies to
/// all DD sink components.
class `Vector::sinks::datadog::traces::config::DatadogTracesConfig` {
  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::datadog::traces::config::DatadogTracesConfigBatch`?

  compression: Any?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
class `Vector::sinks::datadog::traces::config::DatadogTracesConfigBatch` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  ///
  /// Default if undefined: `3000000`
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  ///
  /// Default if undefined: `1000`
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `10.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Configuration for the `elasticsearch` sink.
class `Vector::sinks::elasticsearch::config::ElasticsearchConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The API version of Elasticsearch.
  ///
  /// Default if undefined: `"auto"`
  api_version: ("auto"|"v6"|"v7"|"v8")?

  /// Elasticsearch Authentication strategies.
  auth: (Any|AuthAlternate1Alternate0|AuthAlternate1Alternate1)?

  aws: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: Any?

  /// Elasticsearch bulk mode configuration.
  ///
  /// Default if undefined: `{ ["action"] = "index" ["index"] = "vector-%Y.%m.%d" ["version"] = null
  /// ["version_type"] = "internal" }`
  bulk: Bulk?

  /// Compression configuration.
  ///
  /// All compression algorithms use the default compression level unless otherwise specified.
  ///
  /// Default if undefined: `"none"`
  compression: Any?

  /// Elasticsearch data stream mode configuration.
  data_stream: (Any|DataStreamAlternate1)?

  /// Options for determining the health of an endpoint.
  distribution: (Any|DistributionAlternate1)?

  /// The [`doc_type`][doc_type] for your index data.
  ///
  /// This is only relevant for Elasticsearch <= 6.X. If you are using >= 7.0 you do not need to set this
  /// option since Elasticsearch has removed it.
  ///
  /// [doc_type]: https://www.elastic.co/guide/en/elasticsearch/reference/6.8/actions-index.html
  ///
  /// Default if undefined: `"_doc"`
  doc_type: String?

  /// Transformations to prepare an event for serialization.
  ///
  /// Default if undefined: `{}`
  encoding: Any?

  /// The Elasticsearch endpoint to send logs to.
  ///
  /// The endpoint must contain an HTTP scheme, and may specify a hostname or IP address and port.
  @Deprecated
  endpoint: String?

  /// A list of Elasticsearch endpoints to send logs to.
  ///
  /// The endpoint must contain an HTTP scheme, and may specify a hostname or IP address and port.
  ///
  /// Default if undefined: `{}`
  endpoints: Listing<String>?

  /// The name of the event key that should map to Elasticsearchâs [`_id` field][es_id].
  ///
  /// By default, the `_id` field is not set, which allows Elasticsearch to set this automatically.
  /// Setting your own Elasticsearch IDs can [hinder performance][perf_doc].
  ///
  /// [es_id]: https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-id-field.html
  /// [perf_doc]:
  /// https://www.elastic.co/guide/en/elasticsearch/reference/master/tune-for-indexing-speed.html#_use_auto_generated_ids
  id_key: Any?

  /// Configuration for the `metric_to_log` transform.
  metrics: (Any|Any)?

  /// Elasticsearch Indexing mode.
  ///
  /// Default if undefined: `"bulk"`
  mode: ("bulk"|"data_stream")?

  /// The name of the pipeline to apply.
  pipeline: String?

  /// Custom parameters to add to the query string for each HTTP request sent to Elasticsearch.
  query: Mapping<String, String>?

  /// Outbound HTTP request settings.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } ["headers"] {} }`
  request: Any?

  /// Whether or not to retry successful requests containing partial failures.
  ///
  /// To avoid duplicates in Elasticsearch, please use option `id_key`.
  ///
  /// Default if undefined: `false`
  request_retry_partial: Boolean?

  /// Whether or not to send the `type` field to Elasticsearch.
  ///
  /// The `type` field was deprecated in Elasticsearch 7.x and removed in Elasticsearch 8.x.
  ///
  /// If enabled, the `doc_type` option is ignored.
  ///
  /// Default if undefined: `false`
  @Deprecated
  suppress_type_name: Boolean?

  tls: Any?
}

/// HTTP Basic Authentication.
class AuthAlternate1Alternate0 {
  /// Basic authentication password.
  password: Any

  /// HTTP Basic Authentication.
  strategy: "basic"

  /// Basic authentication username.
  user: String
}

/// Configuration of the authentication strategy for interacting with AWS services.
class AuthAlternate1Alternate1 {
  /// Amazon OpenSearch Service-specific authentication.
  strategy: "aws"
}

/// Elasticsearch bulk mode configuration.
///
/// Default if undefined: `{ ["action"] = "index" ["index"] = "vector-%Y.%m.%d" ["version"] = null
/// ["version_type"] = "internal" }`
class Bulk {
  /// Action to use when making requests to the [Elasticsearch Bulk API][es_bulk].
  ///
  /// Only `index` and `create` actions are supported.
  ///
  /// [es_bulk]: https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html
  ///
  /// Default if undefined: `"index"`
  action: Any?

  /// The name of the index to write events to.
  ///
  /// Default if undefined: `"vector-%Y.%m.%d"`
  index: Any?

  /// Version field value.
  version: Any?

  /// Version type.
  ///
  /// Possible values are `internal`, `external` or `external_gt` and `external_gte`.
  ///
  /// [es_index_versioning]:
  /// https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#index-versioning
  ///
  /// Default if undefined: `"internal"`
  version_type: ("internal"|"external"|"external_gte")?
}

/// Elasticsearch data stream mode configuration.
class DataStreamAlternate1 {
  /// Automatically routes events by deriving the data stream name using specific event fields.
  ///
  /// The format of the data stream name is `<type>-<dataset>-<namespace>`, where each value comes from
  /// the `data_stream` configuration field of the same name.
  ///
  /// If enabled, the value of the `data_stream.type`, `data_stream.dataset`, and `data_stream.namespace`
  /// event fields are used if they are present. Otherwise, the values set in this configuration are
  /// used.
  ///
  /// Default if undefined: `true`
  auto_routing: Boolean?

  /// The data stream dataset used to construct the data stream at index time.
  ///
  /// Default if undefined: `"generic"`
  dataset: Any?

  /// The data stream namespace used to construct the data stream at index time.
  ///
  /// Default if undefined: `"default"`
  namespace: Any?

  /// Automatically adds and syncs the `data_stream.*` event fields if they are missing from the event.
  ///
  /// This ensures that fields match the name of the data stream that is receiving events.
  ///
  /// Default if undefined: `true`
  sync_fields: Boolean?

  /// The data stream type used to construct the data stream at index time.
  ///
  /// Default if undefined: `"logs"`
  type: Any?
}

/// Options for determining the health of an endpoint.
class DistributionAlternate1 {
  /// Initial delay between attempts to reactivate endpoints once they become unhealthy.
  ///
  /// Default if undefined: `1`
  retry_initial_backoff_secs: Int(isBetween(0.0, 9.007199254740991E15))?

  /// Maximum delay between attempts to reactivate endpoints once they become unhealthy.
  ///
  /// Default if undefined: `3600`
  retry_max_duration_secs: Any?
}

/// Encoding configuration.
class `Vector::sinks::file::FileSinkConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Compression configuration.
  ///
  /// Default if undefined: `"none"`
  compression: ("gzip"|"zstd"|"none")?

  /// The amount of time that a file can be idle and stay open.
  ///
  /// After not receiving any events in this amount of time, the file is flushed and closed.
  ///
  /// Default if undefined: `30`
  idle_timeout_secs: Any?

  /// Configuration of internal metrics for file-based components.
  ///
  /// Default if undefined: `{ ["include_file_tag"] = false }`
  internal_metrics: Any?

  /// File path to write events to.
  ///
  /// Compression format extension must be explicit.
  path: Any

  timezone: Any?
}

/// Configuration of the authentication strategy for interacting with GCP services.
class `Vector::sinks::gcp::cloudStorage::GcsSinkConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The Predefined ACL to apply to created objects.
  ///
  /// For more information, see [Predefined ACLs][predefined_acls].
  ///
  /// [predefined_acls]: https://cloud.google.com/storage/docs/access-control/lists#predefined-acl
  acl: (
    Any
    |
    "authenticated-read"
    |"bucket-owner-full-control"
    |"bucket-owner-read"
    |"private"
    |"project-private"
    |"public-read")?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: Any?

  /// The GCS bucket name.
  bucket: String

  /// Compression configuration.
  ///
  /// All compression algorithms use the default compression level unless otherwise specified.
  ///
  /// Default if undefined: `"none"`
  compression: Any?

  /// Whether or not to append a UUID v4 token to the end of the object key.
  ///
  /// The UUID is appended to the timestamp portion of the object key, such that if the object key
  /// generated is `date=2022-07-18/1658176486`, setting this field to `true` results in an object key
  /// that looks like `date=2022-07-18/1658176486-30f6652c-71da-4f9f-800d-a1189c47c547`.
  ///
  /// This ensures there are no name collisions, and can be useful in high-volume workloads where object
  /// keys must be unique.
  ///
  /// Default if undefined: `true`
  filename_append_uuid: Boolean?

  /// The filename extension to use in the object key.
  ///
  /// If not specified, the extension is determined by the compression scheme used.
  filename_extension: String?

  /// The timestamp format for the time component of the object key.
  ///
  /// By default, object keys are appended with a timestamp that reflects when the objects are sent to
  /// S3, such that the resulting object key is functionally equivalent to joining the key prefix with
  /// the formatted timestamp, such as `date=2022-07-18/1658176486`.
  ///
  /// This would represent a `key_prefix` set to `date=%F/` and the timestamp of Mon Jul 18 2022 20:34:44
  /// GMT+0000, with the `filename_time_format` being set to `%s`, which renders timestamps in seconds
  /// since the Unix epoch.
  ///
  /// Supports the common [`strftime`][chrono_strftime_specifiers] specifiers found in most languages.
  ///
  /// When set to an empty string, no timestamp is appended to the key prefix.
  ///
  /// [chrono_strftime_specifiers]:
  /// https://docs.rs/chrono/latest/chrono/format/strftime/index.html#specifiers
  ///
  /// Default if undefined: `"%s"`
  filename_time_format: String?

  /// A prefix to apply to all object keys.
  ///
  /// Prefixes are useful for partitioning objects, such as by creating an object key that stores objects
  /// under a particular directory. If using a prefix for this purpose, it must end in `/` in order to
  /// act as a directory path. A trailing `/` is **not** automatically added.
  key_prefix: String?

  /// The set of metadata `key:value` pairs for the created objects.
  ///
  /// For more information, see the [custom metadata][custom_metadata] documentation.
  ///
  /// [custom_metadata]: https://cloud.google.com/storage/docs/metadata#custom-metadata
  metadata: Mapping<String, String>?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 1000 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: `Vector::sinks::gcpChronicle::chronicleUnstructured::ChronicleUnstructuredConfigRequest`?

  /// The storage class for created objects.
  ///
  /// For more information, see the [storage classes][storage_classes] documentation.
  ///
  /// [storage_classes]: https://cloud.google.com/storage/docs/storage-classes
  storage_class: (Any|"STANDARD"|"NEARLINE"|"COLDLINE"|"ARCHIVE")?

  timezone: Any?

  tls: Any?
}

/// Middleware settings for outbound requests.
///
/// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
/// etc.
///
/// Note that the retry backoff policy follows the Fibonacci sequence.
///
/// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"] =
/// 1000 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
/// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
/// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
/// 2.5 ["max_concurrency_limit"] = 200 } }`
class `Vector::sinks::gcpChronicle::chronicleUnstructured::ChronicleUnstructuredConfigRequest` {
  /// Configuration of adaptive concurrency parameters.
  ///
  /// These parameters typically do not require changes from the default, and incorrect values can lead
  /// to meta-stable or unstable performance and sink behavior. Proceed with caution.
  ///
  /// Default if undefined: `{ ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4
  /// ["rtt_deviation_scale"] = 2.5 ["max_concurrency_limit"] = 200 }`
  adaptive_concurrency: Any?

  /// Configuration for outbound request concurrency.
  ///
  /// This can be set either to one of the below enum values or to a positive integer, which denotes a
  /// fixed concurrency limit.
  ///
  /// Default if undefined: `"adaptive"`
  concurrency: Any?

  /// The time window used for the `rate_limit_num` option.
  ///
  /// Default if undefined: `1`
  rate_limit_duration_secs: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum number of requests allowed within the `rate_limit_duration_secs` time window.
  ///
  /// Default if undefined: `1000`
  rate_limit_num: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum number of retries to make for failed requests.
  ///
  /// Default if undefined: `9223372036854775807`
  retry_attempts: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The amount of time to wait before attempting the first retry for a failed request.
  ///
  /// After the first retry has failed, the fibonacci sequence is used to select future backoffs.
  ///
  /// Default if undefined: `1`
  retry_initial_backoff_secs: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The jitter mode to use for retry backoff behavior.
  ///
  /// Default if undefined: `"Full"`
  retry_jitter_mode: Any?

  /// The maximum amount of time to wait between retries.
  ///
  /// Default if undefined: `30`
  retry_max_duration_secs: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The time a request can take before being aborted.
  ///
  /// Datadog highly recommends that you do not lower this value below the service's internal timeout, as
  /// this could create orphaned requests, pile on retries, and result in duplicate data downstream.
  ///
  /// Default if undefined: `60`
  timeout_secs: Int(isBetween(0.0, 9.007199254740991E15))?
}

/// Configuration of the authentication strategy for interacting with GCP services.
///
/// Default if undefined: `{ ["api_key"] = null ["credentials_path"] = null }`
class `Vector::sinks::gcp::pubsub::PubsubConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::gcp::pubsub::PubsubConfigBatch`?

  /// Configures how events are encoded into raw bytes.
  encoding: Any

  /// The endpoint to which to publish events.
  ///
  /// The scheme (`http` or `https`) must be specified. No path should be included since the paths
  /// defined by the [`GCP Pub/Sub`][pubsub_api] API are used.
  ///
  /// The trailing slash `/` must not be included.
  ///
  /// [pubsub_api]: https://cloud.google.com/pubsub/docs/reference/rest
  ///
  /// Default if undefined: `"https://pubsub.googleapis.com"`
  endpoint: String?

  /// The project name to which to publish events.
  project: String

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  tls: Any?

  /// The topic within the project to which to publish events.
  topic: String
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
class `Vector::sinks::gcp::pubsub::PubsubConfigBatch` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  ///
  /// Default if undefined: `10000000`
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  ///
  /// Default if undefined: `1000`
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `1.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Configuration of the authentication strategy for interacting with GCP services.
class `Vector::sinks::gcp::stackdriver::logs::config::StackdriverConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: Any?

  /// Transformations to prepare an event for serialization.
  ///
  /// Default if undefined: `{}`
  encoding: Any?

  /// The log ID to which to publish logs.
  ///
  /// This is a name you create to identify this log stream.
  log_id: Any

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 1000 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: `Vector::sinks::gcpChronicle::chronicleUnstructured::ChronicleUnstructuredConfigRequest`?

  /// A monitored resource.
  ///
  /// Type-specific labels.
  resource: Resource

  /// The field of the log event from which to take the outgoing logâs `severity` field.
  ///
  /// The named field is removed from the log event if present, and must be either an integer between 0
  /// and 800 or a string containing one of the [severity level names][sev_names] (case is ignored) or a
  /// common prefix such as `err`.
  ///
  /// If no severity key is specified, the severity of outgoing records is set to 0 (`DEFAULT`).
  ///
  /// See the [GCP Stackdriver Logging LogSeverity description][logsev_docs] for more details on the
  /// value of the `severity` field.
  ///
  /// [sev_names]: https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logseverity
  /// [logsev_docs]: https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logseverity
  severity_key: Any?

  tls: Any?
}

/// A monitored resource.
///
/// Type-specific labels.
///
/// WARN: both properties and at least one of additionalProperties and patternProperties are set. This is
/// ambiguously defined; can either be defined as a `Mapping` or a class.
class Resource {
  /// The monitored resource type.
  ///
  /// For example, the type of a Compute Engine VM instance is `gce_instance`. See the [Google Cloud
  /// Platform monitored resource documentation][gcp_resources] for more details.
  ///
  /// [gcp_resources]: https://cloud.google.com/monitoring/api/resources
  type: String
}

/// The billing account ID to which to publish logs.
///
/// Exactly one of `billing_account_id`, `folder_id`, `organization_id`, or `project_id` must be set.
class `Vector::sinks::gcp::stackdriver::logs::config::StackdriverConfigAlternate0` {
  billing_account_id: String
}

/// The folder ID to which to publish logs.
///
/// See the [Google Cloud Platform folder documentation][folder_docs] for more details.
///
/// Exactly one of `billing_account_id`, `folder_id`, `organization_id`, or `project_id` must be set.
///
/// [folder_docs]: https://cloud.google.com/resource-manager/docs/creating-managing-folders
class `Vector::sinks::gcp::stackdriver::logs::config::StackdriverConfigAlternate1` {
  folder_id: String
}

/// The organization ID to which to publish logs.
///
/// This would be the identifier assigned to your organization on Google Cloud Platform.
///
/// Exactly one of `billing_account_id`, `folder_id`, `organization_id`, or `project_id` must be set.
class `Vector::sinks::gcp::stackdriver::logs::config::StackdriverConfigAlternate2` {
  organization_id: String
}

/// The project ID to which to publish logs.
///
/// See the [Google Cloud Platform project management documentation][project_docs] for more details.
///
/// Exactly one of `billing_account_id`, `folder_id`, `organization_id`, or `project_id` must be set.
///
/// [project_docs]: https://cloud.google.com/resource-manager/docs/creating-managing-projects
class `Vector::sinks::gcp::stackdriver::logs::config::StackdriverConfigAlternate3` {
  project_id: String
}

/// Configuration of the authentication strategy for interacting with GCP services.
class `Vector::sinks::gcp::stackdriver::metrics::config::StackdriverConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::redis::config::RedisSinkConfigBatch`?

  /// The default namespace to use for metrics that do not have one.
  ///
  /// Metrics with the same name can only be differentiated by their namespace, and not all metrics have
  /// their own namespace.
  ///
  /// Default if undefined: `"namespace"`
  default_namespace: String?

  /// The project ID to which to publish metrics.
  ///
  /// See the [Google Cloud Platform project management documentation][project_docs] for more details.
  ///
  /// [project_docs]: https://cloud.google.com/resource-manager/docs/creating-managing-projects
  project_id: String

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 1000 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: `Vector::sinks::gcpChronicle::chronicleUnstructured::ChronicleUnstructuredConfigRequest`?

  /// A monitored resource.
  ///
  /// Type-specific labels.
  resource: `Vector::sinks::gcp::stackdriver::metrics::config::StackdriverConfigResource`

  tls: Any?
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
class `Vector::sinks::redis::config::RedisSinkConfigBatch` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  ///
  /// Default if undefined: `1`
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `1.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// A monitored resource.
///
/// Type-specific labels.
///
/// WARN: both properties and at least one of additionalProperties and patternProperties are set. This is
/// ambiguously defined; can either be defined as a `Mapping` or a class.
class `Vector::sinks::gcp::stackdriver::metrics::config::StackdriverConfigResource` {
  /// The monitored resource type.
  ///
  /// For example, the type of a Compute Engine VM instance is `gce_instance`.
  type: String
}

/// Configuration of the authentication strategy for interacting with GCP services.
class `Vector::sinks::gcpChronicle::chronicleUnstructured::ChronicleUnstructuredConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::gcpChronicle::chronicleUnstructured::ChronicleUnstructuredConfigBatch`?

  /// The Unique identifier (UUID) corresponding to the Chronicle instance.
  customer_id: String

  /// Configures how events are encoded into raw bytes.
  encoding: Any

  /// The endpoint to send data to.
  endpoint: String?

  /// A set of labels that are attached to each batch of events.
  labels: Mapping<String, String>?

  /// The type of log entries in a request.
  ///
  /// This must be one of the [supported log types][unstructured_log_types_doc], otherwise Chronicle
  /// rejects the entry with an error.
  ///
  /// [unstructured_log_types_doc]:
  /// https://cloud.google.com/chronicle/docs/ingestion/parser-list/supported-default-parsers
  log_type: Any

  /// User-configured environment namespace to identify the data domain the logs originated from.
  namespace: Any?

  /// The GCP region to use.
  region: (Any|"eu"|"us"|"asia")?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 1000 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: `Vector::sinks::gcpChronicle::chronicleUnstructured::ChronicleUnstructuredConfigRequest`?

  tls: Any?
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
class `Vector::sinks::gcpChronicle::chronicleUnstructured::ChronicleUnstructuredConfigBatch` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  ///
  /// Default if undefined: `1000000`
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `15.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Configuration items for GreptimeDB
class `Vector::sinks::greptimedb::GreptimeDBConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::sematext::metrics::SematextMetricsConfigBatch`?

  /// The GreptimeDB [database][database] name to connect.
  ///
  /// Default to `public`, the default database of GreptimeDB.
  ///
  /// Database can be created via `create database` statement on GreptimeDB. If you are using
  /// GreptimeCloud, use `dbname` from the connection information of your instance.
  ///
  /// [database]: https://docs.greptime.com/user-guide/concepts/key-concepts#database
  ///
  /// Default if undefined: `"public"`
  dbname: String?

  /// The host and port of GreptimeDB gRPC service.
  ///
  /// This sink uses GreptimeDB's gRPC interface for data ingestion. By default, GreptimeDB listens to
  /// port 4001 for gRPC protocol.
  ///
  /// The address _must_ include a port.
  endpoint: String

  /// The password for your GreptimeDB instance.
  ///
  /// This is required if your instance has authentication enabled.
  password: Any?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  tls: Any?

  /// The username for your GreptimeDB instance.
  ///
  /// This is required if your instance has authentication enabled.
  username: String?
}

/// Configuration for the `honeycomb` sink.
class `Vector::sinks::honeycomb::config::HoneycombConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The API key that is used to authenticate against Honeycomb.
  api_key: Any

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::honeycomb::config::HoneycombConfigBatch`?

  /// The dataset to which logs are sent.
  dataset: String

  /// Transformations to prepare an event for serialization.
  ///
  /// Default if undefined: `{}`
  encoding: Any?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
class `Vector::sinks::honeycomb::config::HoneycombConfigBatch` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  ///
  /// Default if undefined: `100000`
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `1.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Encoding configuration.
class `Vector::sinks::http::config::HttpSinkConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  auth: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: Any?

  /// Compression configuration.
  ///
  /// All compression algorithms use the default compression level unless otherwise specified.
  ///
  /// Default if undefined: `"none"`
  compression: Any?

  /// A list of custom headers to add to each request.
  @Deprecated
  headers: Mapping<String, String>?

  /// HTTP method.
  ///
  /// The HTTP method to use when making the request.
  ///
  /// Default if undefined: `"post"`
  method: ("get"|"head"|"post"|"put"|"delete"|"options"|"trace"|"patch")?

  /// A string to prefix the payload with.
  ///
  /// This option is ignored if the encoding is not character delimited JSON.
  ///
  /// If specified, the `payload_suffix` must also be specified and together they must produce a valid
  /// JSON object.
  ///
  /// Default if undefined: `""`
  payload_prefix: String?

  /// A string to suffix the payload with.
  ///
  /// This option is ignored if the encoding is not character delimited JSON.
  ///
  /// If specified, the `payload_prefix` must also be specified and together they must produce a valid
  /// JSON object.
  ///
  /// Default if undefined: `""`
  payload_suffix: String?

  /// Outbound HTTP request settings.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } ["headers"] {} }`
  request: Any?

  tls: Any?

  /// The full URI to make HTTP requests to.
  ///
  /// This should include the protocol and host, but can also include the port, path, and any other valid
  /// part of a URI.
  uri: Any
}

/// Configuration for the `humio_logs` sink.
class `Vector::sinks::humio::logs::HumioLogsConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: Any?

  /// Compression configuration.
  ///
  /// All compression algorithms use the default compression level unless otherwise specified.
  ///
  /// Default if undefined: `"none"`
  compression: Any?

  /// Configures how events are encoded into raw bytes.
  encoding: Any

  /// The base URL of the Humio instance.
  ///
  /// The scheme (`http` or `https`) must be specified. No path should be included since the paths
  /// defined by the [`Splunk`][splunk] API are used.
  ///
  /// [splunk]: https://docs.splunk.com/Documentation/Splunk/8.0.0/Data/HECRESTendpoints
  ///
  /// Default if undefined: `"https://cloud.humio.com"`
  endpoint: String?

  /// The type of events sent to this sink. Humio uses this as the name of the parser to use to ingest
  /// the data.
  ///
  /// If unset, Humio defaults it to none.
  event_type: Any?

  /// Overrides the name of the log field used to retrieve the hostname to send to Humio.
  ///
  /// By default, the [global `log_schema.host_key` option][global_host_key] is used if log events are
  /// Legacy namespaced, or the semantic meaning of "host" is used, if defined.
  ///
  /// [global_host_key]:
  /// https://vector.dev/docs/reference/configuration/global-options/#log_schema.host_key
  ///
  /// Default if undefined: `".host"`
  host_key: Any?

  /// Optional name of the repository to ingest into.
  ///
  /// In public-facing APIs, this must (if present) be equal to the repository used to create the ingest
  /// token used for authentication.
  ///
  /// In private cluster setups, Humio can be configured to allow these to be different.
  ///
  /// For more information, see [Humioâs Format of Data][humio_data_format].
  ///
  /// [humio_data_format]: https://docs.humio.com/integrations/data-shippers/hec/#format-of-data
  index: Any?

  /// Event fields to be added to Humioâs extra fields.
  ///
  /// Can be used to tag events by specifying fields starting with `#`.
  ///
  /// For more information, see [Humioâs Format of Data][humio_data_format].
  ///
  /// [humio_data_format]: https://docs.humio.com/integrations/data-shippers/hec/#format-of-data
  ///
  /// Default if undefined: `{}`
  indexed_fields: Listing<Any>?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  /// The source of events sent to this sink.
  ///
  /// Typically the filename the logs originated from. Maps to `@source` in Humio.
  source: Any?

  /// Overrides the name of the log field used to retrieve the timestamp to send to Humio. When set to
  /// `ââ`, a timestamp is not set in the events sent to Humio.
  ///
  /// By default, either the [global `log_schema.timestamp_key` option][global_timestamp_key] is used if
  /// log events are Legacy namespaced, or the semantic meaning of "timestamp" is used, if defined.
  ///
  /// [global_timestamp_key]:
  /// https://vector.dev/docs/reference/configuration/global-options/#log_schema.timestamp_key
  ///
  /// Default if undefined: `".timestamp"`
  timestamp_key: Any?

  /// Overrides the name of the log field used to retrieve the nanosecond-enabled timestamp to send to
  /// Humio.
  ///
  /// Default if undefined: `"@timestamp.nanos"`
  timestamp_nanos_key: String?

  tls: Any?

  /// The Humio ingestion token.
  token: Any
}

/// Configuration for the `metric_to_log` transform.
class `Vector::sinks::humio::metrics::HumioMetricsConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: Any?

  /// Compression configuration.
  ///
  /// All compression algorithms use the default compression level unless otherwise specified.
  ///
  /// Default if undefined: `"none"`
  compression: Any?

  /// The base URL of the Humio instance.
  ///
  /// The scheme (`http` or `https`) must be specified. No path should be included since the paths
  /// defined by the [`Splunk`][splunk] API are used.
  ///
  /// [splunk]: https://docs.splunk.com/Documentation/Splunk/8.0.0/Data/HECRESTendpoints
  ///
  /// Default if undefined: `"https://cloud.humio.com"`
  endpoint: String?

  /// The type of events sent to this sink. Humio uses this as the name of the parser to use to ingest
  /// the data.
  ///
  /// If unset, Humio defaults it to none.
  event_type: Any?

  /// Overrides the name of the log field used to retrieve the hostname to send to Humio.
  ///
  /// By default, the [global `log_schema.host_key` option][global_host_key] is used if log events are
  /// Legacy namespaced, or the semantic meaning of "host" is used, if defined.
  ///
  /// [global_host_key]:
  /// https://vector.dev/docs/reference/configuration/global-options/#log_schema.host_key
  ///
  /// Default if undefined: `"host"`
  host_key: Any?

  /// Optional name of the repository to ingest into.
  ///
  /// In public-facing APIs, this must (if present) be equal to the repository used to create the ingest
  /// token used for authentication.
  ///
  /// In private cluster setups, Humio can be configured to allow these to be different.
  ///
  /// For more information, see [Humioâs Format of Data][humio_data_format].
  ///
  /// [humio_data_format]: https://docs.humio.com/integrations/data-shippers/hec/#format-of-data
  index: Any?

  /// Event fields to be added to Humioâs extra fields.
  ///
  /// Can be used to tag events by specifying fields starting with `#`.
  ///
  /// For more information, see [Humioâs Format of Data][humio_data_format].
  ///
  /// [humio_data_format]: https://docs.humio.com/integrations/data-shippers/hec/#format-of-data
  ///
  /// Default if undefined: `{}`
  indexed_fields: Listing<Any>?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  /// The source of events sent to this sink.
  ///
  /// Typically the filename the metrics originated from. Maps to `@source` in Humio.
  source: Any?

  tls: Any?

  /// The Humio ingestion token.
  token: Any
}

/// Configuration for the `influxdb_logs` sink.
class `Vector::sinks::influxdb::logs::InfluxDbLogsConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::influxdb::logs::InfluxDbLogsConfigBatch`?

  /// Transformations to prepare an event for serialization.
  ///
  /// Default if undefined: `{}`
  encoding: Any?

  /// The endpoint to send data to.
  ///
  /// This should be a full HTTP URI, including the scheme, host, and port.
  endpoint: String

  /// Use this option to customize the key containing the hostname.
  ///
  /// The setting of `log_schema.host_key`, usually `host`, is used here by default.
  host_key: Any?

  /// The name of the InfluxDB measurement that is written to.
  measurement: String?

  /// Use this option to customize the key containing the message.
  ///
  /// The setting of `log_schema.message_key`, usually `message`, is used here by default.
  message_key: Any?

  /// The namespace of the measurement name to use.
  ///
  /// When specified, the measurement name is `<namespace>.vector`.
  @Deprecated
  namespace: String?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  /// Use this option to customize the key containing the source_type.
  ///
  /// The setting of `log_schema.source_type_key`, usually `source_type`, is used here by default.
  source_type_key: Any?

  /// The list of names of log fields that should be added as tags to each measurement.
  ///
  /// By default Vector adds `metric_type` as well as the configured `log_schema.host_key` and
  /// `log_schema.source_type_key` options.
  ///
  /// Default if undefined: `{}`
  tags: Listing<String>?

  tls: Any?
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
class `Vector::sinks::influxdb::logs::InfluxDbLogsConfigBatch` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  ///
  /// Default if undefined: `1000000`
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `1.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Configuration for the `influxdb_metrics` sink.
class `Vector::sinks::influxdb::metrics::InfluxDbConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::sematext::metrics::SematextMetricsConfigBatch`?

  /// Sets the default namespace for any metrics sent.
  ///
  /// This namespace is only used if a metric has no existing namespace. When a namespace is present, it
  /// is used as a prefix to the metric name, and separated with a period (`.`).
  default_namespace: String?

  /// The endpoint to send data to.
  ///
  /// This should be a full HTTP URI, including the scheme, host, and port.
  endpoint: String

  /// The list of quantiles to calculate when sending distribution metrics.
  ///
  /// Default if undefined: `{ 0.5 0.75 0.9 0.95 0.99 }`
  quantiles: Listing<Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))>?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  /// A map of additional tags, in the key/value pair format, to add to each measurement.
  tags: Mapping<String, String>?

  tls: Any?
}

/// Kafka authentication configuration.
class `Vector::sinks::kafka::config::KafkaSinkConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::kafka::config::KafkaSinkConfigBatch`?

  /// A comma-separated list of Kafka bootstrap servers.
  ///
  /// These are the servers in a Kafka cluster that a client should use to bootstrap its connection to
  /// the cluster, allowing discovery of all the other hosts in the cluster.
  ///
  /// Must be in the form of `host:port`, and comma-separated.
  bootstrap_servers: String

  /// Supported compression types for Kafka.
  ///
  /// Default if undefined: `"none"`
  compression: ("none"|"gzip"|"snappy"|"lz4"|"zstd")?

  /// Configures how events are encoded into raw bytes.
  encoding: Any

  /// The log field name to use for the Kafka headers.
  ///
  /// If omitted, no headers are written.
  headers_key: (Any|Any)?

  /// The topic name to use for healthcheck. If omitted, `topic` is used. This option helps prevent
  /// healthcheck warnings when `topic` is templated.
  ///
  /// It is ignored when healthcheck is disabled.
  healthcheck_topic: String?

  /// The log field name or tag key to use for the topic key.
  ///
  /// If the field does not exist in the log or in the tags, a blank value is used. If unspecified, the
  /// key is not sent.
  ///
  /// Kafka uses a hash of the key to choose the partition or uses round-robin if the record has no key.
  key_field: (Any|Any)?

  /// A map of advanced options to pass directly to the underlying `librdkafka` client.
  ///
  /// For more information on configuration options, see [Configuration properties][config_props_docs].
  ///
  /// [config_props_docs]: https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md
  ///
  /// Default if undefined: `{}`
  librdkafka_options: Mapping<String, String>?

  /// Local message timeout, in milliseconds.
  ///
  /// Default if undefined: `300000`
  message_timeout_ms: Any?

  /// Default timeout, in milliseconds, for network requests.
  ///
  /// Default if undefined: `60000`
  socket_timeout_ms: Any?

  /// The Kafka topic name to write events to.
  topic: Any
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
class `Vector::sinks::kafka::config::KafkaSinkConfigBatch` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `1.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Configuration for the `loki` sink.
class `Vector::sinks::loki::config::LokiConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  auth: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::loki::config::LokiConfigBatch`?

  /// Compression configuration. Snappy compression implies sending push requests as Protocol Buffers.
  ///
  /// Default if undefined: `"snappy"`
  compression: Any?

  /// Configures how events are encoded into raw bytes.
  encoding: Any

  /// The base URL of the Loki instance.
  ///
  /// The `path` value is appended to this.
  endpoint: Any

  /// A set of labels that are attached to each batch of events.
  ///
  /// Both keys and values are templateable, which enables you to attach dynamic labels to events.
  ///
  /// Valid label keys include `*`, and prefixes ending with `*`, to allow for the expansion of objects
  /// into multiple labels. See [Label expansion][label_expansion] for more information.
  ///
  /// Note: If the set of labels has high cardinality, this can cause drastic performance issues with
  /// Loki. To prevent this from happening, reduce the number of unique label keys and values.
  ///
  /// [label_expansion]: https://vector.dev/docs/reference/configuration/sinks/loki/#label-expansion
  labels: Mapping<String, Any>?

  /// Out-of-order event behavior.
  ///
  /// Some sources may generate events with timestamps that aren't in chronological order. Even though
  /// the sink sorts the events before sending them to Loki, there is a chance that another event could
  /// come in that is out of order with the latest events sent to Loki. Prior to Loki 2.4.0, this was not
  /// supported and would result in an error during the push request.
  ///
  /// If you're using Loki 2.4.0 or newer, `Accept` is the preferred action, which lets Loki handle any
  /// necessary sorting/reordering. If you're using an earlier version, then you must use `Drop` or
  /// `RewriteTimestamp` depending on which option makes the most sense for your use case.
  ///
  /// Default if undefined: `"accept"`
  out_of_order_action: ("accept"|"rewrite_timestamp"|"drop")?

  /// The path to use in the URL of the Loki instance.
  ///
  /// Default if undefined: `"/loki/api/v1/push"`
  path: String?

  /// Whether or not to delete fields from the event when they are used as labels.
  ///
  /// Default if undefined: `false`
  remove_label_fields: Boolean?

  /// Whether or not to remove the timestamp from the event payload.
  ///
  /// The timestamp is still sent as event metadata for Loki to use for indexing.
  ///
  /// Default if undefined: `true`
  remove_timestamp: Boolean?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  /// The [tenant ID][tenant_id] to specify in requests to Loki.
  ///
  /// When running Loki locally, a tenant ID is not required.
  ///
  /// [tenant_id]: https://grafana.com/docs/loki/latest/operations/multi-tenancy/
  tenant_id: Any?

  tls: Any?
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
class `Vector::sinks::loki::config::LokiConfigBatch` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  ///
  /// Default if undefined: `1000000`
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  ///
  /// Default if undefined: `100000`
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `1.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Configuration for the `mezmo` (formerly `logdna`) sink.
class `Vector::sinks::mezmo::MezmoConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The Ingestion API key.
  api_key: Any

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: Any?

  /// The default app that is set for events that do not contain a `file` or `app` field.
  ///
  /// Default if undefined: `"vector"`
  default_app: String?

  /// The default environment that is set for events that do not contain an `env` field.
  ///
  /// Default if undefined: `"production"`
  default_env: String?

  /// Transformations to prepare an event for serialization.
  ///
  /// Default if undefined: `{}`
  encoding: Any?

  /// The HTTP endpoint to send logs to.
  ///
  /// Both IP address and hostname are accepted formats.
  ///
  /// Default if undefined: `"https://logs.mezmo.com/"`
  endpoint: Any?

  /// The hostname that is attached to each batch of events.
  hostname: Any

  /// The IP address that is attached to each batch of events.
  ip: String?

  /// The MAC address that is attached to each batch of events.
  mac: String?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  /// The tags that are attached to each batch of events.
  tags: Listing<Any>?
}

/// Configuration for the `mqtt` sink
class `Vector::sinks::mqtt::config::MqttSinkConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// If set to true, the MQTT session is cleaned on login.
  ///
  /// Default if undefined: `false`
  clean_session: Boolean?

  /// MQTT client ID.
  client_id: String?

  /// Configures how events are encoded into raw bytes.
  encoding: Any

  /// MQTT server address (The brokerâs domain name or IP address).
  host: String

  /// Connection keep-alive interval.
  ///
  /// Default if undefined: `60`
  keep_alive: UInt16?

  /// MQTT password.
  password: String?

  /// TCP port of the MQTT server to connect to.
  ///
  /// Default if undefined: `1883`
  port: UInt16?

  /// Supported Quality of Service types for MQTT.
  ///
  /// Default if undefined: `"atleastonce"`
  quality_of_service: ("atleastonce"|"atmostonce"|"exactlyonce")?

  tls: Any?

  /// MQTT publish topic (templates allowed)
  topic: Any

  /// MQTT username.
  user: String?
}

/// Configuration for the `nats` sink.
class `Vector::sinks::nats::config::NatsSinkConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  auth: Any?

  /// A NATS [name][nats_connection_name] assigned to the NATS connection.
  ///
  /// [nats_connection_name]: https://docs.nats.io/using-nats/developer/connecting/name
  ///
  /// Default if undefined: `"vector"`
  connection_name: String?

  /// Configures how events are encoded into raw bytes.
  encoding: Any

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: `Vector::sinks::redis::config::RedisSinkConfigRequest`?

  /// The NATS [subject][nats_subject] to publish messages to.
  ///
  /// [nats_subject]: https://docs.nats.io/nats-concepts/subjects
  subject: Any

  tls: Any?

  /// The NATS [URL][nats_url] to connect to.
  ///
  /// The URL must take the form of `nats://server:port`. If the port is not specified it defaults to
  /// 4222.
  ///
  /// [nats_url]: https://docs.nats.io/using-nats/developer/connecting#nats-url
  url: String
}

/// Middleware settings for outbound requests.
///
/// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
/// etc.
///
/// Note that the retry backoff policy follows the Fibonacci sequence.
///
/// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"] =
/// 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
/// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
/// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
/// 2.5 ["max_concurrency_limit"] = 200 } }`
class `Vector::sinks::redis::config::RedisSinkConfigRequest` {
  /// Configuration of adaptive concurrency parameters.
  ///
  /// These parameters typically do not require changes from the default, and incorrect values can lead
  /// to meta-stable or unstable performance and sink behavior. Proceed with caution.
  ///
  /// Default if undefined: `{ ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4
  /// ["rtt_deviation_scale"] = 2.5 ["max_concurrency_limit"] = 200 }`
  adaptive_concurrency: Any?

  /// Configuration for outbound request concurrency.
  ///
  /// This can be set either to one of the below enum values or to a positive integer, which denotes a
  /// fixed concurrency limit.
  ///
  /// Default if undefined: `"none"`
  concurrency: Any?

  /// The time window used for the `rate_limit_num` option.
  ///
  /// Default if undefined: `1`
  rate_limit_duration_secs: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum number of requests allowed within the `rate_limit_duration_secs` time window.
  ///
  /// Default if undefined: `9223372036854775807`
  rate_limit_num: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum number of retries to make for failed requests.
  ///
  /// Default if undefined: `9223372036854775807`
  retry_attempts: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The amount of time to wait before attempting the first retry for a failed request.
  ///
  /// After the first retry has failed, the fibonacci sequence is used to select future backoffs.
  ///
  /// Default if undefined: `1`
  retry_initial_backoff_secs: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The jitter mode to use for retry backoff behavior.
  ///
  /// Default if undefined: `"Full"`
  retry_jitter_mode: Any?

  /// The maximum amount of time to wait between retries.
  ///
  /// Default if undefined: `30`
  retry_max_duration_secs: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The time a request can take before being aborted.
  ///
  /// Datadog highly recommends that you do not lower this value below the service's internal timeout, as
  /// this could create orphaned requests, pile on retries, and result in duplicate data downstream.
  ///
  /// Default if undefined: `60`
  timeout_secs: Int(isBetween(0.0, 9.007199254740991E15))?
}

/// Configuration for the `new_relic` sink.
class `Vector::sinks::newRelic::config::NewRelicConfig` {
  /// The New Relic account ID.
  account_id: Any

  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// New Relic API endpoint.
  api: "events"|"metrics"|"logs"

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::newRelic::config::NewRelicConfigBatch`?

  /// Compression configuration.
  ///
  /// All compression algorithms use the default compression level unless otherwise specified.
  ///
  /// Default if undefined: `"gzip"`
  compression: Any?

  /// Transformations to prepare an event for serialization.
  ///
  /// Default if undefined: `{}`
  encoding: Any?

  /// A valid New Relic license key.
  license_key: Any

  /// New Relic region.
  region: (Any|"us"|"eu")?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
class `Vector::sinks::newRelic::config::NewRelicConfigBatch` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  ///
  /// Default if undefined: `1000000`
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  ///
  /// Default if undefined: `100`
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `1.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Configuration for the `papertrail` sink.
class `Vector::sinks::papertrail::PapertrailConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Configures how events are encoded into raw bytes.
  encoding: Any

  /// The TCP endpoint to send logs to.
  endpoint: Any

  keepalive: Any?

  /// The value to use as the `process` in Papertrail.
  ///
  /// Default if undefined: `"vector"`
  process: Any?

  /// Configures the send buffer size using the `SO_SNDBUF` option on the socket.
  send_buffer_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  tls: Any?
}

/// Configuration for the `prometheus_exporter` sink.
class `Vector::sinks::prometheus::exporter::PrometheusExporterConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The address to expose for scraping.
  ///
  /// The metrics are exposed at the typical Prometheus exporter path, `/metrics`.
  ///
  /// Default if undefined: `"0.0.0.0:9598"`
  address: Any?

  auth: Any?

  /// Default buckets to use for aggregating [distribution][dist_metric_docs] metrics into histograms.
  ///
  /// [dist_metric_docs]:
  /// https://vector.dev/docs/about/under-the-hood/architecture/data-model/metric/#distribution
  ///
  /// Default if undefined: `{ 0.005 0.01 0.025 0.05 0.1 0.25 0.5 1.0 2.5 5.0 10.0 }`
  buckets: Listing<Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))>?

  /// The default namespace for any metrics sent.
  ///
  /// This namespace is only used if a metric has no existing namespace. When a namespace is present, it
  /// is used as a prefix to the metric name, and separated with an underscore (`_`).
  ///
  /// It should follow the Prometheus [naming conventions][prom_naming_docs].
  ///
  /// [prom_naming_docs]: https://prometheus.io/docs/practices/naming/#metric-names
  default_namespace: String?

  /// Whether or not to render [distributions][dist_metric_docs] as an [aggregated
  /// histogram][prom_agg_hist_docs] or [aggregated summary][prom_agg_summ_docs].
  ///
  /// While distributions as a lossless way to represent a set of samples for a metric is supported,
  /// Prometheus clients (the application being scraped, which is this sink) must aggregate locally into
  /// either an aggregated histogram or aggregated summary.
  ///
  /// [dist_metric_docs]:
  /// https://vector.dev/docs/about/under-the-hood/architecture/data-model/metric/#distribution
  /// [prom_agg_hist_docs]: https://prometheus.io/docs/concepts/metric_types/#histogram
  /// [prom_agg_summ_docs]: https://prometheus.io/docs/concepts/metric_types/#summary
  ///
  /// Default if undefined: `false`
  distributions_as_summaries: Boolean?

  /// The interval, in seconds, on which metrics are flushed.
  ///
  /// On the flush interval, if a metric has not been seen since the last flush interval, it is
  /// considered expired and is removed.
  ///
  /// Be sure to configure this value higher than your clientâs scrape interval.
  ///
  /// Default if undefined: `60`
  flush_period_secs: Any?

  /// Quantiles to use for aggregating [distribution][dist_metric_docs] metrics into a summary.
  ///
  /// [dist_metric_docs]:
  /// https://vector.dev/docs/about/under-the-hood/architecture/data-model/metric/#distribution
  ///
  /// Default if undefined: `{ 0.5 0.75 0.9 0.95 0.99 }`
  quantiles: Listing<Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))>?

  /// Suppresses timestamps on the Prometheus output.
  ///
  /// This can sometimes be useful when the source of metrics leads to their timestamps being too far in
  /// the past for Prometheus to allow them, such as when aggregating metrics over long time periods, or
  /// when replaying old metrics from a disk buffer.
  ///
  /// Default if undefined: `false`
  suppress_timestamp: Boolean?

  tls: Any?
}

/// Configuration for the `prometheus_remote_write` sink.
class `Vector::sinks::prometheus::remoteWrite::config::RemoteWriteConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Authentication strategies.
  auth: (
    Any
    |AuthAlternate1AuthAlternate1Alternate0|AuthAlternate1AuthAlternate1Alternate1|AuthAlternate1Alternate2)?

  aws: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null
  /// ["aggregate"] = true }`
  batch: `Vector::sinks::prometheus::remoteWrite::config::RemoteWriteConfigBatch`?

  /// Default buckets to use for aggregating [distribution][dist_metric_docs] metrics into histograms.
  ///
  /// [dist_metric_docs]:
  /// https://vector.dev/docs/about/under-the-hood/architecture/data-model/metric/#distribution
  ///
  /// Default if undefined: `{ 0.005 0.01 0.025 0.05 0.1 0.25 0.5 1.0 2.5 5.0 10.0 }`
  buckets: Listing<Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))>?

  /// Compression configuration.
  ///
  /// All compression algorithms use the default compression level unless otherwise specified.
  ///
  /// Default if undefined: `"snappy"`
  compression: Any?

  /// The default namespace for any metrics sent.
  ///
  /// This namespace is only used if a metric has no existing namespace. When a namespace is present, it
  /// is used as a prefix to the metric name, and separated with an underscore (`_`).
  ///
  /// It should follow the Prometheus [naming conventions][prom_naming_docs].
  ///
  /// [prom_naming_docs]: https://prometheus.io/docs/practices/naming/#metric-names
  default_namespace: String?

  /// The endpoint to send data to.
  ///
  /// The endpoint should include the scheme and the path to write to.
  endpoint: String

  /// Quantiles to use for aggregating [distribution][dist_metric_docs] metrics into a summary.
  ///
  /// [dist_metric_docs]:
  /// https://vector.dev/docs/about/under-the-hood/architecture/data-model/metric/#distribution
  ///
  /// Default if undefined: `{ 0.5 0.75 0.9 0.95 0.99 }`
  quantiles: Listing<Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))>?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  /// The tenant ID to send.
  ///
  /// If set, a header named `X-Scope-OrgID` is added to outgoing requests with the value of this
  /// setting.
  ///
  /// This may be used by Cortex or other remote services to identify the tenant making the request.
  tenant_id: Any?

  tls: Any?
}

/// HTTP Basic Authentication.
class AuthAlternate1AuthAlternate1Alternate0 {
  /// Basic authentication password.
  password: String

  /// HTTP Basic Authentication.
  strategy: "basic"

  /// Basic authentication username.
  user: String
}

/// Bearer authentication.
///
/// A bearer token (OAuth2, JWT, etc) is passed as-is.
class AuthAlternate1AuthAlternate1Alternate1 {
  /// Bearer authentication.
  ///
  /// A bearer token (OAuth2, JWT, etc) is passed as-is.
  strategy: "bearer"

  /// The bearer token to send.
  token: Any
}

/// Configuration of the authentication strategy for interacting with AWS services.
class AuthAlternate1Alternate2 {
  /// Amazon Prometheus Service-specific authentication.
  strategy: "aws"
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null
/// ["aggregate"] = true }`
class `Vector::sinks::prometheus::remoteWrite::config::RemoteWriteConfigBatch` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  ///
  /// Default if undefined: `1000`
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `1.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Configuration for the `pulsar` sink.
class `Vector::sinks::pulsar::config::PulsarSinkConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Authentication configuration.
  auth: (Any|AuthAlternate1)?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_events"] = null ["max_bytes"] = null }`
  batch: `Vector::sinks::pulsar::config::PulsarSinkConfigBatch`?

  /// Supported compression types for Pulsar.
  ///
  /// Default if undefined: `"none"`
  compression: ("none"|"lz4"|"zlib"|"zstd"|"snappy")?

  /// Configures how events are encoded into raw bytes.
  encoding: Any

  /// The endpoint to which the Pulsar client should connect to.
  ///
  /// The endpoint should specify the pulsar protocol and port.
  endpoint: String

  /// The log field name or tags key to use for the partition key.
  ///
  /// If the field does not exist in the log event or metric tags, a blank value will be used.
  ///
  /// If omitted, the key is not sent.
  ///
  /// Pulsar uses a hash of the key to choose the topic-partition or uses round-robin if the record has
  /// no key.
  partition_key_field: Any?

  /// The name of the producer. If not specified, the default name assigned by Pulsar is used.
  producer_name: String?

  /// The log field name to use for the Pulsar properties key.
  ///
  /// If omitted, no properties will be written.
  properties_key: Any?

  /// The Pulsar topic name to write events to.
  topic: Any
}

/// Authentication configuration.
class AuthAlternate1 {
  /// Basic authentication name/username.
  ///
  /// This can be used either for basic authentication (username/password) or JWT authentication. When
  /// used for JWT, the value should be `token`.
  name: String?

  /// OAuth2-specific authentication configuration.
  oauth2: (Any|Oauth2)?

  /// Basic authentication password/token.
  ///
  /// This can be used either for basic authentication (username/password) or JWT authentication. When
  /// used for JWT, the value should be the signed JWT, in the compact representation.
  token: Any?
}

/// OAuth2-specific authentication configuration.
class Oauth2 {
  /// The OAuth2 audience.
  audience: String?

  /// The credentials URL.
  ///
  /// A data URL is also supported.
  credentials_url: String

  /// The issuer URL.
  issuer_url: String

  /// The OAuth2 scope.
  scope: String?
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_events"] = null ["max_bytes"] = null }`
class `Vector::sinks::pulsar::config::PulsarSinkConfigBatch` {
  /// The maximum size of a batch before it is flushed.
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum amount of events in a batch before it is flushed.
  ///
  /// Note this is an unsigned 32 bit integer which is a smaller capacity than many of the other sink
  /// batch settings.
  max_events: Number(isBetween(0.0, 4.294967295E9))?
}

/// Configuration for the `redis` sink.
class `Vector::sinks::redis::config::RedisSinkConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::redis::config::RedisSinkConfigBatch`?

  /// Redis data type to store messages in.
  ///
  /// Default if undefined: `"list"`
  data_type: ("list"|"channel")?

  /// Configures how events are encoded into raw bytes.
  encoding: Any

  /// The URL of the Redis endpoint to connect to.
  ///
  /// The URL _must_ take the form of `protocol://server:port/db` where the protocol can either be
  /// `redis` or `rediss` for connections secured via TLS.
  endpoint: String

  /// The Redis key to publish messages to.
  key: Any

  /// List-specific options.
  list_option: (Any|ListOptionAlternate1)?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: `Vector::sinks::redis::config::RedisSinkConfigRequest`?
}

/// List-specific options.
class ListOptionAlternate1 {
  /// The method to use for pushing messages into a `list`.
  method: "rpush"|"lpush"
}

/// Configuration for the `sematext_logs` sink.
class `Vector::sinks::sematext::logs::SematextLogsConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: Any?

  /// Transformations to prepare an event for serialization.
  ///
  /// Default if undefined: `{}`
  encoding: Any?

  /// The endpoint to send data to.
  ///
  /// Setting this option overrides the `region` option.
  endpoint: String?

  /// The Sematext region to send data to.
  ///
  /// Default if undefined: `"us"`
  region: Any?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  /// The token that is used to write to Sematext.
  token: Any
}

/// Configuration for the `sematext_metrics` sink.
class `Vector::sinks::sematext::metrics::SematextMetricsConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::sematext::metrics::SematextMetricsConfigBatch`?

  /// Sets the default namespace for any metrics sent.
  ///
  /// This namespace is only used if a metric has no existing namespace. When a namespace is present, it
  /// is used as a prefix to the metric name, and separated with a period (`.`).
  default_namespace: String

  /// The endpoint to send data to.
  ///
  /// Setting this option overrides the `region` option.
  endpoint: String?

  /// The Sematext region to send data to.
  ///
  /// Default if undefined: `"us"`
  region: Any?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  /// The token that is used to write to Sematext.
  token: Any
}

/// Socket mode.
class `Vector::sinks::socket::SocketSinkConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?
}

/// Encoding configuration.
class `Vector::sinks::socket::SocketSinkConfigAlternate0` {
  /// Send over TCP.
  mode: "tcp"
}

/// A UDP sink.
class `Vector::sinks::socket::SocketSinkConfigAlternate1` {
  /// Send over UDP.
  mode: "udp"
}

/// Encoding configuration.
class `Vector::sinks::socket::SocketSinkConfigAlternate2` {
  /// Send over a Unix domain socket (UDS).
  mode: "unix"
}

/// Controls how acknowledgements are handled for this sink.
///
/// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
/// handled.
///
/// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
///
/// Default if undefined: `{ ["enabled"] = null }`
class `Vector::sinks::splunkHec::common::acknowledgements::HecClientAcknowledgementsConfig` {
  /// Whether or not end-to-end acknowledgements are enabled.
  ///
  /// When enabled for a sink, any source connected to that sink, where the source supports end-to-end
  /// acknowledgements as well, waits for events to be acknowledged by the sink before acknowledging them
  /// at the source.
  ///
  /// Enabling or disabling acknowledgements at the sink level takes precedence over any global
  /// [`acknowledgements`][global_acks] configuration.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  enabled: Boolean?
}

/// Configuration for the `splunk_hec_logs` sink.
class `Vector::sinks::splunkHec::logs::config::HecLogsSinkConfig` {
  /// Splunk HEC acknowledgement configuration.
  ///
  /// Default if undefined: `{ ["indexer_acknowledgements_enabled"] = true ["query_interval"] = 10
  /// ["retry_limit"] = 30 ["max_pending_acks"] = 1000000 }`
  acknowledgements: Any?

  /// Passes the `auto_extract_timestamp` option to Splunk.
  ///
  /// This option is only relevant to Splunk v8.x and above, and is only applied when `endpoint_target`
  /// is set to `event`.
  ///
  /// Setting this to `true` causes Splunk to extract the timestamp from the message text rather than use
  /// the timestamp embedded in the event. The timestamp must be in the format `yyyy-mm-dd hh:mm:ss`.
  auto_extract_timestamp: Boolean?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: Any?

  /// Compression configuration.
  ///
  /// All compression algorithms use the default compression level unless otherwise specified.
  ///
  /// Default if undefined: `"none"`
  compression: Any?

  /// Default Splunk HEC token.
  ///
  /// If an event has a token set in its secrets (`splunk_hec_token`), it prevails over the one set here.
  default_token: Any

  /// Configures how events are encoded into raw bytes.
  encoding: Any

  /// The base URL of the Splunk instance.
  ///
  /// The scheme (`http` or `https`) must be specified. No path should be included since the paths
  /// defined by the [`Splunk`][splunk] API are used.
  ///
  /// [splunk]: https://docs.splunk.com/Documentation/Splunk/8.0.0/Data/HECRESTendpoints
  endpoint: Uri

  /// Splunk HEC endpoint configuration.
  ///
  /// Default if undefined: `"event"`
  endpoint_target: ("raw"|"event")?

  /// Overrides the name of the log field used to retrieve the hostname to send to Splunk HEC.
  ///
  /// By default, the [global `log_schema.host_key` option][global_host_key] is used if log events are
  /// Legacy namespaced, or the semantic meaning of "host" is used, if defined.
  ///
  /// [global_host_key]:
  /// https://vector.dev/docs/reference/configuration/global-options/#log_schema.host_key
  host_key: Any?

  /// The name of the index to send events to.
  ///
  /// If not specified, the default index defined within Splunk is used.
  index: Any?

  /// Fields to be [added to Splunk index][splunk_field_index_docs].
  ///
  /// [splunk_field_index_docs]: https://docs.splunk.com/Documentation/Splunk/8.0.0/Data/IFXandHEC
  ///
  /// Default if undefined: `{}`
  indexed_fields: Listing<Any>?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  /// The source of events sent to this sink.
  ///
  /// This is typically the filename the logs originated from.
  ///
  /// If unset, the Splunk collector sets it.
  source: Any?

  /// The sourcetype of events sent to this sink.
  ///
  /// If unset, Splunk defaults to `httpevent`.
  sourcetype: Any?

  /// Overrides the name of the log field used to retrieve the timestamp to send to Splunk HEC. When set
  /// to `ââ`, a timestamp is not set in the events sent to Splunk HEC.
  ///
  /// By default, either the [global `log_schema.timestamp_key` option][global_timestamp_key] is used if
  /// log events are Legacy namespaced, or the semantic meaning of "timestamp" is used, if defined.
  ///
  /// [global_timestamp_key]:
  /// https://vector.dev/docs/reference/configuration/global-options/#log_schema.timestamp_key
  timestamp_key: Any?

  tls: Any?
}

/// Configuration of the `splunk_hec_metrics` sink.
class `Vector::sinks::splunkHec::metrics::config::HecMetricsSinkConfig` {
  /// Splunk HEC acknowledgement configuration.
  ///
  /// Default if undefined: `{ ["indexer_acknowledgements_enabled"] = true ["query_interval"] = 10
  /// ["retry_limit"] = 30 ["max_pending_acks"] = 1000000 }`
  acknowledgements: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: Any?

  /// Compression configuration.
  ///
  /// All compression algorithms use the default compression level unless otherwise specified.
  ///
  /// Default if undefined: `"none"`
  compression: Any?

  /// Sets the default namespace for any metrics sent.
  ///
  /// This namespace is only used if a metric has no existing namespace. When a namespace is present, it
  /// is used as a prefix to the metric name, and separated with a period (`.`).
  default_namespace: String?

  /// Default Splunk HEC token.
  ///
  /// If an event has a token set in its metadata, it prevails over the one set here.
  default_token: Any

  /// The base URL of the Splunk instance.
  ///
  /// The scheme (`http` or `https`) must be specified. No path should be included since the paths
  /// defined by the [`Splunk`][splunk] API are used.
  ///
  /// [splunk]: https://docs.splunk.com/Documentation/Splunk/8.0.0/Data/HECRESTendpoints
  endpoint: Uri

  /// Overrides the name of the log field used to retrieve the hostname to send to Splunk HEC.
  ///
  /// By default, the [global `log_schema.host_key` option][global_host_key] is used.
  ///
  /// [global_host_key]:
  /// https://vector.dev/docs/reference/configuration/global-options/#log_schema.host_key
  ///
  /// Default if undefined: `"host"`
  host_key: Any?

  /// The name of the index where to send the events to.
  ///
  /// If not specified, the default index defined within Splunk is used.
  index: Any?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  /// The source of events sent to this sink.
  ///
  /// This is typically the filename the logs originated from.
  ///
  /// If unset, the Splunk collector sets it.
  source: Any?

  /// The sourcetype of events sent to this sink.
  ///
  /// If unset, Splunk defaults to `httpevent`.
  sourcetype: Any?

  tls: Any?
}

/// Socket mode.
class `Vector::sinks::statsd::config::StatsdSinkConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::statsd::config::StatsdSinkConfigBatch`?

  /// Sets the default namespace for any metrics sent.
  ///
  /// This namespace is only used if a metric has no existing namespace. When a namespace is present, it
  /// is used as a prefix to the metric name, and separated with a period (`.`).
  default_namespace: String?
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
class `Vector::sinks::statsd::config::StatsdSinkConfigBatch` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  ///
  /// Default if undefined: `1300`
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  ///
  /// Default if undefined: `1000`
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `1.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// TCP configuration.
class `Vector::sinks::statsd::config::StatsdSinkConfigAlternate0` {
  /// Send over TCP.
  mode: "tcp"
}

/// UDP configuration.
class `Vector::sinks::statsd::config::StatsdSinkConfigAlternate1` {
  /// Send over UDP.
  mode: "udp"
}

/// Unix Domain Socket configuration.
class `Vector::sinks::statsd::config::StatsdSinkConfigAlternate2` {
  /// Send over a Unix domain socket (UDS).
  mode: "unix"
}

/// Configuration of adaptive concurrency parameters.
///
/// These parameters typically do not require changes from the default, and incorrect values can lead to
/// meta-stable or unstable performance and sink behavior. Proceed with caution.
class `Vector::sinks::util::adaptiveConcurrency::AdaptiveConcurrencySettings` {
  /// The fraction of the current value to set the new concurrency limit when decreasing the limit.
  ///
  /// Valid values are greater than `0` and less than `1`. Smaller values cause the algorithm to scale
  /// back rapidly when latency increases.
  ///
  /// Note that the new limit is rounded down after applying this ratio.
  ///
  /// Default if undefined: `0.9`
  decrease_ratio: Number(isBetween(0.0, 1.0))?

  /// The weighting of new measurements compared to older measurements.
  ///
  /// Valid values are greater than `0` and less than `1`.
  ///
  /// ARC uses an exponentially weighted moving average (EWMA) of past RTT measurements as a reference to
  /// compare with the current RTT. Smaller values cause this reference to adjust more slowly, which may
  /// be useful if a service has unusually high response variability.
  ///
  /// Default if undefined: `0.4`
  ewma_alpha: Number(isBetween(0.0, 1.0))?

  /// The initial concurrency limit to use. If not specified, the initial limit will be 1 (no
  /// concurrency).
  ///
  /// It is recommended to set this value to your service's average limit if you're seeing that it takes
  /// a long time to ramp up adaptive concurrency after a restart. You can find this value by looking at
  /// the `adaptive_concurrency_limit` metric.
  ///
  /// Default if undefined: `1`
  initial_concurrency: Int(isBetween(1.0, 9.007199254740991E15))?

  /// The maximum concurrency limit.
  ///
  /// The adaptive request concurrency limit will not go above this bound. This is put in place as a
  /// safeguard.
  ///
  /// Default if undefined: `200`
  max_concurrency_limit: Int(isBetween(1.0, 9.007199254740991E15))?

  /// Scale of RTT deviations which are not considered anomalous.
  ///
  /// Valid values are greater than or equal to `0`, and we expect reasonable values to range from `1.0`
  /// to `3.0`.
  ///
  /// When calculating the past RTT average, we also compute a secondary âdeviationâ value that indicates
  /// how variable those values are. We use that deviation when comparing the past RTT average to the
  /// current measurements, so we can ignore increases in RTT that are within an expected range. This
  /// factor is used to scale up the deviation to an appropriate range. Larger values cause the algorithm
  /// to ignore larger increases in the RTT.
  ///
  /// Default if undefined: `2.5`
  rtt_deviation_scale: Number(isBetween(0.0, 9.007199254740991E15))?
}

/// Event batching behavior.
class `Vector::sinks::util::batch::BatchConfig<vector::sinks::splunkHec::common::util::SplunkHecDefaultBatchSettings>` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  ///
  /// Default if undefined: `1000000`
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `1.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Event batching behavior.
class `Vector::sinks::util::batch::BatchConfig<vector::sinks::util::batch::BulkSizeBasedDefaultBatchSettings>` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  ///
  /// Default if undefined: `10000000`
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `300.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Event batching behavior.
class `Vector::sinks::util::batch::BatchConfig<vector::sinks::util::batch::RealtimeSizeBasedDefaultBatchSettings>` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  ///
  /// Default if undefined: `10000000`
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `1.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Middleware settings for outbound requests.
///
/// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
/// etc.
///
/// Note that the retry backoff policy follows the Fibonacci sequence.
class `Vector::sinks::util::service::TowerRequestConfig` {
  /// Configuration of adaptive concurrency parameters.
  ///
  /// These parameters typically do not require changes from the default, and incorrect values can lead
  /// to meta-stable or unstable performance and sink behavior. Proceed with caution.
  ///
  /// Default if undefined: `{ ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4
  /// ["rtt_deviation_scale"] = 2.5 ["max_concurrency_limit"] = 200 }`
  adaptive_concurrency: Any?

  /// Configuration for outbound request concurrency.
  ///
  /// This can be set either to one of the below enum values or to a positive integer, which denotes a
  /// fixed concurrency limit.
  ///
  /// Default if undefined: `"adaptive"`
  concurrency: Any?

  /// The time window used for the `rate_limit_num` option.
  ///
  /// Default if undefined: `1`
  rate_limit_duration_secs: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum number of requests allowed within the `rate_limit_duration_secs` time window.
  ///
  /// Default if undefined: `9223372036854775807`
  rate_limit_num: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum number of retries to make for failed requests.
  ///
  /// Default if undefined: `9223372036854775807`
  retry_attempts: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The amount of time to wait before attempting the first retry for a failed request.
  ///
  /// After the first retry has failed, the fibonacci sequence is used to select future backoffs.
  ///
  /// Default if undefined: `1`
  retry_initial_backoff_secs: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The jitter mode to use for retry backoff behavior.
  ///
  /// Default if undefined: `"Full"`
  retry_jitter_mode: Any?

  /// The maximum amount of time to wait between retries.
  ///
  /// Default if undefined: `30`
  retry_max_duration_secs: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The time a request can take before being aborted.
  ///
  /// Datadog highly recommends that you do not lower this value below the service's internal timeout, as
  /// this could create orphaned requests, pile on retries, and result in duplicate data downstream.
  ///
  /// Default if undefined: `60`
  timeout_secs: Int(isBetween(0.0, 9.007199254740991E15))?
}

/// Configuration for the `vector` sink.
class `Vector::sinks::vector::config::VectorConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The downstream Vector address to which to connect.
  ///
  /// Both IP address and hostname are accepted formats.
  ///
  /// The address _must_ include a port.
  address: Uri

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: `Vector::sinks::vector::config::VectorConfigBatch`?

  /// Whether or not to compress requests.
  ///
  /// If set to `true`, requests are compressed with [`gzip`][gzip_docs].
  ///
  /// [gzip_docs]: https://www.gzip.org/
  ///
  /// Default if undefined: `false`
  compression: Boolean?

  /// Middleware settings for outbound requests.
  ///
  /// Various settings can be configured, such as concurrency and rate limits, timeouts, retry behavior,
  /// etc.
  ///
  /// Note that the retry backoff policy follows the Fibonacci sequence.
  ///
  /// Default if undefined: `{ ["timeout_secs"] = 60 ["rate_limit_duration_secs"] = 1 ["rate_limit_num"]
  /// = 9223372036854775807 ["retry_attempts"] = 9223372036854775807 ["retry_max_duration_secs"] = 30
  /// ["retry_initial_backoff_secs"] = 1 ["retry_jitter_mode"] = "Full" ["adaptive_concurrency"] {
  /// ["initial_concurrency"] = 1 ["decrease_ratio"] = 0.9 ["ewma_alpha"] = 0.4 ["rtt_deviation_scale"] =
  /// 2.5 ["max_concurrency_limit"] = 200 } }`
  request: Any?

  tls: Any?

  /// Version of the configuration.
  version: (Any|"2")?
}

/// Event batching behavior.
///
/// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
class `Vector::sinks::vector::config::VectorConfigBatch` {
  /// The maximum size of a batch that is processed by a sink.
  ///
  /// This is based on the uncompressed size of the batched events, before they are
  /// serialized/compressed.
  max_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum size of a batch before it is flushed.
  ///
  /// Default if undefined: `1000`
  max_events: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The maximum age of a batch before it is flushed.
  ///
  /// Default if undefined: `1.0`
  timeout_secs: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?
}

/// Encoding configuration.
class `Vector::sinks::webhdfs::config::WebHdfsConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Event batching behavior.
  ///
  /// Default if undefined: `{ ["max_bytes"] = null ["max_events"] = null ["timeout_secs"] = null }`
  batch: Any?

  /// Compression configuration.
  ///
  /// All compression algorithms use the default compression level unless otherwise specified.
  ///
  /// Default if undefined: `"gzip"`
  compression: Any?

  /// An HDFS cluster consists of a single NameNode, a master server that manages the file system
  /// namespace and regulates access to files by clients.
  ///
  /// The endpoint is the HDFS's web restful HTTP API endpoint.
  ///
  /// For more information, see the [HDFS Architecture][hdfs_arch] documentation.
  ///
  /// [hdfs_arch]:
  /// https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#NameNode_and_DataNodes
  ///
  /// Default if undefined: `""`
  endpoint: String?

  /// A prefix to apply to all keys.
  ///
  /// Prefixes are useful for partitioning objects, such as by creating a blob key that stores blobs
  /// under a particular directory. If using a prefix for this purpose, it must end in `/` to act as a
  /// directory path. A trailing `/` is **not** automatically added.
  ///
  /// The final file path is in the format of `{root}/{prefix}{suffix}`.
  ///
  /// Default if undefined: `""`
  prefix: String?

  /// The root path for WebHDFS.
  ///
  /// Must be a valid directory.
  ///
  /// The final file path is in the format of `{root}/{prefix}{suffix}`.
  ///
  /// Default if undefined: `""`
  root: String?
}

/// Configuration for the `websocket` sink.
class `Vector::sinks::websocket::config::WebSocketSinkConfig` {
  /// Controls how acknowledgements are handled for this sink.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  auth: Any?

  /// Configures how events are encoded into raw bytes.
  encoding: Any

  /// The interval, in seconds, between sending [Ping][ping]s to the remote peer.
  ///
  /// If this option is not configured, pings are not sent on an interval.
  ///
  /// If the `ping_timeout` is not set, pings are still sent but there is no expectation of pong response
  /// times.
  ///
  /// [ping]: https://www.rfc-editor.org/rfc/rfc6455#section-5.5.2
  ping_interval: Number(isBetween(1.0, 9.007199254740991E15))?

  /// The number of seconds to wait for a [Pong][pong] response from the remote peer.
  ///
  /// If a response is not received within this time, the connection is re-established.
  ///
  /// [pong]: https://www.rfc-editor.org/rfc/rfc6455#section-5.5.3
  ping_timeout: Number(isBetween(1.0, 9.007199254740991E15))?

  tls: Any?

  /// The WebSocket URI to connect to.
  ///
  /// This should include the protocol and host, but can also include the port, path, and any other valid
  /// part of a URI.
  uri: String
}

/// Configuration for the `amqp` source.
///
/// Supports AMQP version 0.9.1
class `Vector::sources::SourcesAlternate0` {
  /// Collect events from AMQP 0.9.1 compatible brokers like RabbitMQ.
  type: "amqp"
}

/// Configuration for the `apache_metrics` source.
class `Vector::sources::SourcesAlternate1` {
  /// Collect metrics from Apache's HTTPD server.
  type: "apache_metrics"
}

/// Configuration for the `aws_ecs_metrics` source.
class `Vector::sources::SourcesAlternate2` {
  /// Collect Docker container stats for tasks running in AWS ECS and AWS Fargate.
  type: "aws_ecs_metrics"
}

/// Configuration for the `aws_kinesis_firehose` source.
class `Vector::sources::SourcesAlternate3` {
  /// Collect logs from AWS Kinesis Firehose.
  type: "aws_kinesis_firehose"
}

/// Configuration for the `aws_s3` source.
class `Vector::sources::SourcesAlternate4` {
  /// Collect logs from AWS S3.
  type: "aws_s3"
}

/// Configuration for the `aws_sqs` source.
class `Vector::sources::SourcesAlternate5` {
  /// Collect logs from AWS SQS.
  type: "aws_sqs"
}

/// Configuration for the `datadog_agent` source.
class `Vector::sources::SourcesAlternate6` {
  /// Receive logs, metrics, and traces collected by a Datadog Agent.
  type: "datadog_agent"
}

/// Configuration for the `demo_logs` source.
class `Vector::sources::SourcesAlternate7` {
  /// Generate fake log events, which can be useful for testing and demos.
  type: "demo_logs"
}

/// Configuration for the `dnstap` source.
class `Vector::sources::SourcesAlternate8` {
  /// Collect DNS logs from a dnstap-compatible server.
  type: "dnstap"
}

/// Configuration for the `docker_logs` source.
class `Vector::sources::SourcesAlternate9` {
  /// Collect container logs from a Docker Daemon.
  type: "docker_logs"
}

/// Configuration for the `eventstoredb_metrics` source.
class `Vector::sources::SourcesAlternate10` {
  /// Receive metrics from collected by a EventStoreDB.
  type: "eventstoredb_metrics"
}

/// Configuration for the `exec` source.
class `Vector::sources::SourcesAlternate11` {
  /// Collect output from a process running on the host.
  type: "exec"
}

/// Configuration for the `file` source.
class `Vector::sources::SourcesAlternate12` {
  /// Collect logs from files.
  type: "file"
}

/// Configuration for the `file_descriptor` source.
class `Vector::sources::SourcesAlternate13` {
  /// Collect logs from a file descriptor.
  type: "file_descriptor"
}

/// Configuration for the `fluent` source.
class `Vector::sources::SourcesAlternate14` {
  /// Collect logs from a Fluentd or Fluent Bit agent.
  type: "fluent"
}

/// Configuration for the `gcp_pubsub` source.
class `Vector::sources::SourcesAlternate15` {
  /// Fetch observability events from GCP's Pub/Sub messaging system.
  type: "gcp_pubsub"
}

/// Configuration for `heroku_logs` source.
class `Vector::sources::SourcesAlternate16` {
  /// Collect logs from Heroku's Logplex, the router responsible for receiving logs from your Heroku
  /// apps.
  type: "heroku_logs"
}

/// Configuration for the `host_metrics` source.
class `Vector::sources::SourcesAlternate17` {
  /// Collect metric data from the local system.
  type: "host_metrics"
}

/// Configuration for the `http` source.
class `Vector::sources::SourcesAlternate18` {
  /// Host an HTTP endpoint to receive logs.
  type: "http"
}

/// Configuration for the `http_client` source.
class `Vector::sources::SourcesAlternate19` {
  /// Pull observability data from an HTTP server at a configured interval.
  type: "http_client"
}

/// Configuration for the `http_server` source.
class `Vector::sources::SourcesAlternate20` {
  /// Host an HTTP endpoint to receive logs.
  type: "http_server"
}

/// Configuration for the `internal_logs` source.
class `Vector::sources::SourcesAlternate21` {
  /// Expose internal log messages emitted by the running Vector instance.
  type: "internal_logs"
}

/// Configuration for the `internal_metrics` source.
class `Vector::sources::SourcesAlternate22` {
  /// Expose internal metrics emitted by the running Vector instance.
  type: "internal_metrics"
}

/// Configuration for the `journald` source.
class `Vector::sources::SourcesAlternate23` {
  /// Collect logs from JournalD.
  type: "journald"
}

/// Configuration for the `kafka` source.
class `Vector::sources::SourcesAlternate24` {
  /// Collect logs from Apache Kafka.
  type: "kafka"
}

/// Configuration for the `kubernetes_logs` source.
class `Vector::sources::SourcesAlternate25` {
  /// Collect Pod logs from Kubernetes Nodes.
  type: "kubernetes_logs"
}

/// Configuration for the `logstash` source.
class `Vector::sources::SourcesAlternate26` {
  /// Collect logs from a Logstash agent.
  type: "logstash"
}

/// Configuration for the `mongodb_metrics` source.
class `Vector::sources::SourcesAlternate27` {
  /// Collect metrics from the MongoDB database.
  type: "mongodb_metrics"
}

/// Configuration for the `nats` source.
class `Vector::sources::SourcesAlternate28` {
  /// Read observability data from subjects on the NATS messaging system.
  type: "nats"
}

/// Configuration for the `nginx_metrics` source.
class `Vector::sources::SourcesAlternate29` {
  /// Collect metrics from NGINX.
  type: "nginx_metrics"
}

/// Configuration for the `opentelemetry` source.
class `Vector::sources::SourcesAlternate30` {
  /// Receive OTLP data through gRPC or HTTP.
  type: "opentelemetry"
}

/// Configuration for the `postgresql_metrics` source.
class `Vector::sources::SourcesAlternate31` {
  /// Collect metrics from the PostgreSQL database.
  type: "postgresql_metrics"
}

/// Configuration for the `prometheus_pushgateway` source.
class `Vector::sources::SourcesAlternate32` {
  /// Receive metrics via the Prometheus Pushgateway protocol.
  type: "prometheus_pushgateway"
}

/// Configuration for the `prometheus_remote_write` source.
class `Vector::sources::SourcesAlternate33` {
  /// Receive metric via the Prometheus Remote Write protocol.
  type: "prometheus_remote_write"
}

/// Configuration for the `prometheus_scrape` source.
class `Vector::sources::SourcesAlternate34` {
  /// Collect metrics from Prometheus exporters.
  type: "prometheus_scrape"
}

/// Configuration for the `pulsar` source.
class `Vector::sources::SourcesAlternate35` {
  /// Collect logs from Apache Pulsar.
  type: "pulsar"
}

/// Configuration for the `redis` source.
class `Vector::sources::SourcesAlternate36` {
  /// Collect observability data from Redis.
  type: "redis"
}

/// Configuration for the `socket` source.
class `Vector::sources::SourcesAlternate37` {
  /// Collect logs over a socket.
  type: "socket"
}

/// Configuration for the `splunk_hec` source.
class `Vector::sources::SourcesAlternate38` {
  /// Receive logs from Splunk.
  type: "splunk_hec"
}

/// Configuration for the `statsd` source.
class `Vector::sources::SourcesAlternate39` {
  /// Collect metrics emitted by the StatsD aggregator.
  type: "statsd"
}

/// Configuration for the `stdin` source.
class `Vector::sources::SourcesAlternate40` {
  /// Collect logs sent via stdin.
  type: "stdin"
}

/// Configuration for the `syslog` source.
class `Vector::sources::SourcesAlternate41` {
  /// Collect logs sent via Syslog.
  type: "syslog"
}

/// Configuration for the `unit_test` source.
class `Vector::sources::SourcesAlternate42` {
  /// Unit test.
  type: "unit_test"
}

/// Configuration for the `unit_test_stream` source.
class `Vector::sources::SourcesAlternate43` {
  /// Unit test stream.
  type: "unit_test_stream"
}

/// Configuration for the `vector` source.
class `Vector::sources::SourcesAlternate44` {
  /// Collect observability data from a Vector instance.
  type: "vector"
}

/// Configuration for the `amqp` source.
///
/// AMQP connection options.
class `Vector::sources::amqp::AmqpSourceConfig` {
  /// Controls how acknowledgements are handled by this source.
  ///
  /// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
  /// or sink level.
  ///
  /// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
  /// behavior.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The identifier for the consumer.
  ///
  /// Default if undefined: `"vector"`
  consumer: String?

  /// Configures how events are decoded from raw bytes.
  ///
  /// Default if undefined: `{ ["codec"] = "bytes" }`
  decoding: Any?

  /// The `AMQP` exchange key.
  ///
  /// Default if undefined: `"exchange"`
  exchange_key: Any?

  /// Framing configuration.
  ///
  /// Framing handles how events are separated when encoded in a raw byte form, where each event is a
  /// frame that must be prefixed, or delimited, in a way that marks where an event begins and ends
  /// within the byte stream.
  ///
  /// Default if undefined: `{ ["method"] = "bytes" }`
  framing: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// The `AMQP` offset key.
  ///
  /// Default if undefined: `"offset"`
  offset_key: Any?

  /// The name of the queue to consume.
  ///
  /// Default if undefined: `"vector"`
  queue: String?

  /// The `AMQP` routing key.
  ///
  /// Default if undefined: `"routing"`
  routing_key_field: Any?
}

/// Configuration for the `apache_metrics` source.
class `Vector::sources::apacheMetrics::ApacheMetricsConfig` {
  /// The list of `mod_status` endpoints to scrape metrics from.
  endpoints: Listing<String>

  /// The namespace of the metric.
  ///
  /// Disabled if empty.
  ///
  /// Default if undefined: `"apache"`
  namespace: String?

  /// The interval between scrapes.
  ///
  /// Default if undefined: `15`
  scrape_interval_secs: Any?
}

/// Configuration for the `aws_ecs_metrics` source.
class `Vector::sources::awsEcsMetrics::AwsEcsMetricsSourceConfig` {
  /// Base URI of the task metadata endpoint.
  ///
  /// If empty, the URI is automatically discovered based on the latest version detected.
  ///
  /// By default: - The version 4 endpoint base URI is stored in the environment variable
  /// `ECS_CONTAINER_METADATA_URI_V4`. - The version 3 endpoint base URI is stored in the environment
  /// variable `ECS_CONTAINER_METADATA_URI`. - The version 2 endpoint base URI is `169.254.170.2/v2/`.
  ///
  /// Default if undefined: `"http://169.254.170.2/v2"`
  endpoint: String?

  /// The namespace of the metric.
  ///
  /// Disabled if empty.
  ///
  /// Default if undefined: `"awsecs"`
  namespace: String?

  /// The interval between scrapes, in seconds.
  ///
  /// Default if undefined: `15`
  scrape_interval_secs: Any?

  /// The version of the task metadata endpoint to use.
  ///
  /// If empty, the version is automatically discovered based on environment variables.
  ///
  /// By default: - Version 4 is used if the environment variable `ECS_CONTAINER_METADATA_URI_V4` is
  /// defined. - Version 3 is used if the environment variable `ECS_CONTAINER_METADATA_URI_V4` is not
  /// defined, but the environment variable `ECS_CONTAINER_METADATA_URI` _is_ defined. - Version 2 is
  /// used if neither of the environment variables `ECS_CONTAINER_METADATA_URI_V4` or
  /// `ECS_CONTAINER_METADATA_URI` are defined.
  ///
  /// Default if undefined: `"v2"`
  version: ("v2"|"v3"|"v4")?
}

/// Configuration for the `aws_kinesis_firehose` source.
class `Vector::sources::awsKinesisFirehose::AwsKinesisFirehoseConfig` {
  /// An access key to authenticate requests against.
  ///
  /// AWS Kinesis Firehose can be configured to pass along a user-configurable access key with each
  /// request. If configured, `access_key` should be set to the same value. Otherwise, all requests are
  /// allowed.
  @Deprecated
  access_key: Any?

  /// A list of access keys to authenticate requests against.
  ///
  /// AWS Kinesis Firehose can be configured to pass along a user-configurable access key with each
  /// request. If configured, `access_keys` should be set to the same value. Otherwise, all requests are
  /// allowed.
  access_keys: Listing<Any>?

  /// Controls how acknowledgements are handled by this source.
  ///
  /// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
  /// or sink level.
  ///
  /// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
  /// behavior.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The socket address to listen for connections on.
  address: Any

  /// Configures how events are decoded from raw bytes.
  ///
  /// Default if undefined: `{ ["codec"] = "bytes" }`
  decoding: Any?

  /// Framing configuration.
  ///
  /// Framing handles how events are separated when encoded in a raw byte form, where each event is a
  /// frame that must be prefixed, or delimited, in a way that marks where an event begins and ends
  /// within the byte stream.
  ///
  /// Default if undefined: `{ ["method"] = "bytes" }`
  framing: Any?

  /// Configuration of HTTP server keepalive parameters.
  ///
  /// Default if undefined: `{ ["max_connection_age_secs"] = 300 ["max_connection_age_jitter_factor"] =
  /// 0.1 }`
  keepalive: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// The compression scheme to use for decompressing records within the Firehose message.
  ///
  /// Some services, like AWS CloudWatch Logs, [compresses the events with gzip][events_with_gzip],
  /// before sending them AWS Kinesis Firehose. This option can be used to automatically decompress them
  /// before forwarding them to the next component.
  ///
  /// Note that this is different from [Content encoding option][encoding_option] of the Firehose HTTP
  /// endpoint destination. That option controls the content encoding of the entire HTTP request.
  ///
  /// [events_with_gzip]:
  /// https://docs.aws.amazon.com/firehose/latest/dev/writing-with-cloudwatch-logs.html
  /// [encoding_option]:
  /// https://docs.aws.amazon.com/firehose/latest/dev/create-destination.html#create-destination-http
  ///
  /// Default if undefined: `"auto"`
  record_compression: ("auto"|"none"|"gzip")?

  /// Whether or not to store the AWS Firehose Access Key in event secrets.
  ///
  /// If set to `true`, when incoming requests contains an access key sent by AWS Firehose, it is kept in
  /// the event secrets as "aws_kinesis_firehose_access_key".
  store_access_key: Boolean

  tls: Any?
}

/// Configuration of the region/endpoint to use when interacting with an AWS service.
///
/// Default if undefined: `{ ["region"] = null ["endpoint"] = null ["compression"] = "auto" ["strategy"]
/// = "sqs" ["sqs"] = null ["assume_role"] = null ["auth"] { ["load_timeout_secs"] = null ["imds"] {
/// ["max_attempts"] = 4 ["connect_timeout_seconds"] = 1 ["read_timeout_seconds"] = 1 } ["region"] = null
/// } ["multiline"] = null ["acknowledgements"] { ["enabled"] = null } ["tls_options"] = null
/// ["log_namespace"] = null ["framing"] { ["method"] = "newline_delimited" } ["decoding"] { ["codec"] =
/// "bytes" } }`
class `Vector::sources::awsS3::AwsS3Config` {
  /// Controls how acknowledgements are handled by this source.
  ///
  /// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
  /// or sink level.
  ///
  /// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
  /// behavior.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The ARN of an [IAM role][iam_role] to assume at startup.
  ///
  /// [iam_role]: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html
  @Deprecated
  assume_role: String?

  /// Configuration of the authentication strategy for interacting with AWS services.
  ///
  /// Default if undefined: `{ ["load_timeout_secs"] = null ["imds"] { ["max_attempts"] = 4
  /// ["connect_timeout_seconds"] = 1 ["read_timeout_seconds"] = 1 } ["region"] = null }`
  auth: Any?

  /// The compression scheme used for decompressing objects retrieved from S3.
  compression: ("auto"|"none"|"gzip"|"zstd")?

  /// Configures how events are decoded from raw bytes.
  ///
  /// Default if undefined: `{ ["codec"] = "bytes" }`
  decoding: Any?

  /// Framing configuration.
  ///
  /// Framing handles how events are separated when encoded in a raw byte form, where each event is a
  /// frame that must be prefixed, or delimited, in a way that marks where an event begins and ends
  /// within the byte stream.
  ///
  /// Default if undefined: `{ ["method"] = "newline_delimited" }`
  framing: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// Multiline aggregation configuration.
  ///
  /// If not specified, multiline aggregation is disabled.
  multiline: Any?

  /// Configuration options for SQS.
  sqs: (Any|SqsAlternate1)?

  /// The strategy to use to consume objects from S3.
  strategy: ("sqs")?

  tls_options: Any?
}

/// SQS configuration options.
class SqsAlternate1 {
  /// Number of concurrent tasks to create for polling the queue for messages.
  ///
  /// Defaults to the number of available CPUs on the system.
  ///
  /// Should not typically need to be changed, but it can sometimes be beneficial to raise this value
  /// when there is a high rate of messages being pushed into the queue and the objects being fetched are
  /// small. In these cases, system resources may not be fully utilized without fetching more messages
  /// per second, as the SQS message consumption rate affects the S3 object retrieval rate.
  client_concurrency: Number(isBetween(1.0, 9.007199254740991E15))?

  /// Whether to delete non-retryable messages.
  ///
  /// If a message is rejected by the sink and not retryable, it is deleted from the queue.
  ///
  /// Default if undefined: `true`
  delete_failed_message: Boolean?

  /// Whether to delete the message once it is processed.
  ///
  /// It can be useful to set this to `false` for debugging or during the initial setup.
  ///
  /// Default if undefined: `true`
  delete_message: Boolean?

  /// Maximum number of messages to poll from SQS in a batch
  ///
  /// Defaults to 10
  ///
  /// Should be set to a smaller value when the files are large to help prevent the ingestion of one file
  /// from causing the other files to exceed the visibility_timeout. Valid values are 1 - 10
  ///
  /// Default if undefined: `10`
  max_number_of_messages: UInt32?

  /// How long to wait while polling the queue for new messages, in seconds.
  ///
  /// Generally, this should not be changed unless instructed to do so, as if messages are available,
  /// they are always consumed, regardless of the value of `poll_secs`.
  ///
  /// Default if undefined: `15`
  poll_secs: UInt32?

  /// The URL of the SQS queue to poll for bucket notifications.
  queue_url: Uri

  tls_options: Any?

  /// The visibility timeout to use for messages, in seconds.
  ///
  /// This controls how long a message is left unavailable after it is received. If a message is
  /// received, and takes longer than `visibility_timeout_secs` to process and delete the message from
  /// the queue, it is made available again for another consumer.
  ///
  /// This can happen if there is an issue between consuming a message and deleting it.
  ///
  /// Default if undefined: `300`
  visibility_timeout_secs: UInt32?
}

/// Configuration of the region/endpoint to use when interacting with an AWS service.
class `Vector::sources::awsSqs::config::AwsSqsConfig` {
  /// Controls how acknowledgements are handled by this source.
  ///
  /// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
  /// or sink level.
  ///
  /// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
  /// behavior.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Configuration of the authentication strategy for interacting with AWS services.
  ///
  /// Default if undefined: `{ ["load_timeout_secs"] = null ["imds"] { ["max_attempts"] = 4
  /// ["connect_timeout_seconds"] = 1 ["read_timeout_seconds"] = 1 } ["region"] = null }`
  auth: Any?

  /// Number of concurrent tasks to create for polling the queue for messages.
  ///
  /// Defaults to the number of available CPUs on the system.
  ///
  /// Should not typically need to be changed, but it can sometimes be beneficial to raise this value
  /// when there is a high rate of messages being pushed into the queue and the messages being fetched
  /// are small. In these cases, system resources may not be fully utilized without fetching more
  /// messages per second, as it spends more time fetching the messages than processing them.
  client_concurrency: Number(isBetween(1.0, 9.007199254740991E15))?

  /// Configures how events are decoded from raw bytes.
  ///
  /// Default if undefined: `{ ["codec"] = "bytes" }`
  decoding: Any?

  /// Whether to delete the message once it is processed.
  ///
  /// It can be useful to set this to `false` for debugging or during the initial setup.
  ///
  /// Default if undefined: `true`
  delete_message: Boolean?

  /// Framing configuration.
  ///
  /// Framing handles how events are separated when encoded in a raw byte form, where each event is a
  /// frame that must be prefixed, or delimited, in a way that marks where an event begins and ends
  /// within the byte stream.
  ///
  /// Default if undefined: `{ ["method"] = "bytes" }`
  framing: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// How long to wait while polling the queue for new messages, in seconds.
  ///
  /// Generally, this should not be changed unless instructed to do so, as if messages are available,
  /// they are always consumed, regardless of the value of `poll_secs`.
  ///
  /// Default if undefined: `15`
  poll_secs: UInt32?

  /// The URL of the SQS queue to poll for messages.
  queue_url: String

  tls: Any?

  /// The visibility timeout to use for messages, in seconds.
  ///
  /// This controls how long a message is left unavailable after it is received. If a message is
  /// received, and takes longer than `visibility_timeout_secs` to process and delete the message from
  /// the queue, it is made available again for another consumer.
  ///
  /// This can happen if there is an issue between consuming a message and deleting it.
  ///
  /// Default if undefined: `300`
  visibility_timeout_secs: UInt32?
}

/// Configuration for the `datadog_agent` source.
class `Vector::sources::datadogAgent::DatadogAgentConfig` {
  /// Controls how acknowledgements are handled by this source.
  ///
  /// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
  /// or sink level.
  ///
  /// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
  /// behavior.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The socket address to accept connections on.
  ///
  /// It _must_ include a port.
  address: Any

  /// Configures how events are decoded from raw bytes.
  ///
  /// Default if undefined: `{ ["codec"] = "bytes" }`
  decoding: Any?

  /// If this is set to `true`, logs are not accepted by the component.
  ///
  /// Default if undefined: `false`
  disable_logs: Boolean?

  /// If this is set to `true`, metrics (beta) are not accepted by the component.
  ///
  /// Default if undefined: `false`
  disable_metrics: Boolean?

  /// If this is set to `true`, traces (alpha) are not accepted by the component.
  ///
  /// Default if undefined: `false`
  disable_traces: Boolean?

  /// Framing configuration.
  ///
  /// Framing handles how events are separated when encoded in a raw byte form, where each event is a
  /// frame that must be prefixed, or delimited, in a way that marks where an event begins and ends
  /// within the byte stream.
  ///
  /// Default if undefined: `{ ["method"] = "bytes" }`
  framing: Any?

  /// Configuration of HTTP server keepalive parameters.
  ///
  /// Default if undefined: `{ ["max_connection_age_secs"] = 300 ["max_connection_age_jitter_factor"] =
  /// 0.1 }`
  keepalive: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// If this is set to `true`, logs, metrics (beta), and traces (alpha) are sent to different outputs.
  ///
  /// For a source component named `agent`, the received logs, metrics (beta), and traces (alpha) can
  /// then be configured as input to other components by specifying `agent.logs`, `agent.metrics`, and
  /// `agent.traces`, respectively.
  ///
  /// Default if undefined: `false`
  multiple_outputs: Boolean?

  /// If this is set to `true`, when log events contain the field `ddtags`, the string value that
  /// contains a list of key:value pairs set by the Agent is parsed and expanded into an array.
  ///
  /// Default if undefined: `false`
  parse_ddtags: Boolean?

  /// If this is set to `true`, when incoming events contain a Datadog API key, it is stored in the event
  /// metadata and used if the event is sent to a Datadog sink.
  ///
  /// Default if undefined: `true`
  store_api_key: Boolean?

  tls: Any?
}

/// Output format configuration.
class `Vector::sources::demoLogs::DemoLogsConfig` {
  /// The total number of lines to output.
  ///
  /// By default, the source continuously prints logs (infinitely).
  ///
  /// Default if undefined: `9223372036854775807`
  count: Int(isBetween(0.0, 9.007199254740991E15))?

  /// Configures how events are decoded from raw bytes.
  ///
  /// Default if undefined: `{ ["codec"] = "bytes" }`
  decoding: Any?

  /// Framing configuration.
  ///
  /// Framing handles how events are separated when encoded in a raw byte form, where each event is a
  /// frame that must be prefixed, or delimited, in a way that marks where an event begins and ends
  /// within the byte stream.
  ///
  /// Default if undefined: `{ ["method"] = "bytes" }`
  framing: Any?

  /// The amount of time, in seconds, to pause between each batch of output lines.
  ///
  /// The default is one batch per second. To remove the delay and output batches as quickly as possible,
  /// set `interval` to `0.0`.
  ///
  /// Default if undefined: `1.0`
  interval: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?
}

/// Lines are chosen at random from the list specified using `lines`.
class `Vector::sources::demoLogs::DemoLogsConfigAlternate0` {
  /// Lines are chosen at random from the list specified using `lines`.
  format: "shuffle"

  /// The list of lines to output.
  lines: Listing<String>

  /// If `true`, each output line starts with an increasing sequence number, beginning with 0.
  ///
  /// Default if undefined: `false`
  sequence: Boolean?
}

/// Randomly generated logs in [Apache common][apache_common] format.
///
/// [apache_common]: https://httpd.apache.org/docs/current/logs.html#common
class `Vector::sources::demoLogs::DemoLogsConfigAlternate1` {
  /// Randomly generated logs in [Apache common][apache_common] format.
  ///
  /// [apache_common]: https://httpd.apache.org/docs/current/logs.html#common
  format: "apache_common"
}

/// Randomly generated logs in [Apache error][apache_error] format.
///
/// [apache_error]: https://httpd.apache.org/docs/current/logs.html#errorlog
class `Vector::sources::demoLogs::DemoLogsConfigAlternate2` {
  /// Randomly generated logs in [Apache error][apache_error] format.
  ///
  /// [apache_error]: https://httpd.apache.org/docs/current/logs.html#errorlog
  format: "apache_error"
}

/// Randomly generated logs in Syslog format ([RFC 5424][syslog_5424]).
///
/// [syslog_5424]: https://tools.ietf.org/html/rfc5424
class `Vector::sources::demoLogs::DemoLogsConfigAlternate3` {
  /// Randomly generated logs in Syslog format ([RFC 5424][syslog_5424]).
  ///
  /// [syslog_5424]: https://tools.ietf.org/html/rfc5424
  format: "syslog"
}

/// Randomly generated logs in Syslog format ([RFC 3164][syslog_3164]).
///
/// [syslog_3164]: https://tools.ietf.org/html/rfc3164
class `Vector::sources::demoLogs::DemoLogsConfigAlternate4` {
  /// Randomly generated logs in Syslog format ([RFC 3164][syslog_3164]).
  ///
  /// [syslog_3164]: https://tools.ietf.org/html/rfc3164
  format: "bsd_syslog"
}

/// Randomly generated HTTP server logs in [JSON][json] format.
///
/// [json]: https://en.wikipedia.org/wiki/JSON
class `Vector::sources::demoLogs::DemoLogsConfigAlternate5` {
  /// Randomly generated HTTP server logs in [JSON][json] format.
  ///
  /// [json]: https://en.wikipedia.org/wiki/JSON
  format: "json"
}

/// Listening mode for the `dnstap` source.
class `Vector::sources::dnstap::DnstapConfig` {
  /// Overrides the name of the log field used to add the source path to each event.
  ///
  /// The value is the socket path itself.
  ///
  /// By default, the [global `log_schema.host_key` option][global_host_key] is used.
  ///
  /// [global_host_key]:
  /// https://vector.dev/docs/reference/configuration/global-options/#log_schema.host_key
  host_key: Any?

  /// The namespace to use for logs. This overrides the global settings.
  log_namespace: Boolean?

  /// Whether to downcase all DNSTAP hostnames received for consistency
  ///
  /// Default if undefined: `false`
  lowercase_hostnames: Boolean?

  /// Maximum number of frames that can be processed concurrently.
  max_frame_handling_tasks: Number(isBetween(0.0, 4.294967295E9))?

  /// Maximum DNSTAP frame length that the source accepts.
  ///
  /// If any frame is longer than this, it is discarded.
  ///
  /// Default if undefined: `102400`
  max_frame_length: Int(isBetween(0.0, 9.007199254740991E15))?

  /// Whether or not to concurrently process DNSTAP frames.
  multithreaded: Boolean?

  /// Whether or not to skip parsing or decoding of DNSTAP frames.
  ///
  /// If set to `true`, frames are not parsed or decoded. The raw frame data is set as a field on the
  /// event (called `rawData`) and encoded as a base64 string.
  raw_data_only: Boolean?
}

/// TCP configuration for the `dnstap` source.
class `Vector::sources::dnstap::DnstapConfigAlternate0` {
  /// Listen on TCP.
  mode: "tcp"
}

/// Unix domain socket configuration for the `dnstap` source.
class `Vector::sources::dnstap::DnstapConfigAlternate1` {
  /// Listen on a Unix domain socket
  mode: "unix"
}

/// Configuration for the `docker_logs` source.
///
/// Default if undefined: `{ ["host_key"] = "host" ["docker_host"] = null ["exclude_containers"] = null
/// ["include_containers"] = null ["include_labels"] = null ["include_images"] = null
/// ["partial_event_marker_field"] = "_partial" ["auto_partial_merge"] = true ["retry_backoff_secs"] = 2
/// ["multiline"] = null ["tls"] = null ["log_namespace"] = null }`
class `Vector::sources::dockerLogs::DockerLogsConfig` {
  /// Enables automatic merging of partial events.
  auto_partial_merge: Boolean?

  /// Docker host to connect to.
  ///
  /// Use an HTTPS URL to enable TLS encryption.
  ///
  /// If absent, the `DOCKER_HOST` environment variable is used. If `DOCKER_HOST` is also absent, the
  /// default Docker local socket (`/var/run/docker.sock` on Unix platforms, `//./pipe/docker_engine` on
  /// Windows) is used.
  docker_host: String?

  /// A list of container IDs or names of containers to exclude from log collection.
  ///
  /// Matching is prefix first, so specifying a value of `foo` would match any container named `foo` as
  /// well as any container whose name started with `foo`. This applies equally whether matching
  /// container IDs or names.
  ///
  /// By default, the source collects logs for all containers. If `exclude_containers` is configured, any
  /// container that matches a configured exclusion is excluded even if it is also included with
  /// `include_containers`, so care should be taken when using prefix matches as they cannot be
  /// overridden by a corresponding entry in `include_containers`, for example, excluding `foo` by
  /// attempting to include `foo-specific-id`.
  ///
  /// This can be used in conjunction with `include_containers`.
  exclude_containers: Listing<String>?

  /// Overrides the name of the log field used to add the current hostname to each event.
  ///
  /// By default, the [global `log_schema.host_key` option][global_host_key] is used.
  ///
  /// [global_host_key]:
  /// https://vector.dev/docs/reference/configuration/global-options/#log_schema.host_key
  ///
  /// Default if undefined: `"host"`
  host_key: Any?

  /// A list of container IDs or names of containers to include in log collection.
  ///
  /// Matching is prefix first, so specifying a value of `foo` would match any container named `foo` as
  /// well as any container whose name started with `foo`. This applies equally whether matching
  /// container IDs or names.
  ///
  /// By default, the source collects logs for all containers. If `include_containers` is configured,
  /// only containers that match a configured inclusion and are also not excluded get matched.
  ///
  /// This can be used in conjunction with `exclude_containers`.
  include_containers: Listing<String>?

  /// A list of image names to match against.
  ///
  /// If not provided, all images are included.
  include_images: Listing<String>?

  /// A list of container object labels to match against when filtering running containers.
  ///
  /// Labels should follow the syntax described in the [Docker object
  /// labels](https://docs.docker.com/config/labels-custom-metadata/) documentation.
  include_labels: Listing<String>?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// Multiline aggregation configuration.
  ///
  /// If not specified, multiline aggregation is disabled.
  multiline: Any?

  /// Overrides the name of the log field used to mark an event as partial.
  ///
  /// If `auto_partial_merge` is disabled, partial events are emitted with a log field, set by this
  /// configuration value, indicating that the event is not complete.
  ///
  /// Default if undefined: `"_partial"`
  partial_event_marker_field: String?

  /// The amount of time to wait before retrying after an error.
  ///
  /// Default if undefined: `2`
  retry_backoff_secs: Any?

  /// Configuration of TLS when connecting to the Docker daemon.
  ///
  /// Only relevant when connecting to Docker with an HTTPS URL.
  ///
  /// If not configured, the environment variable `DOCKER_CERT_PATH` is used. If `DOCKER_CERT_PATH` is
  /// absent, then` DOCKER_CONFIG` is used. If both environment variables are absent, the certificates in
  /// `~/.docker/` are read.
  tls: (Any|TlsAlternate1)?
}

/// Configuration of TLS when connecting to the Docker daemon.
///
/// Only relevant when connecting to Docker with an HTTPS URL.
///
/// If not configured, the environment variable `DOCKER_CERT_PATH` is used. If `DOCKER_CERT_PATH` is
/// absent, then` DOCKER_CONFIG` is used. If both environment variables are absent, the certificates in
/// `~/.docker/` are read.
class TlsAlternate1 {
  /// Path to the CA certificate file.
  ca_file: Any

  /// Path to the TLS certificate file.
  crt_file: Any

  /// Path to the TLS key file.
  key_file: Any
}

/// Configuration for the `eventstoredb_metrics` source.
class `Vector::sources::eventstoredbMetrics::EventStoreDbConfig` {
  /// Overrides the default namespace for the metrics emitted by the source.
  ///
  /// By default, `eventstoredb` is used.
  default_namespace: String?

  /// Endpoint to scrape stats from.
  ///
  /// Default if undefined: `"https://localhost:2113/stats"`
  endpoint: String?

  /// The interval between scrapes, in seconds.
  ///
  /// Default if undefined: `15`
  scrape_interval_secs: Any?
}

/// Configuration for the `exec` source.
class `Vector::sources::exec::ExecConfig` {
  /// Whether or not to clear the environment before setting custom environment variables.
  ///
  /// Default if undefined: `false`
  clear_environment: Boolean?

  /// The command to run, plus any arguments required.
  command: Listing<String>

  /// Configures how events are decoded from raw bytes.
  ///
  /// Default if undefined: `{ ["codec"] = "bytes" }`
  decoding: Any?

  /// Custom environment variables to set or update when running the command. If a variable name already
  /// exists in the environment, its value is replaced.
  environment: Mapping<String, String>?

  framing: Any?

  /// Whether or not the output from stderr should be included when generating events.
  ///
  /// Default if undefined: `true`
  include_stderr: Boolean?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// The maximum buffer size allowed before a log event is generated.
  ///
  /// Default if undefined: `1000000`
  maximum_buffer_size_bytes: Int(isBetween(0.0, 9.007199254740991E15))?

  /// Mode of operation for running the command.
  mode: "scheduled"|"streaming"

  /// Configuration options for scheduled commands.
  scheduled: (Any|ScheduledAlternate1)?

  /// Configuration options for streaming commands.
  streaming: (Any|StreamingAlternate1)?

  /// The directory in which to run the command.
  working_directory: Any?
}

/// Configuration options for scheduled commands.
class ScheduledAlternate1 {
  /// The interval, in seconds, between scheduled command runs.
  ///
  /// If the command takes longer than `exec_interval_secs` to run, it is killed.
  ///
  /// Default if undefined: `60`
  exec_interval_secs: Int(isBetween(0.0, 9.007199254740991E15))?
}

/// Configuration options for streaming commands.
class StreamingAlternate1 {
  /// The amount of time, in seconds, before rerunning a streaming command that exited.
  ///
  /// Default if undefined: `5`
  respawn_interval_secs: Int(isBetween(0.0, 9.007199254740991E15))?

  /// Whether or not the command should be rerun if the command exits.
  ///
  /// Default if undefined: `true`
  respawn_on_exit: Boolean?
}

/// Configuration for the `file` source.
class `Vector::sources::file::FileConfig` {
  /// Controls how acknowledgements are handled by this source.
  ///
  /// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
  /// or sink level.
  ///
  /// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
  /// behavior.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The directory used to persist file checkpoint positions.
  ///
  /// By default, the [global `data_dir` option][global_data_dir] is used. Make sure the running user has
  /// write permissions to this directory.
  ///
  /// If this directory is specified, then Vector will attempt to create it.
  ///
  /// [global_data_dir]: https://vector.dev/docs/reference/configuration/global-options/#data_dir
  data_dir: Any?

  /// Character set encoding.
  encoding: (Any|EncodingEncodingAlternate1)?

  /// Array of file patterns to exclude.
  /// [Globbing](https://vector.dev/docs/reference/configuration/sources/file/#globbing) is supported.
  ///
  /// Takes precedence over the `include` option. Note: The `exclude` patterns are applied _after_ the
  /// attempt to glob everything in `include`. This means that all files are first matched by `include`
  /// and then filtered by the `exclude` patterns. This can be impactful if `include` contains
  /// directories with contents that are not accessible.
  ///
  /// Default if undefined: `{}`
  exclude: Listing<Any>?

  /// Overrides the name of the log field used to add the file path to each event.
  ///
  /// The value is the full path to the file where the event was read message.
  ///
  /// Set to `""` to suppress this key.
  ///
  /// Default if undefined: `"file"`
  file_key: Any?

  /// Configuration for how files should be identified.
  ///
  /// This is important for `checkpointing` when file rotation is used.
  ///
  /// Default if undefined: `{ ["strategy"] = "checksum" ["bytes"] = null ["ignored_header_bytes"] = 0
  /// ["lines"] = 1 }`
  fingerprint: (FingerprintAlternate0|FingerprintAlternate1)?

  /// The delay between file discovery calls.
  ///
  /// This controls the interval at which files are searched. A higher value results in greater chances
  /// of some short-lived files being missed between searches, but a lower value increases the
  /// performance impact of file discovery.
  ///
  /// Default if undefined: `1000`
  glob_minimum_cooldown_ms: Any?

  /// Overrides the name of the log field used to add the current hostname to each event.
  ///
  /// By default, the [global `log_schema.host_key` option][global_host_key] is used.
  ///
  /// Set to `""` to suppress this key.
  ///
  /// [global_host_key]:
  /// https://vector.dev/docs/reference/configuration/global-options/#log_schema.host_key
  ///
  /// Default if undefined: `"host"`
  host_key: Any?

  /// Whether or not to ignore existing checkpoints when determining where to start reading a file.
  ///
  /// Checkpoints are still written normally.
  ignore_checkpoints: Boolean?

  /// Ignore missing files when fingerprinting.
  ///
  /// This may be useful when used with source directories containing dangling symlinks.
  ///
  /// Default if undefined: `false`
  ignore_not_found: Boolean?

  /// Ignore files with a data modification date older than the specified number of seconds.
  ignore_older_secs: Number(isBetween(0.0, 9.007199254740991E15))?

  /// Array of file patterns to include.
  /// [Globbing](https://vector.dev/docs/reference/configuration/sources/file/#globbing) is supported.
  include: Listing<Any>

  /// Configuration of internal metrics for file-based components.
  ///
  /// Default if undefined: `{ ["include_file_tag"] = false }`
  internal_metrics: Any?

  /// String sequence used to separate one file line from another.
  ///
  /// Default if undefined: `""" """`
  line_delimiter: String?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// The maximum size of a line before it is discarded.
  ///
  /// This protects against malformed lines or tailing incorrect files.
  ///
  /// Default if undefined: `102400`
  max_line_bytes: Int(isBetween(0.0, 9.007199254740991E15))?

  /// Max amount of bytes to read from a single file before switching over to the next file. **Note:**
  /// This does not apply when `oldest_first` is `true`.
  ///
  /// This allows distributing the reads more or less evenly across the files.
  ///
  /// Default if undefined: `2048`
  max_read_bytes: Int(isBetween(0.0, 9.007199254740991E15))?

  /// String value used to identify the start of a multi-line message.
  @Deprecated
  message_start_indicator: String?

  /// How long to wait for more data when aggregating a multi-line message, in milliseconds.
  ///
  /// Default if undefined: `1000`
  @Deprecated
  multi_line_timeout: Int(isBetween(0.0, 9.007199254740991E15))?

  /// Multiline aggregation configuration.
  ///
  /// If not specified, multiline aggregation is disabled.
  multiline: Any?

  /// Enables adding the file offset to each event and sets the name of the log field used.
  ///
  /// The value is the byte offset of the start of the line within the file.
  ///
  /// Off by default, the offset is only added to the event if this is set.
  offset_key: Any?

  /// Instead of balancing read capacity fairly across all watched files, prioritize draining the oldest
  /// files before moving on to read data from more recent files.
  ///
  /// Default if undefined: `false`
  oldest_first: Boolean?

  /// File position to use when reading a new file.
  ///
  /// Default if undefined: `"beginning"`
  read_from: Any?

  /// After reaching EOF, the number of seconds to wait before removing the file, unless new data is
  /// written.
  ///
  /// If not specified, files are not removed.
  remove_after_secs: Number(isBetween(0.0, 9.007199254740991E15))?

  /// How long to keep an open handle to a rotated log file. The default value represents "no limit"
  ///
  /// Default if undefined: `9223372036854775807`
  rotate_wait_secs: Any?

  /// Whether or not to start reading from the beginning of a new file.
  @Deprecated
  start_at_beginning: Boolean?
}

/// Character set encoding.
class EncodingEncodingAlternate1 {
  /// Encoding of the source messages.
  ///
  /// Takes one of the encoding [label strings](https://encoding.spec.whatwg.org/#concept-encoding-get)
  /// defined as part of the [Encoding Standard](https://encoding.spec.whatwg.org/).
  ///
  /// When set, the messages are transcoded from the specified encoding to UTF-8, which is the encoding
  /// that is assumed internally for string-like data. Enable this transcoding operation if you need your
  /// data to be in UTF-8 for further processing. At the time of transcoding, any malformed sequences
  /// (that can't be mapped to UTF-8) is replaced with the Unicode [REPLACEMENT
  /// CHARACTER](https://en.wikipedia.org/wiki/Specials_(Unicode_block)#Replacement_character) and
  /// warnings are logged.
  charset: String
}

/// Read lines from the beginning of the file and compute a checksum over them.
class FingerprintAlternate0 {
  /// Maximum number of bytes to use, from the lines that are read, for generating the checksum.
  bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  /// The number of bytes to skip ahead (or ignore) when reading the data used for generating the
  /// checksum.
  ///
  /// This can be helpful if all files share a common header that should be skipped.
  ///
  /// Default if undefined: `0`
  ignored_header_bytes: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The number of lines to read for generating the checksum.
  ///
  /// If your files share a common header that is not always a fixed size,
  ///
  /// If the file has less than this amount of lines, it wonât be read at all.
  ///
  /// Default if undefined: `1`
  lines: Int(isBetween(0.0, 9.007199254740991E15))?

  /// Read lines from the beginning of the file and compute a checksum over them.
  strategy: "checksum"
}

/// Use the [device and inode][inode] as the identifier.
///
/// [inode]: https://en.wikipedia.org/wiki/Inode
class FingerprintAlternate1 {
  /// Use the [device and inode][inode] as the identifier.
  ///
  /// [inode]: https://en.wikipedia.org/wiki/Inode
  strategy: "device_and_inode"
}

/// Configuration for the `file_descriptor` source.
class `Vector::sources::fileDescriptors::fileDescriptor::FileDescriptorSourceConfig` {
  /// Configures how events are decoded from raw bytes.
  ///
  /// Default if undefined: `{ ["codec"] = "bytes" }`
  decoding: Any?

  /// The file descriptor number to read from.
  fd: UInt32

  framing: Any?

  /// Overrides the name of the log field used to add the current hostname to each event.
  ///
  /// By default, the [global `host_key`
  /// option](https://vector.dev/docs/reference/configuration//global-options#log_schema.host_key) is
  /// used.
  host_key: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// The maximum buffer size, in bytes, of incoming messages.
  ///
  /// Messages larger than this are truncated.
  ///
  /// Default if undefined: `102400`
  max_length: Int(isBetween(0.0, 9.007199254740991E15))?
}

/// Configuration for the `stdin` source.
///
/// Default if undefined: `{ ["max_length"] = 102400 ["host_key"] = null ["framing"] = null ["decoding"]
/// { ["codec"] = "bytes" } ["log_namespace"] = null }`
class `Vector::sources::fileDescriptors::stdin::StdinConfig` {
  /// Configures how events are decoded from raw bytes.
  ///
  /// Default if undefined: `{ ["codec"] = "bytes" }`
  decoding: Any?

  framing: Any?

  /// Overrides the name of the log field used to add the current hostname to each event.
  ///
  /// By default, the [global `log_schema.host_key` option][global_host_key] is used.
  ///
  /// [global_host_key]:
  /// https://vector.dev/docs/reference/configuration/global-options/#log_schema.host_key
  host_key: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// The maximum buffer size, in bytes, of incoming messages.
  ///
  /// Messages larger than this are truncated.
  ///
  /// Default if undefined: `102400`
  max_length: Int(isBetween(0.0, 9.007199254740991E15))?
}

/// Configuration for the `fluent` source.
class `Vector::sources::fluent::FluentConfig` {
  /// Controls how acknowledgements are handled by this source.
  ///
  /// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
  /// or sink level.
  ///
  /// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
  /// behavior.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The socket address to listen for connections on, or `systemd{#N}` to use the Nth socket passed by
  /// systemd socket activation.
  ///
  /// If a socket address is used, it _must_ include a port.
  address: Any

  /// The maximum number of TCP connections that are allowed at any given time.
  connection_limit: Number(isBetween(0.0, 4.294967295E9))?

  keepalive: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  permit_origin: Any?

  /// The size of the receive buffer used for each connection.
  ///
  /// This generally should not need to be changed.
  receive_buffer_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  tls: Any?
}

/// Configuration of the authentication strategy for interacting with GCP services.
class `Vector::sources::gcpPubsub::PubsubConfig` {
  /// The acknowledgement deadline, in seconds, to use for this stream.
  ///
  /// Messages that are not acknowledged when this deadline expires may be retransmitted.
  @Deprecated
  ack_deadline_seconds: Number(isBetween(0.0, 65535.0))?

  /// The acknowledgement deadline, in seconds, to use for this stream.
  ///
  /// Messages that are not acknowledged when this deadline expires may be retransmitted.
  ///
  /// Default if undefined: `600`
  ack_deadline_secs: Any?

  /// Controls how acknowledgements are handled by this source.
  ///
  /// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
  /// or sink level.
  ///
  /// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
  /// behavior.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Configures how events are decoded from raw bytes.
  ///
  /// Default if undefined: `{ ["codec"] = "bytes" }`
  decoding: Any?

  /// The endpoint from which to pull data.
  ///
  /// Default if undefined: `"https://pubsub.googleapis.com"`
  endpoint: String?

  /// Framing configuration.
  ///
  /// Framing handles how events are separated when encoded in a raw byte form, where each event is a
  /// frame that must be prefixed, or delimited, in a way that marks where an event begins and ends
  /// within the byte stream.
  ///
  /// Default if undefined: `{ ["method"] = "bytes" }`
  framing: Any?

  /// The number of messages in a response to mark a stream as "busy". This is used to determine if more
  /// streams should be started.
  ///
  /// The GCP Pub/Sub servers send responses with 100 or more messages when the subscription is busy.
  ///
  /// Default if undefined: `100`
  full_response_size: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The amount of time, in seconds, with no received activity before sending a keepalive request. If
  /// this is set larger than `60`, you may see periodic errors sent from the server.
  ///
  /// Default if undefined: `60.0`
  keepalive_secs: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// The maximum number of concurrent stream connections to open at once.
  ///
  /// Default if undefined: `10`
  max_concurrency: Int(isBetween(0.0, 9.007199254740991E15))?

  /// How often to poll the currently active streams to see if they are all busy and so open a new
  /// stream.
  ///
  /// Default if undefined: `2.0`
  poll_time_seconds: Any?

  /// The project name from which to pull logs.
  project: String

  /// The amount of time, in seconds, to wait between retry attempts after an error.
  @Deprecated
  retry_delay_seconds: Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))?

  /// The amount of time, in seconds, to wait between retry attempts after an error.
  ///
  /// Default if undefined: `1.0`
  retry_delay_secs: Any?

  /// The subscription within the project which is configured to receive logs.
  subscription: String

  tls: Any?
}

/// Configuration for `heroku_logs` source.
class `Vector::sources::herokuLogs::LogplexConfig` {
  /// Controls how acknowledgements are handled by this source.
  ///
  /// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
  /// or sink level.
  ///
  /// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
  /// behavior.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The socket address to listen for connections on.
  address: Any

  auth: Any?

  /// Configures how events are decoded from raw bytes.
  ///
  /// Default if undefined: `{ ["codec"] = "bytes" }`
  decoding: Any?

  /// Framing configuration.
  ///
  /// Framing handles how events are separated when encoded in a raw byte form, where each event is a
  /// frame that must be prefixed, or delimited, in a way that marks where an event begins and ends
  /// within the byte stream.
  ///
  /// Default if undefined: `{ ["method"] = "bytes" }`
  framing: Any?

  /// Configuration of HTTP server keepalive parameters.
  ///
  /// Default if undefined: `{ ["max_connection_age_secs"] = 300 ["max_connection_age_jitter_factor"] =
  /// 0.1 }`
  keepalive: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// A list of URL query parameters to include in the log event.
  ///
  /// These override any values included in the body with conflicting names.
  ///
  /// Default if undefined: `{}`
  query_parameters: Listing<String>?

  tls: Any?
}

/// Filtering configuration.
class `Vector::sources::hostMetrics::FilterList` {
  /// Any patterns which should be excluded.
  ///
  /// The patterns are matched using globbing.
  excludes: Listing<String>?

  /// Any patterns which should be included.
  ///
  /// The patterns are matched using globbing.
  includes: Listing<String>?
}

/// Configuration for the `host_metrics` source.
class `Vector::sources::hostMetrics::HostMetricsConfig` {
  /// Options for the cgroups (controller groups) metrics collector.
  ///
  /// This collector is only available on Linux systems, and only supports either version 2 or hybrid
  /// cgroups.
  ///
  /// Default if undefined: `{ ["levels"] = 100 ["base"] = null ["groups"] { ["includes"] = null
  /// ["excludes"] = null } }`
  cgroups: (Any|CgroupsAlternate1)?

  /// The list of host metric collector services to use.
  ///
  /// Defaults to all collectors.
  ///
  /// Default if undefined: `{ "cpu" "disk" "filesystem" "load" "host" "memory" "network" "cgroups" }`
  collectors: Listing<"cgroups"|"cpu"|"disk"|"filesystem"|"load"|"host"|"memory"|"network">?

  /// Options for the disk metrics collector.
  ///
  /// Default if undefined: `{ ["devices"] { ["includes"] = null ["excludes"] = null } }`
  disk: Disk?

  /// Options for the filesystem metrics collector.
  ///
  /// Default if undefined: `{ ["devices"] { ["includes"] = null ["excludes"] = null } ["filesystems"] {
  /// ["includes"] = null ["excludes"] = null } ["mountpoints"] { ["includes"] = null ["excludes"] = null
  /// } }`
  filesystem: Filesystem?

  /// Overrides the default namespace for the metrics emitted by the source.
  ///
  /// Default if undefined: `"host"`
  namespace: String?

  /// Options for the network metrics collector.
  ///
  /// Default if undefined: `{ ["devices"] { ["includes"] = null ["excludes"] = null } }`
  network: Network?

  /// The interval between metric gathering, in seconds.
  ///
  /// Default if undefined: `15`
  scrape_interval_secs: Any?
}

/// Options for the cgroups (controller groups) metrics collector.
///
/// This collector is only available on Linux systems, and only supports either version 2 or hybrid
/// cgroups.
///
/// Default if undefined: `{ ["levels"] = 100 ["base"] = null ["groups"] { ["includes"] = null
/// ["excludes"] = null } }`
class CgroupsAlternate1 {
  /// The base cgroup name to provide metrics for.
  base: Any?

  /// Base cgroup directory, for testing use only
  base_dir: Any?

  /// Lists of cgroup name patterns to include or exclude in gathering usage metrics.
  ///
  /// Default if undefined: `{ ["includes"] { "*" } ["excludes"] = null }`
  groups: Any?

  /// The number of levels of the cgroups hierarchy for which to report metrics.
  ///
  /// A value of `1` means the root or named cgroup.
  ///
  /// Default if undefined: `100`
  levels: Int(isBetween(0.0, 9.007199254740991E15))?
}

/// Options for the disk metrics collector.
///
/// Default if undefined: `{ ["devices"] { ["includes"] = null ["excludes"] = null } }`
class Disk {
  /// Lists of device name patterns to include or exclude in gathering I/O utilization metrics.
  ///
  /// Default if undefined: `{ ["includes"] { "*" } ["excludes"] = null }`
  devices: Any?
}

/// Options for the filesystem metrics collector.
///
/// Default if undefined: `{ ["devices"] { ["includes"] = null ["excludes"] = null } ["filesystems"] {
/// ["includes"] = null ["excludes"] = null } ["mountpoints"] { ["includes"] = null ["excludes"] = null }
/// }`
class Filesystem {
  /// Lists of device name patterns to include or exclude in gathering usage metrics.
  ///
  /// Default if undefined: `{ ["includes"] { "*" } ["excludes"] = null }`
  devices: Any?

  /// Lists of filesystem name patterns to include or exclude in gathering usage metrics.
  ///
  /// Default if undefined: `{ ["includes"] { "*" } ["excludes"] = null }`
  filesystems: Any?

  /// Lists of mount point path patterns to include or exclude in gathering usage metrics.
  ///
  /// Default if undefined: `{ ["includes"] { "*" } ["excludes"] = null }`
  mountpoints: Any?
}

/// Options for the network metrics collector.
///
/// Default if undefined: `{ ["devices"] { ["includes"] = null ["excludes"] = null } }`
class Network {
  /// Lists of device name patterns to include or exclude in gathering network utilization metrics.
  ///
  /// Default if undefined: `{ ["includes"] { "*" } ["excludes"] = null }`
  devices: Any?
}

/// Configuration for the `http_client` source.
class `Vector::sources::httpClient::client::HttpClientConfig` {
  /// HTTP Authentication.
  auth: Any?

  /// Decoder to use on the HTTP responses.
  ///
  /// Default if undefined: `{ ["codec"] = "bytes" }`
  decoding: Any?

  /// The HTTP endpoint to collect events from.
  ///
  /// The full path must be specified.
  endpoint: String

  /// Framing to use in the decoding.
  ///
  /// Default if undefined: `{ ["method"] = "bytes" }`
  framing: Any?

  /// Headers to apply to the HTTP requests.
  ///
  /// One or more values for the same header can be provided.
  ///
  /// Default if undefined: `{}`
  headers: Mapping<String, Listing<String>>?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// Specifies the method of the HTTP request.
  ///
  /// Default if undefined: `"GET"`
  method: Any?

  /// Custom parameters for the HTTP request query string.
  ///
  /// One or more values for the same parameter key can be provided.
  ///
  /// The parameters provided in this option are appended to any parameters manually provided in the
  /// `endpoint` option.
  ///
  /// Default if undefined: `{}`
  query: Mapping<String, Listing<String>>?

  /// The interval between scrapes. Requests are run concurrently so if a scrape takes longer than the
  /// interval a new scrape will be started. This can take extra resources, set the timeout to a value
  /// lower than the scrape interval to prevent this from happening.
  ///
  /// Default if undefined: `15`
  scrape_interval_secs: Any?

  /// The timeout for each scrape request.
  ///
  /// Default if undefined: `5.0`
  scrape_timeout_secs: Any?

  /// TLS configuration.
  tls: Any?
}

/// Configuration for the `http_server` source.
class `Vector::sources::httpServer::SimpleHttpConfig` {
  /// Controls how acknowledgements are handled by this source.
  ///
  /// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
  /// or sink level.
  ///
  /// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
  /// behavior.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The socket address to listen for connections on.
  ///
  /// It _must_ include a port.
  address: Any

  auth: Any?

  /// Configures how events are decoded from raw bytes.
  decoding: (Any|`Codecs::decoding::DeserializerConfig`)?

  /// The expected encoding of received data.
  ///
  /// For `json` and `ndjson` encodings, the fields of the JSON objects are output as separate fields.
  encoding: (Any|"text"|"ndjson"|"json"|"binary")?

  framing: Any?

  /// A list of HTTP headers to include in the log event.
  ///
  /// Accepts the wildcard (`*`) character for headers matching a specified pattern.
  ///
  /// Specifying "*" results in all headers included in the log event.
  ///
  /// These override any values included in the JSON payload with conflicting names.
  ///
  /// Default if undefined: `{}`
  headers: Listing<String>?

  /// If set, the name of the log field used to add the remote IP to each event
  ///
  /// Default if undefined: `""`
  host_key: Any?

  /// Configuration of HTTP server keepalive parameters.
  ///
  /// Default if undefined: `{ ["max_connection_age_secs"] = 300 ["max_connection_age_jitter_factor"] =
  /// 0.1 }`
  keepalive: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// Specifies the action of the HTTP request.
  ///
  /// Default if undefined: `"POST"`
  method: Any?

  /// The URL path on which log event POST requests are sent.
  ///
  /// Default if undefined: `"/"`
  path: String?

  /// The event key in which the requested URL path used to send the request is stored.
  ///
  /// Default if undefined: `"path"`
  path_key: Any?

  /// A list of URL query parameters to include in the log event.
  ///
  /// These override any values included in the body with conflicting names.
  ///
  /// Default if undefined: `{}`
  query_parameters: Listing<String>?

  /// Specifies the HTTP response status code that will be returned on successful requests.
  ///
  /// Default if undefined: `200`
  response_code: UInt16?

  /// Whether or not to treat the configured `path` as an absolute path.
  ///
  /// If set to `true`, only requests using the exact URL path specified in `path` are accepted.
  /// Otherwise, requests sent to a URL path that starts with the value of `path` are accepted.
  ///
  /// With `strict_path` set to `false` and `path` set to `""`, the configured HTTP source accepts
  /// requests from any URL path.
  ///
  /// Default if undefined: `true`
  strict_path: Boolean?

  tls: Any?
}

/// Configuration for the `internal_logs` source.
class `Vector::sources::internalLogs::InternalLogsConfig` {
  /// Overrides the name of the log field used to add the current hostname to each event.
  ///
  /// By default, the [global `log_schema.host_key` option][global_host_key] is used.
  ///
  /// Set to `""` to suppress this key.
  ///
  /// [global_host_key]:
  /// https://vector.dev/docs/reference/configuration/global-options/#log_schema.host_key
  ///
  /// Default if undefined: `"host"`
  host_key: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// Overrides the name of the log field used to add the current process ID to each event.
  ///
  /// By default, `"pid"` is used.
  ///
  /// Set to `""` to suppress this key.
  ///
  /// Default if undefined: `"pid"`
  pid_key: Any?
}

/// Configuration for the `internal_metrics` source.
///
/// Default if undefined: `{ ["scrape_interval_secs"] = 1.0 ["tags"] { ["host_key"] = "host" ["pid_key"]
/// = null } ["namespace"] = "vector" }`
class `Vector::sources::internalMetrics::InternalMetricsConfig` {
  /// Overrides the default namespace for the metrics emitted by the source.
  ///
  /// Default if undefined: `"vector"`
  namespace: String?

  /// The interval between metric gathering, in seconds.
  ///
  /// Default if undefined: `1.0`
  scrape_interval_secs: Any?

  /// Tag configuration for the `internal_metrics` source.
  ///
  /// Default if undefined: `{ ["host_key"] = "host" ["pid_key"] = null }`
  tags: `Vector::sources::internalMetrics::InternalMetricsConfigTags`?
}

/// Tag configuration for the `internal_metrics` source.
///
/// Default if undefined: `{ ["host_key"] = "host" ["pid_key"] = null }`
class `Vector::sources::internalMetrics::InternalMetricsConfigTags` {
  /// Overrides the name of the tag used to add the peer host to each metric.
  ///
  /// The value is the peer host's address, including the port. For example, `1.2.3.4:9000`.
  ///
  /// By default, the [global `log_schema.host_key` option][global_host_key] is used.
  ///
  /// Set to `""` to suppress this key.
  ///
  /// [global_host_key]:
  /// https://vector.dev/docs/reference/configuration/global-options/#log_schema.host_key
  ///
  /// Default if undefined: `"host"`
  host_key: Any?

  /// Sets the name of the tag to use to add the current process ID to each metric.
  ///
  /// By default, this is not set and the tag is not automatically added.
  pid_key: String?
}

/// Configuration for the `journald` source.
class `Vector::sources::journald::JournaldConfig` {
  /// Controls how acknowledgements are handled by this source.
  ///
  /// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
  /// or sink level.
  ///
  /// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
  /// behavior.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The systemd journal is read in batches, and a checkpoint is set at the end of each batch.
  ///
  /// This option limits the size of the batch.
  ///
  /// Default if undefined: `16`
  batch_size: Int(isBetween(0.0, 9.007199254740991E15))?

  /// Only include entries that occurred after the current boot of the system.
  ///
  /// Default if undefined: `true`
  current_boot_only: Boolean?

  /// The directory used to persist file checkpoint positions.
  ///
  /// By default, the [global `data_dir` option][global_data_dir] is used. Make sure the running user has
  /// write permissions to this directory.
  ///
  /// If this directory is specified, then Vector will attempt to create it.
  ///
  /// [global_data_dir]: https://vector.dev/docs/reference/configuration/global-options/#data_dir
  data_dir: Any?

  /// Whether to emit the [__CURSOR field][cursor]. See also [sd_journal_get_cursor][get_cursor].
  ///
  /// [cursor]:
  /// https://www.freedesktop.org/software/systemd/man/latest/systemd.journal-fields.html#Address%20Fields
  /// [get_cursor]: https://www.freedesktop.org/software/systemd/man/latest/sd_journal_get_cursor.html
  ///
  /// Default if undefined: `false`
  emit_cursor: Boolean?

  /// A list of sets of field/value pairs that, if any are present in a journal entry, excludes the entry
  /// from this source.
  ///
  /// If `exclude_units` is specified, it is merged into this list.
  ///
  /// Default if undefined: `{}`
  exclude_matches: Mapping<String, Listing<String>(isDistinct)>?

  /// A list of unit names to exclude from monitoring.
  ///
  /// Unit names lacking a `.` have `.service` appended to make them a valid service unit name.
  ///
  /// Default if undefined: `{}`
  exclude_units: Listing<String>?

  /// A list of extra command line arguments to pass to `journalctl`.
  ///
  /// If specified, it is merged to the command line arguments as-is.
  ///
  /// Default if undefined: `{}`
  extra_args: Listing<String>?

  /// A list of sets of field/value pairs to monitor.
  ///
  /// If empty or not present, all journal fields are accepted.
  ///
  /// If `include_units` is specified, it is merged into this list.
  ///
  /// Default if undefined: `{}`
  include_matches: Mapping<String, Listing<String>(isDistinct)>?

  /// A list of unit names to monitor.
  ///
  /// If empty or not present, all units are accepted.
  ///
  /// Unit names lacking a `.` have `.service` appended to make them a valid service unit name.
  ///
  /// Default if undefined: `{}`
  include_units: Listing<String>?

  /// The full path of the journal directory.
  ///
  /// If not set, `journalctl` uses the default system journal path.
  journal_directory: Any?

  /// The [journal namespace][journal-namespace].
  ///
  /// This value is passed to `journalctl` through the [`--namespace`
  /// option][journalctl-namespace-option]. If not set, `journalctl` uses the default namespace.
  ///
  /// [journal-namespace]:
  /// https://www.freedesktop.org/software/systemd/man/systemd-journald.service.html#Journal%20Namespaces
  /// [journalctl-namespace-option]:
  /// https://www.freedesktop.org/software/systemd/man/journalctl.html#--namespace=NAMESPACE
  journal_namespace: String?

  /// The full path of the `journalctl` executable.
  ///
  /// If not set, a search is done for the `journalctl` path.
  journalctl_path: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// Enables remapping the `PRIORITY` field from an integer to string value.
  ///
  /// Has no effect unless the value of the field is already an integer.
  ///
  /// Default if undefined: `false`
  @Deprecated
  remap_priority: Boolean?

  /// Only include entries that appended to the journal after the entries have been read.
  ///
  /// Default if undefined: `false`
  since_now: Boolean?
}

/// Kafka authentication configuration.
class `Vector::sources::kafka::KafkaSourceConfig` {
  /// Controls how acknowledgements are handled by this source.
  ///
  /// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
  /// or sink level.
  ///
  /// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
  /// behavior.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// If offsets for consumer group do not exist, set them using this strategy.
  ///
  /// See the [librdkafka
  /// documentation](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md) for the
  /// `auto.offset.reset` option for further clarification.
  ///
  /// Default if undefined: `"largest"`
  auto_offset_reset: String?

  /// A comma-separated list of Kafka bootstrap servers.
  ///
  /// These are the servers in a Kafka cluster that a client should use to bootstrap its connection to
  /// the cluster, allowing discovery of all the other hosts in the cluster.
  ///
  /// Must be in the form of `host:port`, and comma-separated.
  bootstrap_servers: String

  /// The frequency that the consumer offsets are committed (written) to offset storage.
  ///
  /// Default if undefined: `5000`
  commit_interval_ms: Any?

  /// Configures how events are decoded from raw bytes.
  ///
  /// Default if undefined: `{ ["codec"] = "bytes" }`
  decoding: Any?

  /// Timeout to drain pending acknowledgements during shutdown or a Kafka consumer group rebalance.
  ///
  /// When Vector shuts down or the Kafka consumer group revokes partitions from this consumer, wait a
  /// maximum of `drain_timeout_ms` for the source to process pending acknowledgements. Must be less than
  /// `session_timeout_ms` to ensure the consumer is not excluded from the group during a rebalance.
  ///
  /// Default value is half of `session_timeout_ms`.
  drain_timeout_ms: Number(isBetween(0.0, 9.007199254740991E15))?

  /// Maximum time the broker may wait to fill the response.
  ///
  /// Default if undefined: `100`
  fetch_wait_max_ms: Any?

  /// Framing configuration.
  ///
  /// Framing handles how events are separated when encoded in a raw byte form, where each event is a
  /// frame that must be prefixed, or delimited, in a way that marks where an event begins and ends
  /// within the byte stream.
  ///
  /// Default if undefined: `{ ["method"] = "bytes" }`
  framing: Any?

  /// The consumer group name to be used to consume events from Kafka.
  group_id: String

  /// Overrides the name of the log field used to add the headers to each event.
  ///
  /// The value is the headers of the Kafka message itself.
  ///
  /// By default, `"headers"` is used.
  ///
  /// Default if undefined: `"headers"`
  headers_key: Any?

  /// Overrides the name of the log field used to add the message key to each event.
  ///
  /// The value is the message key of the Kafka message itself.
  ///
  /// By default, `"message_key"` is used.
  ///
  /// Default if undefined: `"message_key"`
  key_field: Any?

  /// Advanced options set directly on the underlying `librdkafka` client.
  ///
  /// See the [librdkafka
  /// documentation](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md) for details.
  librdkafka_options: Mapping<String, String>?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// Metrics (beta) configuration.
  ///
  /// Default if undefined: `{ ["topic_lag_metric"] = false }`
  metrics: Metrics?

  /// Overrides the name of the log field used to add the offset to each event.
  ///
  /// The value is the offset of the Kafka message itself.
  ///
  /// By default, `"offset"` is used.
  ///
  /// Default if undefined: `"offset"`
  offset_key: Any?

  /// Overrides the name of the log field used to add the partition to each event.
  ///
  /// The value is the partition from which the Kafka message was consumed from.
  ///
  /// By default, `"partition"` is used.
  ///
  /// Default if undefined: `"partition"`
  partition_key: Any?

  /// The Kafka session timeout.
  ///
  /// Default if undefined: `10000`
  session_timeout_ms: Any?

  /// Timeout for network requests.
  ///
  /// Default if undefined: `60000`
  socket_timeout_ms: Any?

  /// Overrides the name of the log field used to add the topic to each event.
  ///
  /// The value is the topic from which the Kafka message was consumed from.
  ///
  /// By default, `"topic"` is used.
  ///
  /// Default if undefined: `"topic"`
  topic_key: Any?

  /// The Kafka topics names to read events from.
  ///
  /// Regular expression syntax is supported if the topic begins with `^`.
  topics: Listing<String>
}

/// Metrics (beta) configuration.
///
/// Default if undefined: `{ ["topic_lag_metric"] = false }`
class Metrics {
  /// Expose topic lag metrics for all topics and partitions. Metric names are `kafka_consumer_lag`.
  topic_lag_metric: Boolean
}

/// Configuration for the `kubernetes_logs` source.
///
/// Default if undefined: `{ ["extra_label_selector"] = "" ["extra_namespace_label_selector"] = ""
/// ["self_node_name"] = "${VECTOR_SELF_NODE_NAME}" ["extra_field_selector"] = "" ["auto_partial_merge"]
/// = true ["data_dir"] = null ["pod_annotation_fields"] { ["pod_name"] = ".kubernetes.pod_name"
/// ["pod_namespace"] = ".kubernetes.pod_namespace" ["pod_uid"] = ".kubernetes.pod_uid" ["pod_ip"] =
/// ".kubernetes.pod_ip" ["pod_ips"] = ".kubernetes.pod_ips" ["pod_labels"] = ".kubernetes.pod_labels"
/// ["pod_annotations"] = ".kubernetes.pod_annotations" ["pod_node_name"] = ".kubernetes.pod_node_name"
/// ["pod_owner"] = ".kubernetes.pod_owner" ["container_name"] = ".kubernetes.container_name"
/// ["container_id"] = ".kubernetes.container_id" ["container_image"] = ".kubernetes.container_image"
/// ["container_image_id"] = ".kubernetes.container_image_id" } ["namespace_annotation_fields"] {
/// ["namespace_labels"] = ".kubernetes.namespace_labels" } ["node_annotation_fields"] { ["node_labels"]
/// = ".kubernetes.node_labels" } ["include_paths_glob_patterns"] { "**/*" }
/// ["exclude_paths_glob_patterns"] { "**/*.gz" "**/*.tmp" } ["read_from"] = "beginning"
/// ["ignore_older_secs"] = null ["max_read_bytes"] = 2048 ["oldest_first"] = true ["max_line_bytes"] =
/// 32768 ["fingerprint_lines"] = 1 ["glob_minimum_cooldown_ms"] = 60000 ["ingestion_timestamp_field"] =
/// null ["timezone"] = null ["kube_config_file"] = null ["use_apiserver_cache"] = false
/// ["delay_deletion_ms"] = 60000 ["log_namespace"] = null ["internal_metrics"] { ["include_file_tag"] =
/// false } ["rotate_wait_secs"] = 9223372036854775807 }`
class `Vector::sources::kubernetesLogs::Config` {
  /// Whether or not to automatically merge partial events.
  ///
  /// Partial events are messages that were split by the Kubernetes Container Runtime log driver.
  auto_partial_merge: Boolean?

  /// The directory used to persist file checkpoint positions.
  ///
  /// By default, the [global `data_dir` option][global_data_dir] is used. Make sure the running user has
  /// write permissions to this directory.
  ///
  /// If this directory is specified, then Vector will attempt to create it.
  ///
  /// [global_data_dir]: https://vector.dev/docs/reference/configuration/global-options/#data_dir
  data_dir: Any?

  /// How long to delay removing metadata entries from the cache when a pod deletion event event is
  /// received from the watch stream.
  ///
  /// A longer delay allows for continued enrichment of logs after the originating Pod is removed. If
  /// relevant metadata has been removed, the log is forwarded un-enriched and a warning is emitted.
  delay_deletion_ms: Any?

  /// A list of glob patterns to exclude from reading the files.
  exclude_paths_glob_patterns: Listing<Any>?

  /// Specifies the [field selector][field_selector] to filter Pods with, to be used in addition to the
  /// built-in [Node][node] filter.
  ///
  /// The built-in Node filter uses `self_node_name` to only watch Pods located on the same Node.
  ///
  /// [field_selector]:
  /// https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/ [node]:
  /// https://kubernetes.io/docs/concepts/architecture/nodes/
  extra_field_selector: String?

  /// Specifies the [label selector][label_selector] to filter [Pods][pods] with, to be used in addition
  /// to the built-in [exclude][exclude] filter.
  ///
  /// [label_selector]:
  /// https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors [pods]:
  /// https://kubernetes.io/docs/concepts/workloads/pods/ [exclude]:
  /// https://vector.dev/docs/reference/configuration/sources/kubernetes_logs/#pod-exclusion
  extra_label_selector: String?

  /// Specifies the [label selector][label_selector] to filter [Namespaces][namespaces] with, to be used
  /// in addition to the built-in [exclude][exclude] filter.
  ///
  /// [label_selector]:
  /// https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors
  /// [namespaces]: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
  /// [exclude]:
  /// https://vector.dev/docs/reference/configuration/sources/kubernetes_logs/#namespace-exclusion
  extra_namespace_label_selector: String?

  /// The number of lines to read for generating the checksum.
  ///
  /// If your files share a common header that is not always a fixed size,
  ///
  /// If the file has less than this amount of lines, it wonât be read at all.
  fingerprint_lines: Int(isBetween(0.0, 9.007199254740991E15))?

  /// The interval at which the file system is polled to identify new files to read from.
  ///
  /// This is quite efficient, yet might still create some load on the file system; in addition, it is
  /// currently coupled with checksum dumping in the underlying file server, so setting it too low may
  /// introduce a significant overhead.
  glob_minimum_cooldown_ms: Any?

  /// Ignore files with a data modification date older than the specified number of seconds.
  ignore_older_secs: Number(isBetween(0.0, 9.007199254740991E15))?

  /// A list of glob patterns to include while reading the files.
  include_paths_glob_patterns: Listing<Any>?

  /// Overrides the name of the log field used to add the ingestion timestamp to each event.
  ///
  /// This is useful to compute the latency between important event processing stages. For example, the
  /// time delta between when a log line was written and when it was processed by the `kubernetes_logs`
  /// source.
  ingestion_timestamp_field: Any?

  /// Configuration of internal metrics for file-based components.
  ///
  /// Default if undefined: `{ ["include_file_tag"] = false }`
  internal_metrics: Any?

  /// Optional path to a readable [kubeconfig][kubeconfig] file.
  ///
  /// If not set, a connection to Kubernetes is made using the in-cluster configuration.
  ///
  /// [kubeconfig]: https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/
  kube_config_file: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// The maximum number of bytes a line can contain before being discarded.
  ///
  /// This protects against malformed lines or tailing incorrect files.
  max_line_bytes: Int(isBetween(0.0, 9.007199254740991E15))?

  /// Max amount of bytes to read from a single file before switching over to the next file. **Note:**
  /// This does not apply when `oldest_first` is `true`.
  ///
  /// This allows distributing the reads more or less evenly across the files.
  max_read_bytes: Int(isBetween(0.0, 9.007199254740991E15))?

  /// Configuration for how the events are enriched with Namespace metadata.
  ///
  /// Default if undefined: `{ ["namespace_labels"] = ".kubernetes.namespace_labels" }`
  namespace_annotation_fields: NamespaceAnnotationFields?

  /// Configuration for how the events are enriched with Node metadata.
  ///
  /// Default if undefined: `{ ["node_labels"] = ".kubernetes.node_labels" }`
  node_annotation_fields: NodeAnnotationFields?

  /// Instead of balancing read capacity fairly across all watched files, prioritize draining the oldest
  /// files before moving on to read data from more recent files.
  ///
  /// Default if undefined: `true`
  oldest_first: Boolean?

  /// Configuration for how the events are enriched with Pod metadata.
  ///
  /// Default if undefined: `{ ["pod_name"] = ".kubernetes.pod_name" ["pod_namespace"] =
  /// ".kubernetes.pod_namespace" ["pod_uid"] = ".kubernetes.pod_uid" ["pod_ip"] = ".kubernetes.pod_ip"
  /// ["pod_ips"] = ".kubernetes.pod_ips" ["pod_labels"] = ".kubernetes.pod_labels" ["pod_annotations"] =
  /// ".kubernetes.pod_annotations" ["pod_node_name"] = ".kubernetes.pod_node_name" ["pod_owner"] =
  /// ".kubernetes.pod_owner" ["container_name"] = ".kubernetes.container_name" ["container_id"] =
  /// ".kubernetes.container_id" ["container_image"] = ".kubernetes.container_image"
  /// ["container_image_id"] = ".kubernetes.container_image_id" }`
  pod_annotation_fields: PodAnnotationFields?

  /// File position to use when reading a new file.
  ///
  /// Default if undefined: `"beginning"`
  read_from: Any?

  /// How long to keep an open handle to a rotated log file. The default value represents "no limit"
  ///
  /// Default if undefined: `9223372036854775807`
  rotate_wait_secs: Any?

  /// The name of the Kubernetes [Node][node] that is running.
  ///
  /// Configured to use an environment variable by default, to be evaluated to a value provided by
  /// Kubernetes at Pod creation.
  ///
  /// [node]: https://kubernetes.io/docs/concepts/architecture/nodes/
  self_node_name: String?

  /// The default time zone for timestamps without an explicit zone.
  timezone: Any?

  /// Determines if requests to the kube-apiserver can be served by a cache.
  use_apiserver_cache: Boolean?
}

/// Configuration for how the events are enriched with Namespace metadata.
///
/// Default if undefined: `{ ["namespace_labels"] = ".kubernetes.namespace_labels" }`
class NamespaceAnnotationFields {
  /// Event field for the Namespace's labels.
  ///
  /// Set to `""` to suppress this key.
  namespace_labels: Any?
}

/// Configuration for how the events are enriched with Node metadata.
///
/// Default if undefined: `{ ["node_labels"] = ".kubernetes.node_labels" }`
class NodeAnnotationFields {
  /// Event field for the Node's labels.
  ///
  /// Set to `""` to suppress this key.
  node_labels: Any?
}

/// Configuration for how the events are enriched with Pod metadata.
///
/// Default if undefined: `{ ["pod_name"] = ".kubernetes.pod_name" ["pod_namespace"] =
/// ".kubernetes.pod_namespace" ["pod_uid"] = ".kubernetes.pod_uid" ["pod_ip"] = ".kubernetes.pod_ip"
/// ["pod_ips"] = ".kubernetes.pod_ips" ["pod_labels"] = ".kubernetes.pod_labels" ["pod_annotations"] =
/// ".kubernetes.pod_annotations" ["pod_node_name"] = ".kubernetes.pod_node_name" ["pod_owner"] =
/// ".kubernetes.pod_owner" ["container_name"] = ".kubernetes.container_name" ["container_id"] =
/// ".kubernetes.container_id" ["container_image"] = ".kubernetes.container_image" ["container_image_id"]
/// = ".kubernetes.container_image_id" }`
class PodAnnotationFields {
  /// Event field for the Container's ID.
  ///
  /// Set to `""` to suppress this key.
  container_id: Any?

  /// Event field for the Container's image.
  ///
  /// Set to `""` to suppress this key.
  container_image: Any?

  /// Event field for the Container's image ID.
  ///
  /// Set to `""` to suppress this key.
  container_image_id: Any?

  /// Event field for the Container's name.
  ///
  /// Set to `""` to suppress this key.
  container_name: Any?

  /// Event field for the Pod's annotations.
  ///
  /// Set to `""` to suppress this key.
  pod_annotations: Any?

  /// Event field for the Pod's IPv4 address.
  ///
  /// Set to `""` to suppress this key.
  pod_ip: Any?

  /// Event field for the Pod's IPv4 and IPv6 addresses.
  ///
  /// Set to `""` to suppress this key.
  pod_ips: Any?

  /// Event field for the `Pod`'s labels.
  ///
  /// Set to `""` to suppress this key.
  pod_labels: Any?

  /// Event field for the Pod's name.
  ///
  /// Set to `""` to suppress this key.
  pod_name: Any?

  /// Event field for the Pod's namespace.
  ///
  /// Set to `""` to suppress this key.
  pod_namespace: Any?

  /// Event field for the Pod's node_name.
  ///
  /// Set to `""` to suppress this key.
  pod_node_name: Any?

  /// Event field for the Pod's owner reference.
  ///
  /// Set to `""` to suppress this key.
  pod_owner: Any?

  /// Event field for the Pod's UID.
  ///
  /// Set to `""` to suppress this key.
  pod_uid: Any?
}

/// Configuration for the `logstash` source.
class `Vector::sources::logstash::LogstashConfig` {
  /// Controls how acknowledgements are handled by this source.
  ///
  /// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
  /// or sink level.
  ///
  /// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
  /// behavior.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The socket address to listen for connections on, or `systemd{#N}` to use the Nth socket passed by
  /// systemd socket activation.
  ///
  /// If a socket address is used, it _must_ include a port.
  address: Any

  /// The maximum number of TCP connections that are allowed at any given time.
  connection_limit: Number(isBetween(0.0, 4.294967295E9))?

  keepalive: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  permit_origin: Any?

  /// The size of the receive buffer used for each connection.
  receive_buffer_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  tls: Any?
}

/// Configuration for the `mongodb_metrics` source.
class `Vector::sources::mongodbMetrics::MongoDbMetricsConfig` {
  /// A list of MongoDB instances to scrape.
  ///
  /// Each endpoint must be in the [Connection String URI
  /// Format](https://www.mongodb.com/docs/manual/reference/connection-string/).
  endpoints: Listing<String>

  /// Overrides the default namespace for the metrics emitted by the source.
  ///
  /// If set to an empty string, no namespace is added to the metrics.
  ///
  /// By default, `mongodb` is used.
  ///
  /// Default if undefined: `"mongodb"`
  namespace: String?

  /// The interval between scrapes, in seconds.
  ///
  /// Default if undefined: `15`
  scrape_interval_secs: Any?
}

/// Configuration for the `nats` source.
class `Vector::sources::nats::NatsSourceConfig` {
  auth: Any?

  /// A [name][nats_connection_name] assigned to the NATS connection.
  ///
  /// [nats_connection_name]: https://docs.nats.io/using-nats/developer/connecting/name
  connection_name: String

  /// Configures how events are decoded from raw bytes.
  ///
  /// Default if undefined: `{ ["codec"] = "bytes" }`
  decoding: Any?

  /// Framing configuration.
  ///
  /// Framing handles how events are separated when encoded in a raw byte form, where each event is a
  /// frame that must be prefixed, or delimited, in a way that marks where an event begins and ends
  /// within the byte stream.
  ///
  /// Default if undefined: `{ ["method"] = "bytes" }`
  framing: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// The NATS queue group to join.
  queue: String?

  /// The NATS [subject][nats_subject] to pull messages from.
  ///
  /// [nats_subject]: https://docs.nats.io/nats-concepts/subjects
  subject: String

  /// The `NATS` subject key.
  ///
  /// Default if undefined: `"subject"`
  subject_key_field: Any?

  /// The buffer capacity of the underlying NATS subscriber.
  ///
  /// This value determines how many messages the NATS subscriber buffers before incoming messages are
  /// dropped.
  ///
  /// See the [async_nats documentation][async_nats_subscription_capacity] for more information.
  ///
  /// [async_nats_subscription_capacity]:
  /// https://docs.rs/async-nats/latest/async_nats/struct.ConnectOptions.html#method.subscription_capacity
  ///
  /// Default if undefined: `4096`
  subscriber_capacity: Int(isBetween(0.0, 9.007199254740991E15))?

  tls: Any?

  /// The NATS URL to connect to.
  ///
  /// The URL takes the form of `nats://server:port`. If the port is not specified it defaults to 4222.
  url: String
}

/// Configuration for the `nginx_metrics` source.
class `Vector::sources::nginxMetrics::NginxMetricsConfig` {
  auth: Any?

  /// A list of NGINX instances to scrape.
  ///
  /// Each endpoint must be a valid HTTP/HTTPS URI pointing to an NGINX instance that has the
  /// `ngx_http_stub_status_module` module enabled.
  endpoints: Listing<String>

  /// Overrides the default namespace for the metrics emitted by the source.
  ///
  /// If set to an empty string, no namespace is added to the metrics.
  ///
  /// By default, `nginx` is used.
  ///
  /// Default if undefined: `"nginx"`
  namespace: String?

  /// The interval between scrapes.
  ///
  /// Default if undefined: `15`
  scrape_interval_secs: Any?

  tls: Any?
}

/// Configuration for the `opentelemetry` source.
class `Vector::sources::opentelemetry::OpentelemetryConfig` {
  /// Controls how acknowledgements are handled by this source.
  ///
  /// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
  /// or sink level.
  ///
  /// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
  /// behavior.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Configuration for the `opentelemetry` gRPC server.
  grpc: Grpc

  /// Configuration for the `opentelemetry` HTTP server.
  http: Http

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?
}

/// Configuration for the `opentelemetry` gRPC server.
class Grpc {
  /// The socket address to listen for connections on.
  ///
  /// It _must_ include a port.
  address: Any

  tls: Any?
}

/// Configuration for the `opentelemetry` HTTP server.
class Http {
  /// The socket address to listen for connections on.
  ///
  /// It _must_ include a port.
  address: Any

  /// Configuration of HTTP server keepalive parameters.
  ///
  /// Default if undefined: `{ ["max_connection_age_secs"] = 300 ["max_connection_age_jitter_factor"] =
  /// 0.1 }`
  keepalive: Any?

  tls: Any?
}

/// Configuration for the `postgresql_metrics` source.
class `Vector::sources::postgresqlMetrics::PostgresqlMetricsConfig` {
  /// A list of PostgreSQL instances to scrape.
  ///
  /// Each endpoint must be in the [Connection URI
  /// format](https://www.postgresql.org/docs/current/libpq-connect.html#id-1.7.3.8.3.6).
  endpoints: Listing<String>

  /// A list of databases to match (by using [POSIX Regular
  /// Expressions](https://www.postgresql.org/docs/current/functions-matching.html#FUNCTIONS-POSIX-REGEXP))
  /// against the `datname` column for which you donât want to collect metrics from.
  ///
  /// Specifying `""` includes metrics where `datname` is `NULL`.
  ///
  /// This can be used in conjunction with `include_databases`.
  exclude_databases: Listing<String>?

  /// A list of databases to match (by using [POSIX Regular
  /// Expressions](https://www.postgresql.org/docs/current/functions-matching.html#FUNCTIONS-POSIX-REGEXP))
  /// against the `datname` column for which you want to collect metrics from.
  ///
  /// If not set, metrics are collected from all databases. Specifying `""` includes metrics where
  /// `datname` is `NULL`.
  ///
  /// This can be used in conjunction with `exclude_databases`.
  include_databases: Listing<String>?

  /// Overrides the default namespace for the metrics emitted by the source.
  ///
  /// Default if undefined: `"postgresql"`
  namespace: String?

  /// The interval between scrapes.
  ///
  /// Default if undefined: `15`
  scrape_interval_secs: Any?

  /// Configuration of TLS when connecting to PostgreSQL.
  tls: (Any|TlsTlsAlternate1)?
}

/// Configuration of TLS when connecting to PostgreSQL.
class TlsTlsAlternate1 {
  /// Absolute path to an additional CA certificate file.
  ///
  /// The certificate must be in the DER or PEM (X.509) format.
  ca_file: Any
}

/// Configuration for the `prometheus_pushgateway` source.
class `Vector::sources::prometheus::pushgateway::PrometheusPushgatewayConfig` {
  /// Controls how acknowledgements are handled by this source.
  ///
  /// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
  /// or sink level.
  ///
  /// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
  /// behavior.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The socket address to accept connections on.
  ///
  /// The address _must_ include a port.
  address: Any

  /// Whether to aggregate values across pushes.
  ///
  /// Only applies to counters and histograms as gauges and summaries can't be meaningfully aggregated.
  ///
  /// Default if undefined: `false`
  aggregate_metrics: Boolean?

  auth: Any?

  /// Configuration of HTTP server keepalive parameters.
  ///
  /// Default if undefined: `{ ["max_connection_age_secs"] = 300 ["max_connection_age_jitter_factor"] =
  /// 0.1 }`
  keepalive: Any?

  tls: Any?
}

/// Configuration for the `prometheus_remote_write` source.
class `Vector::sources::prometheus::remoteWrite::PrometheusRemoteWriteConfig` {
  /// Controls how acknowledgements are handled by this source.
  ///
  /// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
  /// or sink level.
  ///
  /// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
  /// behavior.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The socket address to accept connections on.
  ///
  /// The address _must_ include a port.
  address: Any

  auth: Any?

  /// Configuration of HTTP server keepalive parameters.
  ///
  /// Default if undefined: `{ ["max_connection_age_secs"] = 300 ["max_connection_age_jitter_factor"] =
  /// 0.1 }`
  keepalive: Any?

  tls: Any?
}

/// Configuration for the `prometheus_scrape` source.
class `Vector::sources::prometheus::scrape::PrometheusScrapeConfig` {
  auth: Any?

  /// The tag name added to each event representing the scraped instance's endpoint.
  ///
  /// The tag value is the endpoint of the scraped instance.
  endpoint_tag: String?

  /// Endpoints to scrape metrics from.
  endpoints: Listing<String>

  /// Controls how tag conflicts are handled if the scraped source has tags to be added.
  ///
  /// If `true`, the new tag is not added if the scraped metric has the tag already. If `false`, the
  /// conflicting tag is renamed by prepending `exported_` to the original name.
  ///
  /// This matches Prometheusâ `honor_labels` configuration.
  ///
  /// Default if undefined: `false`
  honor_labels: Boolean?

  /// The tag name added to each event representing the scraped instance's `host:port`.
  ///
  /// The tag value is the host and port of the scraped instance.
  instance_tag: String?

  /// Custom parameters for the scrape request query string.
  ///
  /// One or more values for the same parameter key can be provided. The parameters provided in this
  /// option are appended to any parameters manually provided in the `endpoints` option. This option is
  /// especially useful when scraping the `/federate` endpoint.
  ///
  /// Default if undefined: `{}`
  query: Mapping<String, Listing<String>>?

  /// The interval between scrapes. Requests are run concurrently so if a scrape takes longer than the
  /// interval a new scrape will be started. This can take extra resources, set the timeout to a value
  /// lower than the scrape interval to prevent this from happening.
  ///
  /// Default if undefined: `15`
  scrape_interval_secs: Any?

  /// The timeout for each scrape request.
  ///
  /// Default if undefined: `5.0`
  scrape_timeout_secs: Any?

  tls: Any?
}

/// Configuration for the `pulsar` source.
class `Vector::sources::pulsar::PulsarSourceConfig` {
  /// Controls how acknowledgements are handled by this source.
  ///
  /// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
  /// or sink level.
  ///
  /// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
  /// behavior.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// Authentication configuration.
  auth: (Any|AuthAuthAlternate1AuthAlternate1Alternate0|AuthAuthAlternate1AuthAlternate1Alternate1)?

  /// Max count of messages in a batch.
  batch_size: Number(isBetween(0.0, 4.294967295E9))?

  /// The Pulsar consumer name.
  consumer_name: String?

  /// Dead Letter Queue policy configuration.
  dead_letter_queue_policy: (Any|DeadLetterQueuePolicyAlternate1)?

  /// Configures how events are decoded from raw bytes.
  ///
  /// Default if undefined: `{ ["codec"] = "bytes" }`
  decoding: Any?

  /// The endpoint to which the Pulsar client should connect to.
  endpoint: String

  /// Framing configuration.
  ///
  /// Framing handles how events are separated when encoded in a raw byte form, where each event is a
  /// frame that must be prefixed, or delimited, in a way that marks where an event begins and ends
  /// within the byte stream.
  ///
  /// Default if undefined: `{ ["method"] = "bytes" }`
  framing: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// The consumer's priority level.
  ///
  /// The broker follows descending priorities. For example, 0=max-priority, 1, 2,...
  ///
  /// In Shared subscription type, the broker first dispatches messages to the max priority level
  /// consumers if they have permits. Otherwise, the broker considers next priority level consumers.
  priority_level: Number(isBetween(-2.147483648E9, 2.147483647E9))?

  /// The Pulsar subscription name.
  subscription_name: String?

  /// The Pulsar topic names to read events from.
  topics: Listing<String>
}

/// Basic authentication.
class AuthAuthAlternate1AuthAlternate1Alternate0 {
  /// Basic authentication name/username.
  ///
  /// This can be used either for basic authentication (username/password) or JWT authentication. When
  /// used for JWT, the value should be `token`.
  name: String

  /// Basic authentication password/token.
  ///
  /// This can be used either for basic authentication (username/password) or JWT authentication. When
  /// used for JWT, the value should be the signed JWT, in the compact representation.
  token: Any
}

/// OAuth authentication.
class AuthAuthAlternate1AuthAlternate1Alternate1 {
  /// OAuth2-specific authentication configuration.
  oauth2: Oauth2
}

/// Dead Letter Queue policy configuration.
class DeadLetterQueuePolicyAlternate1 {
  /// Name of the dead letter topic where the failing messages will be sent.
  dead_letter_topic: String

  /// Maximum number of times that a message will be redelivered before being sent to the dead letter
  /// queue.
  max_redeliver_count: Int(isBetween(0.0, 9.007199254740991E15))
}

/// Configuration for the `redis` source.
class `Vector::sources::redis::RedisSourceConfig` {
  /// The Redis data type (`list` or `channel`) to use.
  ///
  /// Default if undefined: `"list"`
  data_type: ("list"|"channel")?

  /// Configures how events are decoded from raw bytes.
  ///
  /// Default if undefined: `{ ["codec"] = "bytes" }`
  decoding: Any?

  /// Framing configuration.
  ///
  /// Framing handles how events are separated when encoded in a raw byte form, where each event is a
  /// frame that must be prefixed, or delimited, in a way that marks where an event begins and ends
  /// within the byte stream.
  ///
  /// Default if undefined: `{ ["method"] = "bytes" }`
  framing: Any?

  /// The Redis key to read messages from.
  key: String

  /// Options for the Redis `list` data type.
  list: (Any|ListAlternate1)?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// Sets the name of the log field to use to add the key to each event.
  ///
  /// The value is the Redis key that the event was read from.
  ///
  /// By default, this is not set and the field is not automatically added.
  redis_key: Any?

  /// The Redis URL to connect to.
  ///
  /// The URL must take the form of `protocol://server:port/db` where the `protocol` can either be
  /// `redis` or `rediss` for connections secured using TLS.
  url: String
}

/// Options for the Redis `list` data type.
class ListAlternate1 {
  /// Method for getting events from the `list` data type.
  method: "lpop"|"rpop"
}

/// TCP configuration for the `socket` source.
class `Vector::sources::socket::SocketConfigAlternate0` {
  /// Listen on TCP.
  mode: "tcp"
}

/// UDP configuration for the `socket` source.
class `Vector::sources::socket::SocketConfigAlternate1` {
  /// Listen on UDP.
  mode: "udp"
}

/// Unix domain socket configuration for the `socket` source.
class `Vector::sources::socket::SocketConfigAlternate2` {
  /// Listen on a Unix domain socket (UDS), in datagram mode.
  mode: "unix_datagram"
}

/// Unix domain socket configuration for the `socket` source.
class `Vector::sources::socket::SocketConfigAlternate3` {
  /// Listen on a Unix domain socket (UDS), in stream mode.
  mode: "unix_stream"
}

/// Configuration for the `splunk_hec` source.
///
/// Default if undefined: `{ ["address"] = "0.0.0.0:8088" ["token"] = null ["valid_tokens"] = null
/// ["store_hec_token"] = false ["tls"] = null ["acknowledgements"] { ["enabled"] = null
/// ["max_pending_acks"] = 10000000 ["max_number_of_ack_channels"] = 1000000
/// ["max_pending_acks_per_channel"] = 1000000 ["ack_idle_cleanup"] = false ["max_idle_time"] = 300 }
/// ["log_namespace"] = null ["keepalive"] { ["max_connection_age_secs"] = 300
/// ["max_connection_age_jitter_factor"] = 0.1 } }`
class `Vector::sources::splunkHec::SplunkConfig` {
  /// Acknowledgement configuration for the `splunk_hec` source.
  ///
  /// Default if undefined: `{ ["enabled"] = null ["max_pending_acks"] = 10000000
  /// ["max_number_of_ack_channels"] = 1000000 ["max_pending_acks_per_channel"] = 1000000
  /// ["ack_idle_cleanup"] = false ["max_idle_time"] = 300 }`
  acknowledgements: Acknowledgements?

  /// The socket address to listen for connections on.
  ///
  /// The address _must_ include a port.
  ///
  /// Default if undefined: `"0.0.0.0:8088"`
  address: Any?

  /// Configuration of HTTP server keepalive parameters.
  ///
  /// Default if undefined: `{ ["max_connection_age_secs"] = 300 ["max_connection_age_jitter_factor"] =
  /// 0.1 }`
  keepalive: Any?

  /// The namespace to use for logs. This overrides the global settings.
  log_namespace: Boolean?

  /// Whether or not to forward the Splunk HEC authentication token with events.
  ///
  /// If set to `true`, when incoming requests contain a Splunk HEC token, the token used is kept in the
  /// event metadata and preferentially used if the event is sent to a Splunk HEC sink.
  store_hec_token: Boolean?

  tls: Any?

  /// Optional authorization token.
  ///
  /// If supplied, incoming requests must supply this token in the `Authorization` header, just as a
  /// client would if it was communicating with the Splunk HEC endpoint directly.
  ///
  /// If _not_ supplied, the `Authorization` header is ignored and requests are not authenticated.
  @Deprecated
  token: Any?

  /// A list of valid authorization tokens.
  ///
  /// If supplied, incoming requests must supply one of these tokens in the `Authorization` header, just
  /// as a client would if it was communicating with the Splunk HEC endpoint directly.
  ///
  /// If _not_ supplied, the `Authorization` header is ignored and requests are not authenticated.
  valid_tokens: Listing<Any>?
}

/// Acknowledgement configuration for the `splunk_hec` source.
///
/// Default if undefined: `{ ["enabled"] = null ["max_pending_acks"] = 10000000
/// ["max_number_of_ack_channels"] = 1000000 ["max_pending_acks_per_channel"] = 1000000
/// ["ack_idle_cleanup"] = false ["max_idle_time"] = 300 }`
class Acknowledgements {
  /// Whether or not to remove channels after idling for `max_idle_time` seconds.
  ///
  /// A channel is idling if it is not used for sending data or querying acknowledgement statuses.
  ack_idle_cleanup: Boolean?

  /// Enables end-to-end acknowledgements.
  enabled: Boolean?

  /// The amount of time, in seconds, a channel is allowed to idle before removal.
  ///
  /// Channels can potentially idle for longer than this setting but clients should not rely on such
  /// behavior.
  ///
  /// Minimum of `1`.
  max_idle_time: Int(isBetween(1.0, 9.007199254740991E15))?

  /// The maximum number of Splunk HEC channels clients can use with this source.
  ///
  /// Minimum of `1`.
  max_number_of_ack_channels: Int(isBetween(1.0, 9.007199254740991E15))?

  /// The maximum number of acknowledgement statuses pending query across all channels.
  ///
  /// Equivalent to the `max_number_of_acked_requests_pending_query` Splunk HEC setting.
  ///
  /// Minimum of `1`.
  max_pending_acks: Int(isBetween(1.0, 9.007199254740991E15))?

  /// The maximum number of acknowledgement statuses pending query for a single channel.
  ///
  /// Equivalent to the `max_number_of_acked_requests_pending_query_per_ack_channel` Splunk HEC setting.
  ///
  /// Minimum of `1`.
  max_pending_acks_per_channel: Int(isBetween(1.0, 9.007199254740991E15))?
}

/// TCP configuration for the `statsd` source.
class `Vector::sources::statsd::StatsdConfigAlternate0` {
  /// Listen on TCP.
  mode: "tcp"
}

/// UDP configuration for the `statsd` source.
class `Vector::sources::statsd::StatsdConfigAlternate1` {
  /// Listen on UDP.
  mode: "udp"
}

/// Unix domain socket configuration for the `statsd` source.
class `Vector::sources::statsd::StatsdConfigAlternate2` {
  /// Listen on a Unix domain Socket (UDS).
  mode: "unix"
}

/// Listener mode for the `syslog` source.
class `Vector::sources::syslog::SyslogConfig` {
  /// Overrides the name of the log field used to add the peer host to each event.
  ///
  /// If using TCP or UDP, the value is the peer host's address, including the port. For example,
  /// `1.2.3.4:9000`. If using UDS, the value is the socket path itself.
  ///
  /// By default, the [global `log_schema.host_key` option][global_host_key] is used.
  ///
  /// [global_host_key]:
  /// https://vector.dev/docs/reference/configuration/global-options/#log_schema.host_key
  host_key: Any?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// The maximum buffer size of incoming messages, in bytes.
  ///
  /// Messages larger than this are truncated.
  ///
  /// Default if undefined: `102400`
  max_length: Int(isBetween(0.0, 9.007199254740991E15))?
}

/// Listen on TCP.
class `Vector::sources::syslog::SyslogConfigAlternate0` {
  /// The socket address to listen for connections on, or `systemd{#N}` to use the Nth socket passed by
  /// systemd socket activation.
  ///
  /// If a socket address is used, it _must_ include a port.
  address: Any

  /// The maximum number of TCP connections that are allowed at any given time.
  connection_limit: Number(isBetween(0.0, 4.294967295E9))?

  keepalive: Any?

  /// Listen on TCP.
  mode: "tcp"

  permit_origin: Any?

  /// The size of the receive buffer used for each connection.
  ///
  /// This should not typically needed to be changed.
  receive_buffer_bytes: Number(isBetween(0.0, 9.007199254740991E15))?

  tls: Any?
}

/// Listen on UDP.
class `Vector::sources::syslog::SyslogConfigAlternate1` {
  /// The socket address to listen for connections on, or `systemd{#N}` to use the Nth socket passed by
  /// systemd socket activation.
  ///
  /// If a socket address is used, it _must_ include a port.
  address: Any

  /// Listen on UDP.
  mode: "udp"

  /// The size of the receive buffer used for the listening socket.
  ///
  /// This should not typically needed to be changed.
  receive_buffer_bytes: Number(isBetween(0.0, 9.007199254740991E15))?
}

/// Listen on UDS (Unix domain socket). This only supports Unix stream sockets.
///
/// For Unix datagram sockets, use the `socket` source instead.
class `Vector::sources::syslog::SyslogConfigAlternate2` {
  /// Listen on UDS (Unix domain socket). This only supports Unix stream sockets.
  ///
  /// For Unix datagram sockets, use the `socket` source instead.
  mode: "unix"

  /// The Unix socket path.
  ///
  /// This should be an absolute path.
  path: Any

  /// Unix file mode bits to be applied to the unix socket file as its designated file permissions.
  ///
  /// The file mode value can be specified in any numeric format supported by your configuration
  /// language, but it is most intuitive to use an octal number.
  socket_file_mode: Number(isBetween(0.0, 4.294967295E9))?
}

/// Configuration for the `vector` source.
class `Vector::sources::vector::VectorConfig` {
  /// Controls how acknowledgements are handled by this source.
  ///
  /// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
  /// or sink level.
  ///
  /// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
  /// behavior.
  ///
  /// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
  /// handled.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  /// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
  ///
  /// Default if undefined: `{ ["enabled"] = null }`
  acknowledgements: Any?

  /// The socket address to listen for connections on.
  ///
  /// It _must_ include a port.
  address: Any

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  tls: Any?

  /// Version of the configuration.
  version: (Any|"2")?
}

/// Configuration for the `aggregate` transform.
class `Vector::transforms::TransformsAlternate0` {
  /// Aggregate metrics passing through a topology.
  type: "aggregate"
}

/// Configuration for the `aws_ec2_metadata` transform.
class `Vector::transforms::TransformsAlternate1` {
  /// Parse metadata emitted by AWS EC2 instances.
  type: "aws_ec2_metadata"
}

/// Configuration for the `dedupe` transform.
class `Vector::transforms::TransformsAlternate2` {
  /// Deduplicate logs passing through a topology.
  type: "dedupe"
}

/// Configuration for the `filter` transform.
class `Vector::transforms::TransformsAlternate3` {
  /// Filter events based on a set of conditions.
  type: "filter"
}

/// Configuration for the `log_to_metric` transform.
class `Vector::transforms::TransformsAlternate4` {
  /// Convert log events to metric events.
  type: "log_to_metric"
}

/// Configuration for the `lua` transform.
class `Vector::transforms::TransformsAlternate5` {
  /// Modify event data using the Lua programming language.
  type: "lua"
}

/// Configuration for the `metric_to_log` transform.
class `Vector::transforms::TransformsAlternate6` {
  /// Convert metric events to log events.
  type: "metric_to_log"
}

/// Configuration for the `reduce` transform.
class `Vector::transforms::TransformsAlternate7` {
  /// Collapse multiple log events into a single event based on a set of conditions and merge strategies.
  type: "reduce"
}

/// Configuration for the `remap` transform.
class `Vector::transforms::TransformsAlternate8` {
  /// Modify your observability data as it passes through your topology using Vector Remap Language
  /// (VRL).
  type: "remap"
}

/// Configuration for the `route` transform.
class `Vector::transforms::TransformsAlternate9` {
  /// Split a stream of events into multiple sub-streams based on user-supplied conditions.
  type: "route"
}

/// Configuration for the `sample` transform.
class `Vector::transforms::TransformsAlternate10` {
  /// Sample events from an event stream based on supplied criteria and at a configurable rate.
  type: "sample"
}

/// Configuration for the `tag_cardinality_limit` transform.
class `Vector::transforms::TransformsAlternate11` {
  /// Limit the cardinality of tags on metrics events as a safeguard against cardinality explosion.
  type: "tag_cardinality_limit"
}

/// Configuration for the `throttle` transform.
class `Vector::transforms::TransformsAlternate12` {
  /// Rate limit logs passing through a topology.
  type: "throttle"
}

/// Configuration for the `aggregate` transform.
class `Vector::transforms::aggregate::AggregateConfig` {
  /// The interval between flushes, in milliseconds.
  ///
  /// During this time frame, metrics (beta) with the same series data (name, namespace, tags, and so on)
  /// are aggregated.
  ///
  /// Default if undefined: `10000`
  interval_ms: Int(isBetween(0.0, 9.007199254740991E15))?
}

/// Configuration for the `aws_ec2_metadata` transform.
class `Vector::transforms::awsEc2Metadata::Ec2Metadata` {
  /// Overrides the default EC2 metadata endpoint.
  ///
  /// Default if undefined: `"http://169.254.169.254"`
  endpoint: String?

  /// A list of metadata fields to include in each transformed event.
  ///
  /// Default if undefined: `{ "ami-id" "availability-zone" "instance-id" "instance-type"
  /// "local-hostname" "local-ipv4" "public-hostname" "public-ipv4" "region" "subnet-id" "vpc-id"
  /// "role-name" }`
  fields: Listing<String>?

  /// Sets a prefix for all event fields added by the transform.
  namespace: Any?

  /// Proxy configuration.
  ///
  /// Configure to proxy traffic through an HTTP(S) proxy when making external requests.
  ///
  /// Similar to common proxy configuration convention, you can set different proxies to use based on the
  /// type of traffic being proxied, as well as set specific hosts that should not be proxied.
  ///
  /// Default if undefined: `{}`
  proxy: Any?

  /// The interval between querying for updated metadata, in seconds.
  ///
  /// Default if undefined: `10`
  refresh_interval_secs: Any?

  /// The timeout for querying the EC2 metadata endpoint, in seconds.
  ///
  /// Default if undefined: `1`
  refresh_timeout_secs: Any?

  /// Requires the transform to be able to successfully query the EC2 metadata before starting to process
  /// the data.
  ///
  /// Default if undefined: `true`
  required: Boolean?

  /// A list of instance tags to include in each transformed event.
  ///
  /// Default if undefined: `{}`
  tags: Listing<String>?
}

/// Configuration for the `dedupe` transform.
class `Vector::transforms::dedupe::config::DedupeConfig` {
  /// Caching configuration for deduplication.
  ///
  /// Default if undefined: `{ ["num_events"] = 5000 }`
  cache: Cache?

  /// Options to control what fields to match against.
  ///
  /// When no field matching configuration is specified, events are matched using the `timestamp`,
  /// `host`, and `message` fields from an event. The specific field names used are those set in the
  /// global [`log schema`][global_log_schema] configuration.
  ///
  /// [global_log_schema]: https://vector.dev/docs/reference/configuration/global-options/#log_schema
  fields: (Any|FieldsAlternate1Alternate0|FieldsAlternate1Alternate1)?
}

/// Caching configuration for deduplication.
///
/// Default if undefined: `{ ["num_events"] = 5000 }`
class Cache {
  /// Number of events to cache and use for comparing incoming events to previously seen events.
  num_events: Int(isBetween(1.0, 9.007199254740991E15))
}

/// Matches events using only the specified fields.
class FieldsAlternate1Alternate0 {
  /// A wrapper around `OwnedTargetPath` that allows it to be used in Vector config with prefix default
  /// to `PathPrefix::Event`
  match: Listing<Any>
}

/// Matches events using all fields except for the ignored ones.
class FieldsAlternate1Alternate1 {
  /// A wrapper around `OwnedTargetPath` that allows it to be used in Vector config with prefix default
  /// to `PathPrefix::Event`
  ignore: Listing<Any>
}

/// Configuration for the `filter` transform.
class `Vector::transforms::filter::FilterConfig` {
  /// The condition that every input event is matched against.
  ///
  /// If an event is matched by the condition, it is forwarded. Otherwise, the event is dropped.
  condition: Any
}

/// Configuration for the `log_to_metric` transform.
class `Vector::transforms::logToMetric::LogToMetricConfig` {
  /// Setting this flag changes the behavior of this transformation.<br /> <p>Notably the `metrics` field
  /// will be ignored.</p> <p>All incoming events will be processed and if possible they will be
  /// converted to log events. Otherwise, only items specified in the 'metrics' field will be
  /// processed.</p> <pre class="chroma"><code class="language-toml" data-lang="toml">use
  /// serde_json::json; let json_event = json!({ "counter": { "value": 10.0 }, "kind": "incremental",
  /// "name": "test.transform.counter", "tags": { "env": "test_env", "host": "localhost" } });
  /// </code></pre>
  ///
  /// This is an example JSON representation of a counter with the following properties:
  ///
  /// - `counter`: An object with a single property `value` representing the counter value, in this case,
  /// `10.0`). - `kind`: A string indicating the kind of counter, in this case, "incremental". - `name`:
  /// A string representing the name of the counter, here set to "test.transform.counter". - `tags`: An
  /// object containing additional tags such as "env" and "host".
  ///
  /// Objects that can be processed include counter, histogram, gauge, set and summary.
  all_metrics: Boolean?

  /// A list of metrics to generate.
  metrics: Listing<Metric>
}

/// Specification of the type of an individual metric, and any associated data.
class Metric {
  /// Name of the field in the event to generate the metric.
  field: Any

  /// Overrides the name of the counter.
  ///
  /// If not specified, `field` is used as the name of the metric.
  name: Any?

  /// Sets the namespace for the metric.
  namespace: Any?

  /// Tags to apply to the metric.
  tags: Mapping<String, Any|Listing<Any>>?
}

/// Specification of a counter derived from a log event.
class MetricAlternate0 {
  /// A counter.
  type: "counter"
}

/// A histogram.
class MetricMetricAlternate1 {
  /// A histogram.
  type: "histogram"
}

/// A gauge.
class MetricAlternate2 {
  /// A gauge.
  type: "gauge"
}

/// A set.
class MetricAlternate3 {
  /// A set.
  type: "set"
}

/// A summary.
class MetricAlternate4 {
  /// A summary.
  type: "summary"
}

/// Configuration for version one of the `lua` transform.
class `Vector::transforms::lua::LuaConfigAlternate0` {
  /// A list of directories to search when loading a Lua file via the `require` function.
  ///
  /// If not specified, the modules are looked up in the configuration directories.
  ///
  /// Default if undefined: `{}`
  search_dirs: Listing<String>?

  /// The Lua program to execute for each event.
  source: String
}

/// Configuration for the version two of the `lua` transform.
class `Vector::transforms::lua::LuaConfigAlternate1` {
  /// Lifecycle hooks.
  ///
  /// These hooks can be set to perform additional processing during the lifecycle of the transform.
  hooks: Hooks

  /// When set to `single`, metric tag values are exposed as single strings, the same as they were before
  /// this config option. Tags with multiple values show the last assigned value, and null values are
  /// ignored.
  ///
  /// When set to `full`, all metric tags are exposed as arrays of either string or null values.
  ///
  /// Default if undefined: `"single"`
  metric_tag_values: Any?

  /// A list of directories to search when loading a Lua file via the `require` function.
  ///
  /// If not specified, the modules are looked up in the configuration directories.
  ///
  /// Default if undefined: `{}`
  search_dirs: Listing<Any>?

  /// The Lua program to initialize the transform with.
  ///
  /// The program can be used to import external dependencies, as well as define the functions used for
  /// the various lifecycle hooks. However, it's not strictly required, as the lifecycle hooks can be
  /// configured directly with inline Lua source for each respective hook.
  source: String?

  /// A list of timers which should be configured and executed periodically.
  ///
  /// Default if undefined: `{}`
  timers: Listing<Timer>?
}

/// Lifecycle hooks.
///
/// These hooks can be set to perform additional processing during the lifecycle of the transform.
class Hooks {
  /// The function called when the first event comes in, before `hooks.process` is called.
  ///
  /// It can produce new events using the `emit` function.
  ///
  /// This can either be inline Lua that defines a closure to use, or the name of the Lua function to
  /// call. In both cases, the closure/function takes a single parameter, `emit`, which is a reference to
  /// a function for emitting events.
  init: String?

  /// The function called for each incoming event.
  ///
  /// It can produce new events using the `emit` function.
  ///
  /// This can either be inline Lua that defines a closure to use, or the name of the Lua function to
  /// call. In both cases, the closure/function takes two parameters. The first parameter, `event`, is
  /// the event being processed, while the second parameter, `emit`, is a reference to a function for
  /// emitting events.
  process: String

  /// The function called when the transform is stopped.
  ///
  /// It can produce new events using the `emit` function.
  ///
  /// This can either be inline Lua that defines a closure to use, or the name of the Lua function to
  /// call. In both cases, the closure/function takes a single parameter, `emit`, which is a reference to
  /// a function for emitting events.
  shutdown: String?
}

/// A Lua timer.
class Timer {
  /// The handler function which is called when the timer ticks.
  ///
  /// It can produce new events using the `emit` function.
  ///
  /// This can either be inline Lua that defines a closure to use, or the name of the Lua function to
  /// call. In both cases, the closure/function takes a single parameter, `emit`, which is a reference to
  /// a function for emitting events.
  handler: String

  /// The interval to execute the handler, in seconds.
  interval_seconds: Any
}

/// Configuration for the `metric_to_log` transform.
class `Vector::transforms::metricToLog::MetricToLogConfig` {
  /// Name of the tag in the metric to use for the source host.
  ///
  /// If present, the value of the tag is set on the generated log event in the `host` field, where the
  /// field key uses the [global `host_key` option][global_log_schema_host_key].
  ///
  /// [global_log_schema_host_key]:
  /// https://vector.dev/docs/reference/configuration//global-options#log_schema.host_key
  host_tag: String?

  /// The namespace to use for logs. This overrides the global setting.
  log_namespace: Boolean?

  /// Controls how metric tag values are encoded.
  ///
  /// When set to `single`, only the last non-bare value of tags are displayed with the metric. When set
  /// to `full`, all metric tags are exposed as separate assignments as described by [the `native_json`
  /// codec][vector_native_json].
  ///
  /// [vector_native_json]:
  /// https://github.com/vectordotdev/vector/blob/master/lib/codecs/tests/data/native_encoding/schema.cue
  ///
  /// Default if undefined: `"single"`
  metric_tag_values: Any?

  /// The name of the time zone to apply to timestamp conversions that do not contain an explicit time
  /// zone.
  ///
  /// This overrides the [global `timezone`][global_timezone] option. The time zone name may be any name
  /// in the [TZ database][tz_database] or `local` to indicate system local time.
  ///
  /// [global_timezone]: https://vector.dev/docs/reference/configuration//global-options#timezone
  /// [tz_database]: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
  timezone: Any?
}

/// Configuration for the `reduce` transform.
class `Vector::transforms::reduce::config::ReduceConfig` {
  /// If supplied, every time this interval elapses for a given grouping, the reduced value for that
  /// grouping is flushed. Checked every flush_period_ms.
  end_every_period_ms: Number(isBetween(0.0, 9.007199254740991E15))?

  /// A condition used to distinguish the final event of a transaction.
  ///
  /// If this condition resolves to `true` for an event, the current transaction is immediately flushed
  /// with this event.
  ends_when: Any?

  /// The maximum period of time to wait after the last event is received, in milliseconds, before a
  /// combined event should be considered complete.
  ///
  /// Default if undefined: `30000`
  expire_after_ms: Any?

  /// The interval to check for and flush any expired events, in milliseconds.
  ///
  /// Default if undefined: `1000`
  flush_period_ms: Any?

  /// An ordered list of fields by which to group events.
  ///
  /// Each group with matching values for the specified keys is reduced independently, allowing you to
  /// keep independent event streams separate. When no fields are specified, all events are combined in a
  /// single group.
  ///
  /// For example, if `group_by = ["host", "region"]`, then all incoming events that have the same host
  /// and region are grouped together before being reduced.
  ///
  /// Default if undefined: `{}`
  group_by: Listing<String>?

  /// The maximum number of events to group together.
  max_events: Number(isBetween(1.0, 9.007199254740991E15))?

  /// A map of field names to custom merge strategies.
  ///
  /// For each field specified, the given strategy is used for combining events rather than the default
  /// behavior.
  ///
  /// The default behavior is as follows:
  ///
  /// - The first value of a string field is kept and subsequent values are discarded. - For timestamp
  /// fields the first is kept and a new field `[field-name]_end` is added with the last received
  /// timestamp value. - Numeric values are summed.
  ///
  /// Default if undefined: `{}`
  merge_strategies: Mapping<String, 
    "discard"
    |"retain"
    |"sum"
    |"max"
    |"min"
    |"array"
    |"concat"
    |"concat_newline"
    |"concat_raw"
    |"shortest_array"
    |"longest_array"
    |"flat_unique">?

  /// A condition used to distinguish the first event of a transaction.
  ///
  /// If this condition resolves to `true` for an event, the previous transaction is flushed (without
  /// this event) and a new transaction is started.
  starts_when: Any?
}

/// Configuration for the `remap` transform.
class `Vector::transforms::remap::RemapConfig` {
  /// Drops any event that is manually aborted during processing.
  ///
  /// If a VRL program is manually aborted (using [`abort`][vrl_docs_abort]) when processing an event,
  /// this option controls whether the original, unmodified event is sent downstream without any
  /// modifications or if it is dropped.
  ///
  /// Additionally, dropped events can potentially be diverted to a specially-named output for further
  /// logging and analysis by setting `reroute_dropped`.
  ///
  /// [vrl_docs_abort]: https://vector.dev/docs/reference/vrl/expressions/#abort
  ///
  /// Default if undefined: `true`
  drop_on_abort: Boolean?

  /// Drops any event that encounters an error during processing.
  ///
  /// Normally, if a VRL program encounters an error when processing an event, the original, unmodified
  /// event is sent downstream. In some cases, you may not want to send the event any further, such as if
  /// certain transformation or enrichment is strictly required. Setting `drop_on_error` to `true` allows
  /// you to ensure these events do not get processed any further.
  ///
  /// Additionally, dropped events can potentially be diverted to a specially named output for further
  /// logging and analysis by setting `reroute_dropped`.
  ///
  /// Default if undefined: `false`
  drop_on_error: Boolean?

  /// File path to the [Vector Remap Language][vrl] (VRL) program to execute for each event.
  ///
  /// If a relative path is provided, its root is the current working directory.
  ///
  /// Required if `source` is missing.
  ///
  /// [vrl]: https://vector.dev/docs/reference/vrl
  file: Any?

  /// When set to `single`, metric tag values are exposed as single strings, the same as they were before
  /// this config option. Tags with multiple values show the last assigned value, and null values are
  /// ignored.
  ///
  /// When set to `full`, all metric tags are exposed as arrays of either string or null values.
  ///
  /// Default if undefined: `"single"`
  metric_tag_values: Any?

  /// Reroutes dropped events to a named output instead of halting processing on them.
  ///
  /// When using `drop_on_error` or `drop_on_abort`, events that are "dropped" are processed no further.
  /// In some cases, it may be desirable to keep the events around for further analysis, debugging, or
  /// retrying.
  ///
  /// In these cases, `reroute_dropped` can be set to `true` which forwards the original event to a
  /// specially-named output, `dropped`. The original event is annotated with additional fields
  /// describing why the event was dropped.
  ///
  /// Default if undefined: `false`
  reroute_dropped: Boolean?

  /// The runtime to use for executing VRL code.
  ///
  /// Default if undefined: `"ast"`
  runtime: String?

  /// The [Vector Remap Language][vrl] (VRL) program to execute for each event.
  ///
  /// Required if `file` is missing.
  ///
  /// [vrl]: https://vector.dev/docs/reference/vrl
  source: String?

  /// The name of the timezone to apply to timestamp conversions that do not contain an explicit time
  /// zone.
  ///
  /// This overrides the [global `timezone`][global_timezone] option. The time zone name may be any name
  /// in the [TZ database][tz_database], or `local` to indicate system local time.
  ///
  /// [global_timezone]: https://vector.dev/docs/reference/configuration//global-options#timezone
  /// [tz_database]: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
  timezone: Any?
}

/// Configuration for the `route` transform.
class `Vector::transforms::route::RouteConfig` {
  /// Reroutes unmatched events to a named output instead of silently discarding them.
  ///
  /// Normally, if an event doesn't match any defined route, it is sent to the
  /// `<transform_name>._unmatched` output for further processing. In some cases, you may want to simply
  /// discard unmatched events and not process them any further.
  ///
  /// In these cases, `reroute_unmatched` can be set to `false` to disable the
  /// `<transform_name>._unmatched` output and instead silently discard any unmatched events.
  ///
  /// Default if undefined: `true`
  reroute_unmatched: Boolean?

  /// A table of route identifiers to logical conditions representing the filter of the route.
  ///
  /// Each route can then be referenced as an input by other components with the name
  /// `<transform_name>.<route_id>`. If an event doesnât match any route, and if `reroute_unmatched` is
  /// set to `true` (the default), it is sent to the `<transform_name>._unmatched` output. Otherwise, the
  /// unmatched event is instead silently discarded.
  ///
  /// Both `_unmatched`, as well as `_default`, are reserved output names and thus cannot be used as a
  /// route name.
  route: Mapping<String, Any>?
}

/// Configuration for the `sample` transform.
class `Vector::transforms::sample::config::SampleConfig` {
  /// A logical condition used to exclude events from sampling.
  exclude: Any?

  /// The name of the field whose value is hashed to determine if the event should be sampled.
  ///
  /// Each unique value for the key creates a bucket of related events to be sampled together and the
  /// rate is applied to the buckets themselves to sample `1/N` buckets. The overall rate of sampling may
  /// differ from the configured one if values in the field are not uniformly distributed. If left
  /// unspecified, or if the event doesnât have `key_field`, then the event is sampled independently.
  ///
  /// This can be useful to, for example, ensure that all logs for a given transaction are sampled
  /// together, but that overall `1/N` transactions are sampled.
  key_field: String?

  /// The rate at which events are forwarded, expressed as `1/N`.
  ///
  /// For example, `rate = 1500` means 1 out of every 1500 events are forwarded and the rest are dropped.
  rate: Int(isBetween(0.0, 9.007199254740991E15))
}

/// Controls the approach taken for tracking tag cardinality.
class `Vector::transforms::tagCardinalityLimit::config::TagCardinalityLimitConfig` {
  /// Possible actions to take when an event arrives that would exceed the cardinality limit for one or
  /// more of its tags.
  ///
  /// Default if undefined: `"drop_tag"`
  limit_exceeded_action: ("drop_tag"|"drop_event")?

  /// How many distinct values to accept for any given key.
  ///
  /// Default if undefined: `500`
  value_limit: Int(isBetween(0.0, 9.007199254740991E15))?
}

/// Tracks cardinality exactly.
///
/// This mode has higher memory requirements than `probabilistic`, but never falsely outputs metrics with
/// new tags after the limit has been hit.
class `Vector::transforms::tagCardinalityLimit::config::TagCardinalityLimitConfigAlternate0` {
  /// Tracks cardinality exactly.
  ///
  /// This mode has higher memory requirements than `probabilistic`, but never falsely outputs metrics
  /// with new tags after the limit has been hit.
  mode: "exact"
}

/// Tracks cardinality probabilistically.
///
/// Bloom filter configuration in probabilistic mode.
class `Vector::transforms::tagCardinalityLimit::config::TagCardinalityLimitConfigAlternate1` {
  /// Tracks cardinality probabilistically.
  ///
  /// This mode has lower memory requirements than `exact`, but may occasionally allow metric events to
  /// pass through the transform even when they contain new tags that exceed the configured limit. The
  /// rate at which this happens can be controlled by changing the value of `cache_size_per_key`.
  mode: "probabilistic"
}

/// Configuration for the `throttle` transform.
class `Vector::transforms::throttle::ThrottleConfig` {
  /// A logical condition used to exclude events from sampling.
  exclude: Any?

  /// Configuration of internal metrics for the Throttle transform.
  ///
  /// Default if undefined: `{ ["emit_events_discarded_per_key"] = false }`
  internal_metrics: InternalMetrics?

  /// The value to group events into separate buckets to be rate limited independently.
  ///
  /// If left unspecified, or if the event doesn't have `key_field`, then the event is not rate limited
  /// separately.
  key_field: Any?

  /// The number of events allowed for a given bucket per configured `window_secs`.
  ///
  /// Each unique key has its own `threshold`.
  threshold: UInt32

  /// The time window in which the configured `threshold` is applied, in seconds.
  window_secs: Any
}

/// Configuration of internal metrics for the Throttle transform.
///
/// Default if undefined: `{ ["emit_events_discarded_per_key"] = false }`
class InternalMetrics {
  /// Whether or not to emit the `events_discarded_total` internal metric with the `key` tag.
  ///
  /// If true, the counter will be incremented for each discarded event, including the key value
  /// associated with the discarded event. If false, the counter will not be emitted. Instead, the number
  /// of discarded events can be seen through the `component_discarded_events_total` internal metric.
  ///
  /// Note that this defaults to false because the `key` tag has potentially unbounded cardinality. Only
  /// set this to true if you know that the number of unique keys is bounded.
  ///
  /// Default if undefined: `false`
  emit_events_discarded_per_key: Boolean?
}

/// Controls how acknowledgements are handled for this sink.
///
/// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
/// handled.
///
/// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
class `VectorCore::config::AcknowledgementsConfig` {
  /// Whether or not end-to-end acknowledgements are enabled.
  ///
  /// When enabled for a sink, any source connected to that sink, where the source supports end-to-end
  /// acknowledgements as well, waits for events to be acknowledged by the sink before acknowledging them
  /// at the source.
  ///
  /// Enabling or disabling acknowledgements at the sink level takes precedence over any global
  /// [`acknowledgements`][global_acks] configuration.
  ///
  /// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
  enabled: Boolean?
}

/// Controls how acknowledgements are handled by this source.
///
/// This setting is **deprecated** in favor of enabling `acknowledgements` at the [global][global_acks]
/// or sink level.
///
/// Enabling or disabling acknowledgements at the source level has **no effect** on acknowledgement
/// behavior.
///
/// See [End-to-end Acknowledgements][e2e_acks] for more information on how event acknowledgement is
/// handled.
///
/// [global_acks]: https://vector.dev/docs/reference/configuration/global-options/#acknowledgements
/// [e2e_acks]: https://vector.dev/docs/about/under-the-hood/architecture/end-to-end-acknowledgements/
@Deprecated
class `VectorCore::config::SourceAcknowledgementsConfig` {
  /// Whether or not end-to-end acknowledgements are enabled for this source.
  enabled: Boolean?
}

/// Proxy configuration.
///
/// Configure to proxy traffic through an HTTP(S) proxy when making external requests.
///
/// Similar to common proxy configuration convention, you can set different proxies to use based on the
/// type of traffic being proxied, as well as set specific hosts that should not be proxied.
class `VectorCore::config::proxy::ProxyConfig` {
  /// Enables proxying support.
  ///
  /// Default if undefined: `true`
  enabled: Boolean?

  /// Proxy endpoint to use when proxying HTTP traffic.
  ///
  /// Must be a valid URI string.
  http: Uri?

  /// Proxy endpoint to use when proxying HTTPS traffic.
  ///
  /// Must be a valid URI string.
  https: Uri?

  /// A list of hosts to avoid proxying.
  ///
  /// Multiple patterns are allowed:
  ///
  /// | Pattern | Example match | | ------------------- |
  /// --------------------------------------------------------------------------- | | Domain names |
  /// `example.com` matches requests to `example.com` | | Wildcard domains | `.example.com` matches
  /// requests to `example.com` and its subdomains | | IP addresses | `127.0.0.1` matches requests to
  /// `127.0.0.1` | | [CIDR][cidr] blocks | `192.168.0.0/16` matches requests to any IP addresses in this
  /// range | | Splat | `*` matches all hosts |
  ///
  /// [cidr]: https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing
  ///
  /// Default if undefined: `{}`
  no_proxy: Listing<String>?
}

/// TLS configuration.
class `VectorCore::tls::settings::TlsConfig` {
  /// Sets the list of supported ALPN protocols.
  ///
  /// Declare the supported ALPN protocols, which are used during negotiation with peer. They are
  /// prioritized in the order that they are defined.
  alpn_protocols: Listing<String>?

  /// Absolute path to an additional CA certificate file.
  ///
  /// The certificate must be in the DER or PEM (X.509) format. Additionally, the certificate can be
  /// provided as an inline string in PEM format.
  ca_file: Any?

  /// Absolute path to a certificate file used to identify this server.
  ///
  /// The certificate must be in DER, PEM (X.509), or PKCS#12 format. Additionally, the certificate can
  /// be provided as an inline string in PEM format.
  ///
  /// If this is set, and is not a PKCS#12 archive, `key_file` must also be set.
  crt_file: Any?

  /// Absolute path to a private key file used to identify this server.
  ///
  /// The key must be in DER or PEM (PKCS#8) format. Additionally, the key can be provided as an inline
  /// string in PEM format.
  key_file: Any?

  /// Passphrase used to unlock the encrypted key file.
  ///
  /// This has no effect unless `key_file` is set.
  key_pass: String?

  /// Enables certificate verification. For components that create a server, this requires that the
  /// client connections have a valid client certificate. For components that initiate requests, this
  /// validates that the upstream has a valid certificate.
  ///
  /// If enabled, certificates must not be expired and must be issued by a trusted issuer. This
  /// verification operates in a hierarchical manner, checking that the leaf certificate (the certificate
  /// presented by the client/server) is not only valid, but that the issuer of that certificate is also
  /// valid, and so on until the verification process reaches a root certificate.
  ///
  /// Do NOT set this to `false` unless you understand the risks of not verifying the validity of
  /// certificates.
  verify_certificate: Boolean?

  /// Enables hostname verification.
  ///
  /// If enabled, the hostname used to connect to the remote host must be present in the TLS certificate
  /// presented by the remote host, either as the Common Name or as an entry in the Subject Alternative
  /// Name extension.
  ///
  /// Only relevant for outgoing connections.
  ///
  /// Do NOT set this to `false` unless you understand the risks of not verifying the remote hostname.
  verify_hostname: Boolean?
}

/// TLS configuration.
class `VectorCore::tls::settings::TlsEnableableConfig` {
  /// Whether or not to require TLS for incoming or outgoing connections.
  ///
  /// When enabled and used for incoming connections, an identity certificate is also required. See
  /// `tls.crt_file` for more information.
  enabled: Boolean?
}

/// The user configuration to choose the metric tag strategy.
typealias `Codecs::MetricTagValues` = "single"|"full"

/// Configures how events are decoded from raw bytes.
typealias `Codecs::decoding::DeserializerConfig` = 
  DecodingAlternate1Alternate0
  |DecodingAlternate1Alternate1
  |DecodingAlternate1Alternate2
  |DecodingAlternate1Alternate3
  |DecodingAlternate1Alternate4
  |DecodingAlternate1Alternate5
  |DecodingAlternate1Alternate6
  |DecodingAlternate1Alternate7
  |DecodingAlternate1Alternate8

/// Framing configuration.
///
/// Framing handles how events are separated when encoded in a raw byte form, where each event is a frame
/// that must be prefixed, or delimited, in a way that marks where an event begins and ends within the
/// byte stream.
typealias `Codecs::decoding::FramingConfig` = 
  `Core::option::Option<codecs::decoding::FramingConfig>Alternate1Alternate0`
  |`Core::option::Option<codecs::decoding::FramingConfig>Alternate1Alternate1`
  |`Core::option::Option<codecs::decoding::FramingConfig>Alternate1Alternate2`
  |`Core::option::Option<codecs::decoding::FramingConfig>Alternate1Alternate3`
  |`Core::option::Option<codecs::decoding::FramingConfig>Alternate1Alternate4`

/// Framing configuration.
///
/// Framing handles how events are separated when encoded in a raw byte form, where each event is a frame
/// that must be prefixed, or delimited, in a way that marks where an event begins and ends within the
/// byte stream.
typealias `Core::option::Option<codecs::decoding::FramingConfig>` = Any|`Codecs::decoding::FramingConfig`

/// A file path.
typealias `Core::option::Option<std::path::PathBuf>` = Any|Any

/// Configuration of the region/endpoint to use when interacting with an AWS service.
typealias `Core::option::Option<vector::aws::region::RegionOrEndpoint>` = Any|Any

/// An event matching condition.
///
/// Many methods exist for matching events, such as using a VRL expression, a Datadog Search query
/// string, or hard-coded matchers like "must be a metric" or "fields A, B, and C must match these
/// constraints".
///
/// As VRL is the most common way to apply conditions to events, this type provides a shortcut to define
/// VRL expressions directly in the configuration by passing the VRL expression as a string:
///
/// ```toml condition = '.message == "hooray"' ```
///
/// When other condition types are required, they can be specified with an enum-style notation:
///
/// ```toml condition.type = 'datadog_search' condition.source = 'NOT "foo"' ```
typealias `Core::option::Option<vector::conditions::AnyCondition>` = Any|`Vector::conditions::AnyCondition`

/// Configuration of the authentication strategy for HTTP requests.
///
/// HTTP authentication should be used with HTTPS only, as the authentication credentials are passed as
/// an HTTP header without any additional encryption beyond what is provided by the transport itself.
typealias `Core::option::Option<vector::http::Auth>` = 
  Any
  |
  `Core::option::Option<vector::http::Auth>Alternate1Alternate0`
  |`Core::option::Option<vector::http::Auth>Alternate1Alternate1`

/// Configuration of the authentication strategy when interacting with NATS.
typealias `Core::option::Option<vector::nats::NatsAuthConfig>` = 
  Any
  |
  `Core::option::Option<vector::nats::NatsAuthConfig>Alternate1Alternate0`
  |`Core::option::Option<vector::nats::NatsAuthConfig>Alternate1Alternate1`
  |`Core::option::Option<vector::nats::NatsAuthConfig>Alternate1Alternate2`
  |`Core::option::Option<vector::nats::NatsAuthConfig>Alternate1Alternate3`

/// Configuration settings for InfluxDB v0.x/v1.x.
typealias `Core::option::Option<vector::sinks::influxdb::InfluxDb1Settings>` = Any|`Core::option::Option<vector::sinks::influxdb::InfluxDb1Settings>Alternate1`

/// Configuration settings for InfluxDB v2.x.
typealias `Core::option::Option<vector::sinks::influxdb::InfluxDb2Settings>` = Any|`Core::option::Option<vector::sinks::influxdb::InfluxDb2Settings>Alternate1`

/// Compression configuration.
///
/// All compression algorithms use the default compression level unless otherwise specified.
typealias `Core::option::Option<vector::sinks::util::buffer::compression::Compression>` = Any|`Vector::sinks::util::buffer::compression::Compression`

/// The URI component of a request.
typealias `Core::option::Option<vector::sinks::util::uri::UriSerde>` = Any|Any

/// HTTP Basic authentication configuration.
typealias `Core::option::Option<vector::sources::util::http::auth::HttpSourceAuthConfig>` = Any|`Core::option::Option<vector::sources::util::http::auth::HttpSourceAuthConfig>Alternate1`

/// Configuration of multi-line aggregation.
typealias `Core::option::Option<vector::sources::util::multilineConfig::MultilineConfig>` = Any|`Core::option::Option<vector::sources::util::multilineConfig::MultilineConfig>Alternate1`

/// A templated field.
///
/// In many cases, components can be configured so that part of the component's functionality can be
/// customized on a per-event basis. For example, you have a sink that writes events to a file and you
/// want to specify which file an event should go to by using an event field as part of the input to the
/// filename used.
///
/// By using `Template`, users can specify either fixed strings or templated strings. Templated strings
/// use a common syntax to refer to fields in an event that is used as the input data when rendering the
/// template. An example of a fixed string is `my-file.log`. An example of a template string is
/// `my-file-{{key}}.log`, where `{{key}}` is the key's value when the template is rendered into a
/// string.
typealias `Core::option::Option<vector::template::Template>` = Any|Any

/// Wrapper for sensitive strings containing credentials
typealias `Core::option::Option<vectorCommon::sensitiveString::SensitiveString>` = Any|Any

/// List of allowed origin IP networks. IP addresses must be in CIDR notation.
typealias `Core::option::Option<vectorCore::ipallowlist::IpAllowlistConfig>` = Any|Listing<String>

/// TCP keepalive settings for socket-based components.
typealias `Core::option::Option<vectorCore::tcp::TcpKeepaliveConfig>` = Any|`Core::option::Option<vectorCore::tcp::TcpKeepaliveConfig>Alternate1`

/// TLS configuration.
typealias `Core::option::Option<vectorCore::tls::settings::TlsConfig>` = Any|Any

/// Configures the TLS options for incoming/outgoing connections.
typealias `Core::option::Option<vectorCore::tls::settings::TlsEnableableConfig>` = Any|Any

/// TlsEnableableConfig for `sources`, adding metadata from the client certificate.
typealias `Core::option::Option<vectorCore::tls::settings::TlsSourceConfig>` = Any|`Core::option::Option<vectorCore::tls::settings::TlsSourceConfig>Alternate1`

/// A wrapper around `OwnedValuePath` that allows it to be used in Vector config. This requires a valid
/// path to be used. If you want to allow optional paths, use [optional_path::OptionalValuePath].
typealias `Core::option::Option<vectorLookup::lookupV2::ConfigValuePath>` = Any|Any

/// An optional path that deserializes an empty string to `None`.
typealias `Core::option::Option<vectorLookup::lookupV2::optionalPath::OptionalValuePath>` = Any|Any

/// Timezone to use for any date specifiers in template strings.
///
/// This can refer to any valid timezone as defined in the [TZ database][tzdb], or "local" which refers
/// to the system local timezone. It will default to the [globally configured
/// timezone](https://vector.dev/docs/reference/configuration/global-options/#timezone).
///
/// [tzdb]: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
typealias `Core::option::Option<vrl::compiler::datetime::TimeZone>` = Any|"local"|String

/// File position to use when reading a new file.
typealias `FileSource::ReadFromConfig` = "beginning"|"end"

/// A span of time, in fractional seconds.
typealias `SerdeWith::DurationFractionalSeconds` = Number(isBetween(-9.007199254740991E15, 9.007199254740991E15))

/// A span of time, in whole milliseconds.
typealias `SerdeWith::DurationMilliSeconds` = Int(isBetween(0.0, 9.007199254740991E15))

/// A span of time, in whole seconds.
typealias `SerdeWith::DurationSeconds` = Int(isBetween(0.0, 9.007199254740991E15))

/// A file path.
typealias `Stdlib::PathBuf` = String(matches(Regex(#"(\/.*|[a-zA-Z]:\\(?:([^<>:"\/\\|?*]*[^<>:"\/\\|?*.]\\|..\\)*([^<>:"\/\\|?*]*[^<>:"\/\\|?*.]\\?|..\\))?)"#)))

/// An internet socket address, either IPv4 or IPv6.
typealias `Stdlib::SocketAddr` = String

/// Configuration of the authentication strategy for interacting with AWS services.
typealias `Vector::aws::auth::AwsAuthentication` = 
  AuthAlternate1Alternate2Alternate0
  |AuthAlternate1Alternate2Alternate1
  |AuthAlternate1Alternate2Alternate2
  |AuthAlternate1Alternate2Alternate3

/// An event matching condition.
///
/// Many methods exist for matching events, such as using a VRL expression, a Datadog Search query
/// string, or hard-coded matchers like "must be a metric" or "fields A, B, and C must match these
/// constraints".
///
/// As VRL is the most common way to apply conditions to events, this type provides a shortcut to define
/// VRL expressions directly in the configuration by passing the VRL expression as a string:
///
/// ```toml condition = '.message == "hooray"' ```
///
/// When other condition types are required, they can be specified with an enum-style notation:
///
/// ```toml condition.type = 'datadog_search' condition.source = 'NOT "foo"' ```
typealias `Vector::conditions::AnyCondition` = 
  String
  |
  `Vector::conditions::AnyConditionAlternate1Alternate0`
  |`Vector::conditions::AnyConditionAlternate1Alternate1`
  |`Vector::conditions::AnyConditionAlternate1Alternate2`
  |`Vector::conditions::AnyConditionAlternate1Alternate3`
  |`Vector::conditions::AnyConditionAlternate1Alternate4`

/// A list of upstream [source][sources] or [transform][transforms] IDs.
///
/// Wildcards (`*`) are supported.
///
/// See [configuration][configuration] for more info.
///
/// [sources]: https://vector.dev/docs/reference/configuration/sources/ [transforms]:
/// https://vector.dev/docs/reference/configuration/transforms/ [configuration]:
/// https://vector.dev/docs/reference/configuration/
typealias `Vector::config::id::Inputs<alloc::string::String>` = Listing<String>

/// Configuration for the `unit_test` source.
typealias `Vector::config::unitTest::unitTestComponents::UnitTestSourceConfig` = Dynamic

/// Configuration for the `unit_test_stream` sink.
typealias `Vector::config::unitTest::unitTestComponents::UnitTestStreamSinkConfig` = Dynamic

/// Configuration for the `unit_test_stream` source.
typealias `Vector::config::unitTest::unitTestComponents::UnitTestStreamSourceConfig` = Dynamic

/// Configurable sinks in Vector.
typealias `Vector::sinks::Sinks` = 
  `Vector::sinks::SinksAlternate0`
  |`Vector::sinks::SinksAlternate1`
  |`Vector::sinks::SinksAlternate2`
  |`Vector::sinks::SinksAlternate3`
  |`Vector::sinks::SinksAlternate4`
  |`Vector::sinks::SinksAlternate5`
  |`Vector::sinks::SinksAlternate6`
  |`Vector::sinks::SinksAlternate7`
  |`Vector::sinks::SinksAlternate8`
  |`Vector::sinks::SinksAlternate9`
  |`Vector::sinks::SinksAlternate10`
  |`Vector::sinks::SinksAlternate11`
  |`Vector::sinks::SinksAlternate12`
  |`Vector::sinks::SinksAlternate13`
  |`Vector::sinks::SinksAlternate14`
  |`Vector::sinks::SinksAlternate15`
  |`Vector::sinks::SinksAlternate16`
  |`Vector::sinks::SinksAlternate17`
  |`Vector::sinks::SinksAlternate18`
  |`Vector::sinks::SinksAlternate19`
  |`Vector::sinks::SinksAlternate20`
  |`Vector::sinks::SinksAlternate21`
  |`Vector::sinks::SinksAlternate22`
  |`Vector::sinks::SinksAlternate23`
  |`Vector::sinks::SinksAlternate24`
  |`Vector::sinks::SinksAlternate25`
  |`Vector::sinks::SinksAlternate26`
  |`Vector::sinks::SinksAlternate27`
  |`Vector::sinks::SinksAlternate28`
  |`Vector::sinks::SinksAlternate29`
  |`Vector::sinks::SinksAlternate30`
  |`Vector::sinks::SinksAlternate31`
  |`Vector::sinks::SinksAlternate32`
  |`Vector::sinks::SinksAlternate33`
  |`Vector::sinks::SinksAlternate34`
  |`Vector::sinks::SinksAlternate35`
  |`Vector::sinks::SinksAlternate36`
  |`Vector::sinks::SinksAlternate37`
  |`Vector::sinks::SinksAlternate38`
  |`Vector::sinks::SinksAlternate39`
  |`Vector::sinks::SinksAlternate40`
  |`Vector::sinks::SinksAlternate41`
  |`Vector::sinks::SinksAlternate42`
  |`Vector::sinks::SinksAlternate43`
  |`Vector::sinks::SinksAlternate44`
  |`Vector::sinks::SinksAlternate45`
  |`Vector::sinks::SinksAlternate46`
  |`Vector::sinks::SinksAlternate47`
  |`Vector::sinks::SinksAlternate48`
  |`Vector::sinks::SinksAlternate49`
  |`Vector::sinks::SinksAlternate50`
  |`Vector::sinks::SinksAlternate51`
  |`Vector::sinks::SinksAlternate52`
  |`Vector::sinks::SinksAlternate53`
  |`Vector::sinks::SinksAlternate54`
  |`Vector::sinks::SinksAlternate55`
  |`Vector::sinks::SinksAlternate56`

/// Configuration for the `logdna` sink.
typealias `Vector::sinks::mezmo::LogdnaConfig` = Any

/// The Sematext region to send data to.
typealias `Vector::sinks::sematext::Region` = "us"|"eu"

/// Compression configuration.
///
/// All compression algorithms use the default compression level unless otherwise specified.
typealias `Vector::sinks::util::buffer::compression::Compression` = 
  "none"|"gzip"|"zlib"|"zstd"|"snappy"
  |`Vector::sinks::util::buffer::compression::CompressionAlternate1`

/// The jitter mode to use for retry backoff behavior.
typealias `Vector::sinks::util::retries::JitterMode` = "None"|"Full"

/// Configuration for outbound request concurrency.
///
/// This can be set either to one of the below enum values or to a positive integer, which denotes a
/// fixed concurrency limit.
typealias `Vector::sinks::util::service::concurrency::Concurrency` = "none"|"adaptive"|Int(isBetween(0.0, 9.007199254740991E15))

/// The address to connect to.
///
/// Both IP addresses and hostnames/fully qualified domain names (FQDNs) are accepted formats.
///
/// The address _must_ include a port.
typealias `Vector::sinks::util::service::net::HostAndPort` = String

/// The URI component of a request.
typealias `Vector::sinks::util::uri::UriSerde` = String

/// Configurable sources in Vector.
typealias `Vector::sources::Sources` = 
  `Vector::sources::SourcesAlternate0`
  |`Vector::sources::SourcesAlternate1`
  |`Vector::sources::SourcesAlternate2`
  |`Vector::sources::SourcesAlternate3`
  |`Vector::sources::SourcesAlternate4`
  |`Vector::sources::SourcesAlternate5`
  |`Vector::sources::SourcesAlternate6`
  |`Vector::sources::SourcesAlternate7`
  |`Vector::sources::SourcesAlternate8`
  |`Vector::sources::SourcesAlternate9`
  |`Vector::sources::SourcesAlternate10`
  |`Vector::sources::SourcesAlternate11`
  |`Vector::sources::SourcesAlternate12`
  |`Vector::sources::SourcesAlternate13`
  |`Vector::sources::SourcesAlternate14`
  |`Vector::sources::SourcesAlternate15`
  |`Vector::sources::SourcesAlternate16`
  |`Vector::sources::SourcesAlternate17`
  |`Vector::sources::SourcesAlternate18`
  |`Vector::sources::SourcesAlternate19`
  |`Vector::sources::SourcesAlternate20`
  |`Vector::sources::SourcesAlternate21`
  |`Vector::sources::SourcesAlternate22`
  |`Vector::sources::SourcesAlternate23`
  |`Vector::sources::SourcesAlternate24`
  |`Vector::sources::SourcesAlternate25`
  |`Vector::sources::SourcesAlternate26`
  |`Vector::sources::SourcesAlternate27`
  |`Vector::sources::SourcesAlternate28`
  |`Vector::sources::SourcesAlternate29`
  |`Vector::sources::SourcesAlternate30`
  |`Vector::sources::SourcesAlternate31`
  |`Vector::sources::SourcesAlternate32`
  |`Vector::sources::SourcesAlternate33`
  |`Vector::sources::SourcesAlternate34`
  |`Vector::sources::SourcesAlternate35`
  |`Vector::sources::SourcesAlternate36`
  |`Vector::sources::SourcesAlternate37`
  |`Vector::sources::SourcesAlternate38`
  |`Vector::sources::SourcesAlternate39`
  |`Vector::sources::SourcesAlternate40`
  |`Vector::sources::SourcesAlternate41`
  |`Vector::sources::SourcesAlternate42`
  |`Vector::sources::SourcesAlternate43`
  |`Vector::sources::SourcesAlternate44`

/// Configuration for the `http` source.
typealias `Vector::sources::httpServer::HttpConfig` = Any

/// Listening mode for the `socket` source.
typealias `Vector::sources::socket::SocketConfig` = 
  `Vector::sources::socket::SocketConfigAlternate0`
  |`Vector::sources::socket::SocketConfigAlternate1`
  |`Vector::sources::socket::SocketConfigAlternate2`
  |`Vector::sources::socket::SocketConfigAlternate3`

/// Configuration for the `statsd` source.
typealias `Vector::sources::statsd::StatsdConfig` = 
  `Vector::sources::statsd::StatsdConfigAlternate0`
  |`Vector::sources::statsd::StatsdConfigAlternate1`
  |`Vector::sources::statsd::StatsdConfigAlternate2`

/// HTTP method.
typealias `Vector::sources::util::http::method::HttpMethod` = "HEAD"|"GET"|"POST"|"PUT"|"PATCH"|"DELETE"

/// The socket address to listen for connections on, or `systemd{#N}` to use the Nth socket passed by
/// systemd socket activation.
///
/// If a socket address is used, it _must_ include a port.
typealias `Vector::sources::util::net::SocketListenAddr` = String

/// A templated field.
///
/// In many cases, components can be configured so that part of the component's functionality can be
/// customized on a per-event basis. For example, you have a sink that writes events to a file and you
/// want to specify which file an event should go to by using an event field as part of the input to the
/// filename used.
///
/// By using `Template`, users can specify either fixed strings or templated strings. Templated strings
/// use a common syntax to refer to fields in an event that is used as the input data when rendering the
/// template. An example of a fixed string is `my-file.log`. An example of a template string is
/// `my-file-{{key}}.log`, where `{{key}}` is the key's value when the template is rendered into a
/// string.
typealias `Vector::template::Template` = String

/// Configurable transforms in Vector.
typealias `Vector::transforms::Transforms` = 
  `Vector::transforms::TransformsAlternate0`
  |`Vector::transforms::TransformsAlternate1`
  |`Vector::transforms::TransformsAlternate2`
  |`Vector::transforms::TransformsAlternate3`
  |`Vector::transforms::TransformsAlternate4`
  |`Vector::transforms::TransformsAlternate5`
  |`Vector::transforms::TransformsAlternate6`
  |`Vector::transforms::TransformsAlternate7`
  |`Vector::transforms::TransformsAlternate8`
  |`Vector::transforms::TransformsAlternate9`
  |`Vector::transforms::TransformsAlternate10`
  |`Vector::transforms::TransformsAlternate11`
  |`Vector::transforms::TransformsAlternate12`

/// Configuration for the `lua` transform.
typealias `Vector::transforms::lua::LuaConfig` = `Vector::transforms::lua::LuaConfigAlternate0`|`Vector::transforms::lua::LuaConfigAlternate1`

/// Wrapper for sensitive strings containing credentials
typealias `VectorCommon::sensitiveString::SensitiveString` = String

/// Metric kind.
///
/// Metrics can be either absolute of incremental. Absolute metrics represent a sort of "last write wins"
/// scenario, where the latest absolute value seen is meant to be the actual metric value. In contrast,
/// and perhaps intuitively, incremental metrics are meant to be additive, such that we don't know what
/// total value of the metric is, but we know that we'll be adding or subtracting the given value from
/// it.
///
/// Generally speaking, most metrics storage systems deal with incremental updates. A notable exception
/// is Prometheus, which deals with, and expects, absolute values from clients.
typealias `VectorCore::event::metric::MetricKind` = "incremental"|"absolute"

/// A wrapper around `OwnedTargetPath` that allows it to be used in Vector config with prefix default to
/// `PathPrefix::Event`
typealias `VectorLookup::lookupV2::ConfigTargetPath` = String

/// A wrapper around `OwnedValuePath` that allows it to be used in Vector config. This requires a valid
/// path to be used. If you want to allow optional paths, use [optional_path::OptionalValuePath].
typealias `VectorLookup::lookupV2::ConfigValuePath` = String

/// An optional path that deserializes an empty string to `None`.
typealias `VectorLookup::lookupV2::optionalPath::OptionalValuePath` = String
